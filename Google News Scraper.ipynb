{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google News Scraper\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a scraper that I built to retrieve the top Google News articles from the first page of the Google's News tab for the search query **bitcoin**.\n",
    "\n",
    "The scraper is chunked into each of the months for successive years beginning, 2014 until 2017.\n",
    "> This chunking helps us scrape Google search results without violating their TOS (which could result in your IP being banned for a few minutes or even a few days). Basically to avoid imitating a robot.\n",
    "\n",
    "Articles for each day of a particular month are extracted via the urls scraped from the corresponding Google search result page for that day. The article text from the url is then obtained via the \n",
    "\n",
    ">[Newspaper3k library](https://github.com/codelucas/newspaper).\n",
    "\n",
    "> Note: When a url fails to be processed by Newspaper3k it throws a, *You must download() an article first!* error that usually indicates that that news source is absent within the library.\n",
    "\n",
    "Finally I created a dictionary to store the dates as keys and the article lists for those dates as values. \n",
    "> I then merged all the collected csv files into one csv file containing the top articles list for each date starting from January 7, 2014 till Dec 12, 2017. You can view the merging in my other notebook for [Sentiment Analysis of Top Google News Articles for keyword bitcoin](Sentiment Analysis of Top Google News Articles for keyword bitcoin.ipynb).\n",
    "\n",
    "Consider the first implementation for the January of 2014 to be an example that was extended to the rest of the years to understand how exactly the code works. \n",
    "> **Note**: In my implementation, I scraped all articles for the entire year for 2014 and then exported the data to a CSV file as opposed to month wise CSV files for the rest of the years. \n",
    "\n",
    "> The entire process of scraping took a week owing to restrictions imposed by Google's TOS for scraping. So if you are running this notebook ensure that you run each block of code individually as opposed to consecutively at once to avoid getting a 503 error (usually a result of bot detection on Google's end) that will block your access for a few hours. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "from random import randint\n",
    "import re\n",
    "from selenium import webdriver\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YEAR = 2014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MONTH = JANUARY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Date list to iterate over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "dt = datetime.datetime(2014, 1, 1)\n",
    "end = datetime.datetime(2014, 2, 1)\n",
    "step = datetime.timedelta(days=1)\n",
    "\n",
    "dates_list = []\n",
    "\n",
    "while dt < end:\n",
    "    dates_list.append(dt.strftime('%m/%d/%Y'))\n",
    "    dt += step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to scrape News urls from Google Search News and Parse Article Text via Newspaper3k library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "news_dictionary = {}\n",
    "\n",
    "for dt in dates_list:\n",
    "    def scrape_news_urls(link):\n",
    "        # Time to wait before each request randomized to emulate human interaction with browser\n",
    "        time.sleep(randint(0, 20))\n",
    "        payload = {'as_epq': 'bitcoin', 'tbs':'cdr:1,cd_min:'+dt+',cd_max:'+dt+\"'\",'tbm':'nws'}\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'}\n",
    "        r = requests.get(link, headers=headers, params=payload)\n",
    "        print(r.status_code)  # Print the status code\n",
    "        content = r.text\n",
    "        news_urls = []\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        news_url = soup.findAll(\"a\", {\"class\": \"l _PMs\"}, href=True)\n",
    "        for url in news_url:\n",
    "            news_urls.append(url['href'])\n",
    "        return news_urls\n",
    "    \n",
    "    URL = 'https://www.google.com/search'\n",
    "    n_url = scrape_news_urls(URL)\n",
    "    article_list = []\n",
    "    \n",
    "    for url in n_url:\n",
    "        if re.match(r'https://www.ft.com/',url) is not None:\n",
    "            from sys import exit\n",
    "            from selenium import webdriver\n",
    "            \n",
    "            # Function to access Financial Times(needs mandatory subscription) through Selenium enabled authentication \n",
    "            def open_browser():\n",
    "                global browser\n",
    "                browser = webdriver.Firefox()\n",
    "                browser.get('https://accounts.ft.com/login?location=https%3A%2F%2Fwww.ft.com%2F')\n",
    "                \n",
    "                username_input = input(\"Please enter your username (email address): \")\n",
    "                username_submit = browser.find_element_by_id(\"email\")\n",
    "                username_submit.send_keys(username_input)\n",
    "                browser.find_element_by_name(\"Next\").click()\n",
    "                \n",
    "                print(\"\"\"Please type in your password, then answer the following:\"\"\")\n",
    "                password_input = input(\"Please enter your password: \")\n",
    "                password_submit = browser.find_element_by_id(\"password\")\n",
    "                password_submit.send_keys(password_input)\n",
    "                confirmation = input(\"Have you typed in your password yet? (y/n): \")\n",
    "                \n",
    "                if confirmation == \"y\":\n",
    "                    browser.find_element_by_name(\"Sign in\").click() #submit\n",
    "                else:\n",
    "                    browser.close()\n",
    "                    exit()\n",
    "            \n",
    "            def article_search(url):\n",
    "                browser.get(url)\n",
    "                art = browser.find_element_by_xpath(\"//*[@id='site-content']/div[3]\")\n",
    "                article_list.append(art.text.rstrip('\\n').replace('\\n', ' '))\n",
    "            \n",
    "            # Uncomment if you are running for the first time to ensure access via authentication \n",
    "            def start(url):\n",
    "#                 choice = input(\"Do you need to log into FT.com? (y/n): \")\n",
    "#                 if choice == \"y\":\n",
    "#                     open_browser()    \n",
    "#                 elif choice == \"n\":\n",
    "                article_search(url)        \n",
    "#                 else:\n",
    "#                     exit()\n",
    "\n",
    "            start(url)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                from newspaper import Article\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article_list.append(article.text)\n",
    "            except:\n",
    "                article_list.append(\"\")\n",
    "\n",
    "    news_dictionary[dt] = article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['01/01/2014', '01/02/2014', '01/03/2014', '01/04/2014', '01/05/2014', '01/06/2014', '01/07/2014', '01/08/2014', '01/09/2014', '01/10/2014', '01/11/2014', '01/12/2014', '01/13/2014', '01/14/2014', '01/15/2014', '01/16/2014', '01/17/2014', '01/18/2014', '01/19/2014', '01/20/2014', '01/21/2014', '01/22/2014', '01/23/2014', '01/24/2014', '01/25/2014', '01/26/2014', '01/27/2014', '01/28/2014', '01/29/2014', '01/30/2014', '01/31/2014'])\n"
     ]
    }
   ],
   "source": [
    "# Check if all dates were processed \n",
    "print(news_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read dictionary into Pandas dataframe and write results to CSV (Sample extended to years 2015-2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view the actual implementation for only the year 2014 [here](#articles_2014_csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dates = list(news_dictionary.keys())\n",
    "articleslist = list(news_dictionary.values())\n",
    "\n",
    "df_date_articles = pd.DataFrame({'Date':dates})\n",
    "df_date_articles['Articles'] = articleslist\n",
    "# Only change will be in the csv file name for the rest of the code blocks\n",
    "df_date_articles.to_csv('articles_2014_jan.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MONTH = FEBRUARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "Article `download()` failed with 403 Client Error: Forbidden for url: https://seekingalpha.com/article/1987521-bitchip-the-bitcoin-killer-the-merchants-solution-to-the-bitcoin-problem on URL https://seekingalpha.com/article/1987521-bitchip-the-bitcoin-killer-the-merchants-solution-to-the-bitcoin-problem\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "dt = datetime.datetime(2014, 2, 1)\n",
    "end = datetime.datetime(2014, 3, 1)\n",
    "step = datetime.timedelta(days=1)\n",
    "\n",
    "dates_list = []\n",
    "\n",
    "while dt < end:\n",
    "    dates_list.append(dt.strftime('%m/%d/%Y'))\n",
    "    dt += step\n",
    "\n",
    "for dt in dates_list:\n",
    "    def scrape_news_urls(link):\n",
    "        time.sleep(randint(0, 20))\n",
    "        payload = {'as_epq': 'bitcoin', 'tbs':'cdr:1,cd_min:'+dt+',cd_max:'+dt+\"'\",'tbm':'nws'}\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'}\n",
    "        r = requests.get(link, headers=headers, params=payload)\n",
    "        print(r.status_code)  # Print the status code\n",
    "        content = r.text\n",
    "        news_urls = []\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        news_url = soup.findAll(\"a\", {\"class\": \"l _PMs\"}, href=True)\n",
    "        for url in news_url:\n",
    "            news_urls.append(url['href'])\n",
    "        return news_urls\n",
    "    \n",
    "    URL = 'https://www.google.com/search'\n",
    "    n_url = scrape_news_urls(URL)\n",
    "    article_list = []\n",
    "    \n",
    "    for url in n_url:\n",
    "        if re.match(r'https://www.ft.com/',url) is not None:\n",
    "            from sys import exit\n",
    "            from selenium import webdriver\n",
    "            \n",
    "            # Function to access Financial Times(needs mandatory subscription) through Selenium enabled authentication \n",
    "            def open_browser():\n",
    "                global browser\n",
    "                browser = webdriver.Firefox()\n",
    "                browser.get('https://accounts.ft.com/login?location=https%3A%2F%2Fwww.ft.com%2F')\n",
    "                \n",
    "                username_input = input(\"Please enter your username (email address): \")\n",
    "                username_submit = browser.find_element_by_id(\"email\")\n",
    "                username_submit.send_keys(username_input)\n",
    "                browser.find_element_by_name(\"Next\").click()\n",
    "                \n",
    "                print(\"\"\"Please type in your password, then answer the following:\"\"\")\n",
    "                password_input = input(\"Please enter your password: \")\n",
    "                password_submit = browser.find_element_by_id(\"password\")\n",
    "                password_submit.send_keys(password_input)\n",
    "                confirmation = input(\"Have you typed in your password yet? (y/n): \")\n",
    "                \n",
    "                if confirmation == \"y\":\n",
    "                    browser.find_element_by_name(\"Sign in\").click() #submit\n",
    "                else:\n",
    "                    browser.close()\n",
    "                    exit()\n",
    "            \n",
    "            def article_search(url):\n",
    "                browser.get(url)\n",
    "                art = browser.find_element_by_xpath(\"//*[@id='site-content']/div[3]\")\n",
    "                article_list.append(art.text.rstrip('\\n').replace('\\n', ' '))\n",
    "            \n",
    "            # Uncomment if you are running for the first time to ensure access via authentication \n",
    "            def start(url):\n",
    "#                 choice = input(\"Do you need to log into FT.com? (y/n): \")\n",
    "#                 if choice == \"y\":\n",
    "#                     open_browser()    \n",
    "#                 elif choice == \"n\":\n",
    "                article_search(url)        \n",
    "#                 else:\n",
    "#                     exit()\n",
    "\n",
    "            start(url)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                from newspaper import Article\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article_list.append(article.text)\n",
    "            except:\n",
    "                article_list.append(\"\")\n",
    "\n",
    "    news_dictionary[dt] = article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['01/01/2014', '01/02/2014', '01/03/2014', '01/04/2014', '01/05/2014', '01/06/2014', '01/07/2014', '01/08/2014', '01/09/2014', '01/10/2014', '01/11/2014', '01/12/2014', '01/13/2014', '01/14/2014', '01/15/2014', '01/16/2014', '01/17/2014', '01/18/2014', '01/19/2014', '01/20/2014', '01/21/2014', '01/22/2014', '01/23/2014', '01/24/2014', '01/25/2014', '01/26/2014', '01/27/2014', '01/28/2014', '01/29/2014', '01/30/2014', '01/31/2014', '02/01/2014', '02/02/2014', '02/03/2014', '02/04/2014', '02/05/2014', '02/06/2014', '02/07/2014', '02/08/2014', '02/09/2014', '02/10/2014', '02/11/2014', '02/12/2014', '02/13/2014', '02/14/2014', '02/15/2014', '02/16/2014', '02/17/2014', '02/18/2014', '02/19/2014', '02/20/2014', '02/21/2014', '02/22/2014', '02/23/2014', '02/24/2014', '02/25/2014', '02/26/2014', '02/27/2014', '02/28/2014'])\n"
     ]
    }
   ],
   "source": [
    "# Check if all dates were processed \n",
    "print(news_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MONTH = MARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "Do you need to log into FT.com? (y/n): y\n",
      "Please enter your username (email address): arjun.chndr@gmail.com\n",
      "Please type in your password, then answer the following:\n",
      "Please enter your password: 2F1g5e7E4\n",
      "Have you typed in your password yet? (y/n): y\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "Do you need to log into FT.com? (y/n): n\n",
      "Do you need to log into FT.com? (y/n): n\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "Do you need to log into FT.com? (y/n): n\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "dt = datetime.datetime(2014, 3, 1)\n",
    "end = datetime.datetime(2014, 4, 1)\n",
    "step = datetime.timedelta(days=1)\n",
    "\n",
    "dates_list = []\n",
    "\n",
    "while dt < end:\n",
    "    dates_list.append(dt.strftime('%m/%d/%Y'))\n",
    "    dt += step\n",
    "\n",
    "for dt in dates_list:\n",
    "    def scrape_news_urls(link):\n",
    "        time.sleep(randint(0, 20))\n",
    "        payload = {'as_epq': 'bitcoin', 'tbs':'cdr:1,cd_min:'+dt+',cd_max:'+dt+\"'\",'tbm':'nws'}\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'}\n",
    "        r = requests.get(link, headers=headers, params=payload)\n",
    "        print(r.status_code)  # Print the status code\n",
    "        content = r.text\n",
    "        news_urls = []\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        news_url = soup.findAll(\"a\", {\"class\": \"l _PMs\"}, href=True)\n",
    "        for url in news_url:\n",
    "            news_urls.append(url['href'])\n",
    "        return news_urls\n",
    "    \n",
    "    URL = 'https://www.google.com/search'\n",
    "    n_url = scrape_news_urls(URL)\n",
    "    article_list = []\n",
    "    \n",
    "    for url in n_url:\n",
    "        if re.match(r'https://www.ft.com/',url) is not None:\n",
    "            from sys import exit\n",
    "            from selenium import webdriver\n",
    "            \n",
    "            # Function to access Financial Times(needs mandatory subscription) through Selenium enabled authentication \n",
    "            def open_browser():\n",
    "                global browser\n",
    "                browser = webdriver.Firefox()\n",
    "                browser.get('https://accounts.ft.com/login?location=https%3A%2F%2Fwww.ft.com%2F')\n",
    "                \n",
    "                username_input = input(\"Please enter your username (email address): \")\n",
    "                username_submit = browser.find_element_by_id(\"email\")\n",
    "                username_submit.send_keys(username_input)\n",
    "                browser.find_element_by_name(\"Next\").click()\n",
    "                \n",
    "                print(\"\"\"Please type in your password, then answer the following:\"\"\")\n",
    "                password_input = input(\"Please enter your password: \")\n",
    "                password_submit = browser.find_element_by_id(\"password\")\n",
    "                password_submit.send_keys(password_input)\n",
    "                confirmation = input(\"Have you typed in your password yet? (y/n): \")\n",
    "                \n",
    "                if confirmation == \"y\":\n",
    "                    browser.find_element_by_name(\"Sign in\").click() #submit\n",
    "                else:\n",
    "                    browser.close()\n",
    "                    exit()\n",
    "            \n",
    "            def article_search(url):\n",
    "                browser.get(url)\n",
    "                art = browser.find_element_by_xpath(\"//*[@id='site-content']/div[3]\")\n",
    "                article_list.append(art.text.rstrip('\\n').replace('\\n', ' '))\n",
    "            \n",
    "            # Uncomment if you are running for the first time to ensure access via authentication \n",
    "            def start(url):\n",
    "                choice = input(\"Do you need to log into FT.com? (y/n): \")\n",
    "                if choice == \"y\":\n",
    "                    open_browser()    \n",
    "                elif choice == \"n\":\n",
    "                    article_search(url)        \n",
    "                else:\n",
    "                    exit()\n",
    "\n",
    "            start(url)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                from newspaper import Article\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article_list.append(article.text)\n",
    "            except:\n",
    "                article_list.append(\"\")\n",
    "\n",
    "    news_dictionary[dt] = article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['01/01/2014', '01/02/2014', '01/03/2014', '01/04/2014', '01/05/2014', '01/06/2014', '01/07/2014', '01/08/2014', '01/09/2014', '01/10/2014', '01/11/2014', '01/12/2014', '01/13/2014', '01/14/2014', '01/15/2014', '01/16/2014', '01/17/2014', '01/18/2014', '01/19/2014', '01/20/2014', '01/21/2014', '01/22/2014', '01/23/2014', '01/24/2014', '01/25/2014', '01/26/2014', '01/27/2014', '01/28/2014', '01/29/2014', '01/30/2014', '01/31/2014', '02/01/2014', '02/02/2014', '02/03/2014', '02/04/2014', '02/05/2014', '02/06/2014', '02/07/2014', '02/08/2014', '02/09/2014', '02/10/2014', '02/11/2014', '02/12/2014', '02/13/2014', '02/14/2014', '02/15/2014', '02/16/2014', '02/17/2014', '02/18/2014', '02/19/2014', '02/20/2014', '02/21/2014', '02/22/2014', '02/23/2014', '02/24/2014', '02/25/2014', '02/26/2014', '02/27/2014', '02/28/2014', '03/01/2014', '03/02/2014', '03/03/2014', '03/04/2014', '03/05/2014', '03/06/2014', '03/07/2014', '03/08/2014', '03/09/2014', '03/10/2014', '03/11/2014', '03/12/2014', '03/13/2014', '03/14/2014', '03/15/2014', '03/16/2014', '03/17/2014', '03/18/2014', '03/19/2014', '03/20/2014', '03/21/2014', '03/22/2014', '03/23/2014', '03/24/2014', '03/25/2014', '03/26/2014', '03/27/2014', '03/28/2014', '03/29/2014', '03/30/2014', '03/31/2014'])\n"
     ]
    }
   ],
   "source": [
    "# Check if all dates were processed \n",
    "print(news_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MONTH = APRIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "dt = datetime.datetime(2014, 4, 1)\n",
    "end = datetime.datetime(2014, 5, 1)\n",
    "step = datetime.timedelta(days=1)\n",
    "\n",
    "dates_list = []\n",
    "\n",
    "while dt < end:\n",
    "    dates_list.append(dt.strftime('%m/%d/%Y'))\n",
    "    dt += step\n",
    "\n",
    "for dt in dates_list:\n",
    "    def scrape_news_urls(link):\n",
    "        time.sleep(randint(0, 20))\n",
    "        payload = {'as_epq': 'bitcoin', 'tbs':'cdr:1,cd_min:'+dt+',cd_max:'+dt+\"'\",'tbm':'nws'}\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'}\n",
    "        r = requests.get(link, headers=headers, params=payload)\n",
    "        print(r.status_code)  # Print the status code\n",
    "        content = r.text\n",
    "        news_urls = []\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        news_url = soup.findAll(\"a\", {\"class\": \"l _PMs\"}, href=True)\n",
    "        for url in news_url:\n",
    "            news_urls.append(url['href'])\n",
    "        return news_urls\n",
    "    \n",
    "    URL = 'https://www.google.com/search'\n",
    "    n_url = scrape_news_urls(URL)\n",
    "    article_list = []\n",
    "    \n",
    "    for url in n_url:\n",
    "        if re.match(r'https://www.ft.com/',url) is not None:\n",
    "            from sys import exit\n",
    "            from selenium import webdriver\n",
    "            \n",
    "            # Function to access Financial Times(needs mandatory subscription) through Selenium enabled authentication \n",
    "            def open_browser():\n",
    "                global browser\n",
    "                browser = webdriver.Firefox()\n",
    "                browser.get('https://accounts.ft.com/login?location=https%3A%2F%2Fwww.ft.com%2F')\n",
    "                \n",
    "                username_input = input(\"Please enter your username (email address): \")\n",
    "                username_submit = browser.find_element_by_id(\"email\")\n",
    "                username_submit.send_keys(username_input)\n",
    "                browser.find_element_by_name(\"Next\").click()\n",
    "                \n",
    "                print(\"\"\"Please type in your password, then answer the following:\"\"\")\n",
    "                password_input = input(\"Please enter your password: \")\n",
    "                password_submit = browser.find_element_by_id(\"password\")\n",
    "                password_submit.send_keys(password_input)\n",
    "                confirmation = input(\"Have you typed in your password yet? (y/n): \")\n",
    "                \n",
    "                if confirmation == \"y\":\n",
    "                    browser.find_element_by_name(\"Sign in\").click() #submit\n",
    "                else:\n",
    "                    browser.close()\n",
    "                    exit()\n",
    "            \n",
    "            def article_search(url):\n",
    "                browser.get(url)\n",
    "                art = browser.find_element_by_xpath(\"//*[@id='site-content']/div[3]\")\n",
    "                article_list.append(art.text.rstrip('\\n').replace('\\n', ' '))\n",
    "            \n",
    "            # Uncomment if you are running for the first time to ensure access via authentication \n",
    "            def start(url):\n",
    "#                 choice = input(\"Do you need to log into FT.com? (y/n): \")\n",
    "#                 if choice == \"y\":\n",
    "#                     open_browser()    \n",
    "#                 elif choice == \"n\":\n",
    "                article_search(url)        \n",
    "#                 else:\n",
    "#                     exit()\n",
    "\n",
    "            start(url)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                from newspaper import Article\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article_list.append(article.text)\n",
    "            except:\n",
    "                article_list.append(\"\")\n",
    "\n",
    "    news_dictionary[dt] = article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['01/01/2014', '01/02/2014', '01/03/2014', '01/04/2014', '01/05/2014', '01/06/2014', '01/07/2014', '01/08/2014', '01/09/2014', '01/10/2014', '01/11/2014', '01/12/2014', '01/13/2014', '01/14/2014', '01/15/2014', '01/16/2014', '01/17/2014', '01/18/2014', '01/19/2014', '01/20/2014', '01/21/2014', '01/22/2014', '01/23/2014', '01/24/2014', '01/25/2014', '01/26/2014', '01/27/2014', '01/28/2014', '01/29/2014', '01/30/2014', '01/31/2014', '02/01/2014', '02/02/2014', '02/03/2014', '02/04/2014', '02/05/2014', '02/06/2014', '02/07/2014', '02/08/2014', '02/09/2014', '02/10/2014', '02/11/2014', '02/12/2014', '02/13/2014', '02/14/2014', '02/15/2014', '02/16/2014', '02/17/2014', '02/18/2014', '02/19/2014', '02/20/2014', '02/21/2014', '02/22/2014', '02/23/2014', '02/24/2014', '02/25/2014', '02/26/2014', '02/27/2014', '02/28/2014', '03/01/2014', '03/02/2014', '03/03/2014', '03/04/2014', '03/05/2014', '03/06/2014', '03/07/2014', '03/08/2014', '03/09/2014', '03/10/2014', '03/11/2014', '03/12/2014', '03/13/2014', '03/14/2014', '03/15/2014', '03/16/2014', '03/17/2014', '03/18/2014', '03/19/2014', '03/20/2014', '03/21/2014', '03/22/2014', '03/23/2014', '03/24/2014', '03/25/2014', '03/26/2014', '03/27/2014', '03/28/2014', '03/29/2014', '03/30/2014', '03/31/2014', '04/01/2014', '04/02/2014', '04/03/2014', '04/04/2014', '04/05/2014', '04/06/2014', '04/07/2014', '04/08/2014', '04/09/2014', '04/10/2014', '04/11/2014', '04/12/2014', '04/13/2014', '04/14/2014', '04/15/2014', '04/16/2014', '04/17/2014', '04/18/2014', '04/19/2014', '04/20/2014', '04/21/2014', '04/22/2014', '04/23/2014', '04/24/2014', '04/25/2014', '04/26/2014', '04/27/2014', '04/28/2014', '04/29/2014', '04/30/2014'])\n"
     ]
    }
   ],
   "source": [
    "# Check if all dates were processed \n",
    "print(news_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MONTH = MAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "You must `download()` an article first!\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "dt = datetime.datetime(2014, 5, 1)\n",
    "end = datetime.datetime(2014, 6, 1)\n",
    "step = datetime.timedelta(days=1)\n",
    "\n",
    "dates_list = []\n",
    "\n",
    "while dt < end:\n",
    "    dates_list.append(dt.strftime('%m/%d/%Y'))\n",
    "    dt += step\n",
    "\n",
    "for dt in dates_list:\n",
    "    def scrape_news_urls(link):\n",
    "        time.sleep(randint(0, 20))\n",
    "        payload = {'as_epq': 'bitcoin', 'tbs':'cdr:1,cd_min:'+dt+',cd_max:'+dt+\"'\",'tbm':'nws'}\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'}\n",
    "        r = requests.get(link, headers=headers, params=payload)\n",
    "        print(r.status_code)  # Print the status code\n",
    "        content = r.text\n",
    "        news_urls = []\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        news_url = soup.findAll(\"a\", {\"class\": \"l _PMs\"}, href=True)\n",
    "        for url in news_url:\n",
    "            news_urls.append(url['href'])\n",
    "        return news_urls\n",
    "    \n",
    "    URL = 'https://www.google.com/search'\n",
    "    n_url = scrape_news_urls(URL)\n",
    "    article_list = []\n",
    "    \n",
    "    for url in n_url:\n",
    "        if re.match(r'https://www.ft.com/',url) is not None:\n",
    "            from sys import exit\n",
    "            from selenium import webdriver\n",
    "            \n",
    "            # Function to access Financial Times(needs mandatory subscription) through Selenium enabled authentication \n",
    "            def open_browser():\n",
    "                global browser\n",
    "                browser = webdriver.Firefox()\n",
    "                browser.get('https://accounts.ft.com/login?location=https%3A%2F%2Fwww.ft.com%2F')\n",
    "                \n",
    "                username_input = input(\"Please enter your username (email address): \")\n",
    "                username_submit = browser.find_element_by_id(\"email\")\n",
    "                username_submit.send_keys(username_input)\n",
    "                browser.find_element_by_name(\"Next\").click()\n",
    "                \n",
    "                print(\"\"\"Please type in your password, then answer the following:\"\"\")\n",
    "                password_input = input(\"Please enter your password: \")\n",
    "                password_submit = browser.find_element_by_id(\"password\")\n",
    "                password_submit.send_keys(password_input)\n",
    "                confirmation = input(\"Have you typed in your password yet? (y/n): \")\n",
    "                \n",
    "                if confirmation == \"y\":\n",
    "                    browser.find_element_by_name(\"Sign in\").click() #submit\n",
    "                else:\n",
    "                    browser.close()\n",
    "                    exit()\n",
    "            \n",
    "            def article_search(url):\n",
    "                browser.get(url)\n",
    "                art = browser.find_element_by_xpath(\"//*[@id='site-content']/div[3]\")\n",
    "                article_list.append(art.text.rstrip('\\n').replace('\\n', ' '))\n",
    "            \n",
    "            # Uncomment if you are running for the first time to ensure access via authentication \n",
    "            def start(url):\n",
    "#                 choice = input(\"Do you need to log into FT.com? (y/n): \")\n",
    "#                 if choice == \"y\":\n",
    "#                     open_browser()    \n",
    "#                 elif choice == \"n\":\n",
    "                article_search(url)        \n",
    "#                 else:\n",
    "#                     exit()\n",
    "\n",
    "            start(url)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                from newspaper import Article\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article_list.append(article.text)\n",
    "            except:\n",
    "                article_list.append(\"\")\n",
    "\n",
    "    news_dictionary[dt] = article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['01/01/2014', '01/02/2014', '01/03/2014', '01/04/2014', '01/05/2014', '01/06/2014', '01/07/2014', '01/08/2014', '01/09/2014', '01/10/2014', '01/11/2014', '01/12/2014', '01/13/2014', '01/14/2014', '01/15/2014', '01/16/2014', '01/17/2014', '01/18/2014', '01/19/2014', '01/20/2014', '01/21/2014', '01/22/2014', '01/23/2014', '01/24/2014', '01/25/2014', '01/26/2014', '01/27/2014', '01/28/2014', '01/29/2014', '01/30/2014', '01/31/2014', '02/01/2014', '02/02/2014', '02/03/2014', '02/04/2014', '02/05/2014', '02/06/2014', '02/07/2014', '02/08/2014', '02/09/2014', '02/10/2014', '02/11/2014', '02/12/2014', '02/13/2014', '02/14/2014', '02/15/2014', '02/16/2014', '02/17/2014', '02/18/2014', '02/19/2014', '02/20/2014', '02/21/2014', '02/22/2014', '02/23/2014', '02/24/2014', '02/25/2014', '02/26/2014', '02/27/2014', '02/28/2014', '03/01/2014', '03/02/2014', '03/03/2014', '03/04/2014', '03/05/2014', '03/06/2014', '03/07/2014', '03/08/2014', '03/09/2014', '03/10/2014', '03/11/2014', '03/12/2014', '03/13/2014', '03/14/2014', '03/15/2014', '03/16/2014', '03/17/2014', '03/18/2014', '03/19/2014', '03/20/2014', '03/21/2014', '03/22/2014', '03/23/2014', '03/24/2014', '03/25/2014', '03/26/2014', '03/27/2014', '03/28/2014', '03/29/2014', '03/30/2014', '03/31/2014', '04/01/2014', '04/02/2014', '04/03/2014', '04/04/2014', '04/05/2014', '04/06/2014', '04/07/2014', '04/08/2014', '04/09/2014', '04/10/2014', '04/11/2014', '04/12/2014', '04/13/2014', '04/14/2014', '04/15/2014', '04/16/2014', '04/17/2014', '04/18/2014', '04/19/2014', '04/20/2014', '04/21/2014', '04/22/2014', '04/23/2014', '04/24/2014', '04/25/2014', '04/26/2014', '04/27/2014', '04/28/2014', '04/29/2014', '04/30/2014', '05/01/2014', '05/02/2014', '05/03/2014', '05/04/2014', '05/05/2014', '05/06/2014', '05/07/2014', '05/08/2014', '05/09/2014', '05/10/2014', '05/11/2014', '05/12/2014', '05/13/2014', '05/14/2014', '05/15/2014', '05/16/2014', '05/17/2014', '05/18/2014', '05/19/2014', '05/20/2014', '05/21/2014', '05/22/2014', '05/23/2014', '05/24/2014', '05/25/2014', '05/26/2014', '05/27/2014', '05/28/2014', '05/29/2014', '05/30/2014', '05/31/2014'])\n"
     ]
    }
   ],
   "source": [
    "# Check if all dates were processed \n",
    "print(news_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MONTH = JUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "Article `download()` failed with 403 Client Error: Forbidden for url: https://seekingalpha.com/article/2247873-an-alternate-way-to-profit-off-bitcoin on URL https://seekingalpha.com/article/2247873-an-alternate-way-to-profit-off-bitcoin\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "You must `download()` an article first!\n"
     ]
    }
   ],
   "source": [
    "dt = datetime.datetime(2014, 6, 1)\n",
    "end = datetime.datetime(2014, 7, 1)\n",
    "step = datetime.timedelta(days=1)\n",
    "\n",
    "dates_list = []\n",
    "\n",
    "while dt < end:\n",
    "    dates_list.append(dt.strftime('%m/%d/%Y'))\n",
    "    dt += step\n",
    "\n",
    "for dt in dates_list:\n",
    "    def scrape_news_urls(link):\n",
    "        time.sleep(randint(0, 20))\n",
    "        payload = {'as_epq': 'bitcoin', 'tbs':'cdr:1,cd_min:'+dt+',cd_max:'+dt+\"'\",'tbm':'nws'}\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'}\n",
    "        r = requests.get(link, headers=headers, params=payload)\n",
    "        print(r.status_code)  # Print the status code\n",
    "        content = r.text\n",
    "        news_urls = []\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        news_url = soup.findAll(\"a\", {\"class\": \"l _PMs\"}, href=True)\n",
    "        for url in news_url:\n",
    "            news_urls.append(url['href'])\n",
    "        return news_urls\n",
    "    \n",
    "    URL = 'https://www.google.com/search'\n",
    "    n_url = scrape_news_urls(URL)\n",
    "    article_list = []\n",
    "    \n",
    "    for url in n_url:\n",
    "        if re.match(r'https://www.ft.com/',url) is not None:\n",
    "            from sys import exit\n",
    "            from selenium import webdriver\n",
    "            \n",
    "            # Function to access Financial Times(needs mandatory subscription) through Selenium enabled authentication \n",
    "            def open_browser():\n",
    "                global browser\n",
    "                browser = webdriver.Firefox()\n",
    "                browser.get('https://accounts.ft.com/login?location=https%3A%2F%2Fwww.ft.com%2F')\n",
    "                \n",
    "                username_input = input(\"Please enter your username (email address): \")\n",
    "                username_submit = browser.find_element_by_id(\"email\")\n",
    "                username_submit.send_keys(username_input)\n",
    "                browser.find_element_by_name(\"Next\").click()\n",
    "                \n",
    "                print(\"\"\"Please type in your password, then answer the following:\"\"\")\n",
    "                password_input = input(\"Please enter your password: \")\n",
    "                password_submit = browser.find_element_by_id(\"password\")\n",
    "                password_submit.send_keys(password_input)\n",
    "                confirmation = input(\"Have you typed in your password yet? (y/n): \")\n",
    "                \n",
    "                if confirmation == \"y\":\n",
    "                    browser.find_element_by_name(\"Sign in\").click() #submit\n",
    "                else:\n",
    "                    browser.close()\n",
    "                    exit()\n",
    "            \n",
    "            def article_search(url):\n",
    "                browser.get(url)\n",
    "                art = browser.find_element_by_xpath(\"//*[@id='site-content']/div[3]\")\n",
    "                article_list.append(art.text.rstrip('\\n').replace('\\n', ' '))\n",
    "            \n",
    "            # Uncomment if you are running for the first time to ensure access via authentication \n",
    "            def start(url):\n",
    "#                 choice = input(\"Do you need to log into FT.com? (y/n): \")\n",
    "#                 if choice == \"y\":\n",
    "#                     open_browser()    \n",
    "#                 elif choice == \"n\":\n",
    "                article_search(url)        \n",
    "#                 else:\n",
    "#                     exit()\n",
    "\n",
    "            start(url)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                from newspaper import Article\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article_list.append(article.text)\n",
    "            except:\n",
    "                article_list.append(\"\")\n",
    "\n",
    "    news_dictionary[dt] = article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['01/01/2014', '01/02/2014', '01/03/2014', '01/04/2014', '01/05/2014', '01/06/2014', '01/07/2014', '01/08/2014', '01/09/2014', '01/10/2014', '01/11/2014', '01/12/2014', '01/13/2014', '01/14/2014', '01/15/2014', '01/16/2014', '01/17/2014', '01/18/2014', '01/19/2014', '01/20/2014', '01/21/2014', '01/22/2014', '01/23/2014', '01/24/2014', '01/25/2014', '01/26/2014', '01/27/2014', '01/28/2014', '01/29/2014', '01/30/2014', '01/31/2014', '02/01/2014', '02/02/2014', '02/03/2014', '02/04/2014', '02/05/2014', '02/06/2014', '02/07/2014', '02/08/2014', '02/09/2014', '02/10/2014', '02/11/2014', '02/12/2014', '02/13/2014', '02/14/2014', '02/15/2014', '02/16/2014', '02/17/2014', '02/18/2014', '02/19/2014', '02/20/2014', '02/21/2014', '02/22/2014', '02/23/2014', '02/24/2014', '02/25/2014', '02/26/2014', '02/27/2014', '02/28/2014', '03/01/2014', '03/02/2014', '03/03/2014', '03/04/2014', '03/05/2014', '03/06/2014', '03/07/2014', '03/08/2014', '03/09/2014', '03/10/2014', '03/11/2014', '03/12/2014', '03/13/2014', '03/14/2014', '03/15/2014', '03/16/2014', '03/17/2014', '03/18/2014', '03/19/2014', '03/20/2014', '03/21/2014', '03/22/2014', '03/23/2014', '03/24/2014', '03/25/2014', '03/26/2014', '03/27/2014', '03/28/2014', '03/29/2014', '03/30/2014', '03/31/2014', '04/01/2014', '04/02/2014', '04/03/2014', '04/04/2014', '04/05/2014', '04/06/2014', '04/07/2014', '04/08/2014', '04/09/2014', '04/10/2014', '04/11/2014', '04/12/2014', '04/13/2014', '04/14/2014', '04/15/2014', '04/16/2014', '04/17/2014', '04/18/2014', '04/19/2014', '04/20/2014', '04/21/2014', '04/22/2014', '04/23/2014', '04/24/2014', '04/25/2014', '04/26/2014', '04/27/2014', '04/28/2014', '04/29/2014', '04/30/2014', '05/01/2014', '05/02/2014', '05/03/2014', '05/04/2014', '05/05/2014', '05/06/2014', '05/07/2014', '05/08/2014', '05/09/2014', '05/10/2014', '05/11/2014', '05/12/2014', '05/13/2014', '05/14/2014', '05/15/2014', '05/16/2014', '05/17/2014', '05/18/2014', '05/19/2014', '05/20/2014', '05/21/2014', '05/22/2014', '05/23/2014', '05/24/2014', '05/25/2014', '05/26/2014', '05/27/2014', '05/28/2014', '05/29/2014', '05/30/2014', '05/31/2014', '06/01/2014', '06/02/2014', '06/03/2014', '06/04/2014', '06/05/2014', '06/06/2014', '06/07/2014', '06/08/2014', '06/09/2014', '06/10/2014', '06/11/2014', '06/12/2014', '06/13/2014', '06/14/2014', '06/15/2014', '06/16/2014', '06/17/2014', '06/18/2014', '06/19/2014', '06/20/2014', '06/21/2014', '06/22/2014', '06/23/2014', '06/24/2014', '06/25/2014', '06/26/2014', '06/27/2014', '06/28/2014', '06/29/2014', '06/30/2014'])\n"
     ]
    }
   ],
   "source": [
    "# Check if all dates were processed \n",
    "print(news_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MONTH = JULY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "You must `download()` an article first!\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "dt = datetime.datetime(2014, 7, 1)\n",
    "end = datetime.datetime(2014, 8, 1)\n",
    "step = datetime.timedelta(days=1)\n",
    "\n",
    "dates_list = []\n",
    "\n",
    "while dt < end:\n",
    "    dates_list.append(dt.strftime('%m/%d/%Y'))\n",
    "    dt += step\n",
    "\n",
    "for dt in dates_list:\n",
    "    def scrape_news_urls(link):\n",
    "        time.sleep(randint(0, 20))\n",
    "        payload = {'as_epq': 'bitcoin', 'tbs':'cdr:1,cd_min:'+dt+',cd_max:'+dt+\"'\",'tbm':'nws'}\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'}\n",
    "        r = requests.get(link, headers=headers, params=payload)\n",
    "        print(r.status_code)  # Print the status code\n",
    "        content = r.text\n",
    "        news_urls = []\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        news_url = soup.findAll(\"a\", {\"class\": \"l _PMs\"}, href=True)\n",
    "        for url in news_url:\n",
    "            news_urls.append(url['href'])\n",
    "        return news_urls\n",
    "    \n",
    "    URL = 'https://www.google.com/search'\n",
    "    n_url = scrape_news_urls(URL)\n",
    "    article_list = []\n",
    "    \n",
    "    for url in n_url:\n",
    "        if re.match(r'https://www.ft.com/',url) is not None:\n",
    "            from sys import exit\n",
    "            from selenium import webdriver\n",
    "            \n",
    "            # Function to access Financial Times(needs mandatory subscription) through Selenium enabled authentication \n",
    "            def open_browser():\n",
    "                global browser\n",
    "                browser = webdriver.Firefox()\n",
    "                browser.get('https://accounts.ft.com/login?location=https%3A%2F%2Fwww.ft.com%2F')\n",
    "                \n",
    "                username_input = input(\"Please enter your username (email address): \")\n",
    "                username_submit = browser.find_element_by_id(\"email\")\n",
    "                username_submit.send_keys(username_input)\n",
    "                browser.find_element_by_name(\"Next\").click()\n",
    "                \n",
    "                print(\"\"\"Please type in your password, then answer the following:\"\"\")\n",
    "                password_input = input(\"Please enter your password: \")\n",
    "                password_submit = browser.find_element_by_id(\"password\")\n",
    "                password_submit.send_keys(password_input)\n",
    "                confirmation = input(\"Have you typed in your password yet? (y/n): \")\n",
    "                \n",
    "                if confirmation == \"y\":\n",
    "                    browser.find_element_by_name(\"Sign in\").click() #submit\n",
    "                else:\n",
    "                    browser.close()\n",
    "                    exit()\n",
    "            \n",
    "            def article_search(url):\n",
    "                browser.get(url)\n",
    "                art = browser.find_element_by_xpath(\"//*[@id='site-content']/div[3]\")\n",
    "                article_list.append(art.text.rstrip('\\n').replace('\\n', ' '))\n",
    "            \n",
    "            # Uncomment if you are running for the first time to ensure access via authentication \n",
    "            def start(url):\n",
    "#                 choice = input(\"Do you need to log into FT.com? (y/n): \")\n",
    "#                 if choice == \"y\":\n",
    "#                     open_browser()    \n",
    "#                 elif choice == \"n\":\n",
    "                article_search(url)        \n",
    "#                 else:\n",
    "#                     exit()\n",
    "\n",
    "            start(url)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                from newspaper import Article\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article_list.append(article.text)\n",
    "            except:\n",
    "                article_list.append(\"\")\n",
    "\n",
    "    news_dictionary[dt] = article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['01/01/2014', '01/02/2014', '01/03/2014', '01/04/2014', '01/05/2014', '01/06/2014', '01/07/2014', '01/08/2014', '01/09/2014', '01/10/2014', '01/11/2014', '01/12/2014', '01/13/2014', '01/14/2014', '01/15/2014', '01/16/2014', '01/17/2014', '01/18/2014', '01/19/2014', '01/20/2014', '01/21/2014', '01/22/2014', '01/23/2014', '01/24/2014', '01/25/2014', '01/26/2014', '01/27/2014', '01/28/2014', '01/29/2014', '01/30/2014', '01/31/2014', '02/01/2014', '02/02/2014', '02/03/2014', '02/04/2014', '02/05/2014', '02/06/2014', '02/07/2014', '02/08/2014', '02/09/2014', '02/10/2014', '02/11/2014', '02/12/2014', '02/13/2014', '02/14/2014', '02/15/2014', '02/16/2014', '02/17/2014', '02/18/2014', '02/19/2014', '02/20/2014', '02/21/2014', '02/22/2014', '02/23/2014', '02/24/2014', '02/25/2014', '02/26/2014', '02/27/2014', '02/28/2014', '03/01/2014', '03/02/2014', '03/03/2014', '03/04/2014', '03/05/2014', '03/06/2014', '03/07/2014', '03/08/2014', '03/09/2014', '03/10/2014', '03/11/2014', '03/12/2014', '03/13/2014', '03/14/2014', '03/15/2014', '03/16/2014', '03/17/2014', '03/18/2014', '03/19/2014', '03/20/2014', '03/21/2014', '03/22/2014', '03/23/2014', '03/24/2014', '03/25/2014', '03/26/2014', '03/27/2014', '03/28/2014', '03/29/2014', '03/30/2014', '03/31/2014', '04/01/2014', '04/02/2014', '04/03/2014', '04/04/2014', '04/05/2014', '04/06/2014', '04/07/2014', '04/08/2014', '04/09/2014', '04/10/2014', '04/11/2014', '04/12/2014', '04/13/2014', '04/14/2014', '04/15/2014', '04/16/2014', '04/17/2014', '04/18/2014', '04/19/2014', '04/20/2014', '04/21/2014', '04/22/2014', '04/23/2014', '04/24/2014', '04/25/2014', '04/26/2014', '04/27/2014', '04/28/2014', '04/29/2014', '04/30/2014', '05/01/2014', '05/02/2014', '05/03/2014', '05/04/2014', '05/05/2014', '05/06/2014', '05/07/2014', '05/08/2014', '05/09/2014', '05/10/2014', '05/11/2014', '05/12/2014', '05/13/2014', '05/14/2014', '05/15/2014', '05/16/2014', '05/17/2014', '05/18/2014', '05/19/2014', '05/20/2014', '05/21/2014', '05/22/2014', '05/23/2014', '05/24/2014', '05/25/2014', '05/26/2014', '05/27/2014', '05/28/2014', '05/29/2014', '05/30/2014', '05/31/2014', '06/01/2014', '06/02/2014', '06/03/2014', '06/04/2014', '06/05/2014', '06/06/2014', '06/07/2014', '06/08/2014', '06/09/2014', '06/10/2014', '06/11/2014', '06/12/2014', '06/13/2014', '06/14/2014', '06/15/2014', '06/16/2014', '06/17/2014', '06/18/2014', '06/19/2014', '06/20/2014', '06/21/2014', '06/22/2014', '06/23/2014', '06/24/2014', '06/25/2014', '06/26/2014', '06/27/2014', '06/28/2014', '06/29/2014', '06/30/2014', '07/01/2014', '07/02/2014', '07/03/2014', '07/04/2014', '07/05/2014', '07/06/2014', '07/07/2014', '07/08/2014', '07/09/2014', '07/10/2014', '07/11/2014', '07/12/2014', '07/13/2014', '07/14/2014', '07/15/2014', '07/16/2014', '07/17/2014', '07/18/2014', '07/19/2014', '07/20/2014', '07/21/2014', '07/22/2014', '07/23/2014', '07/24/2014', '07/25/2014', '07/26/2014', '07/27/2014', '07/28/2014', '07/29/2014', '07/30/2014', '07/31/2014'])\n"
     ]
    }
   ],
   "source": [
    "# Check if all dates were processed \n",
    "print(news_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MONTH = AUGUST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "Article `download()` failed with 503 Server Error: Service Temporarily Unavailable for url: http://www.moneylife.in/article/can-bitcoins-remain-unregulated/38388.html on URL http://www.moneylife.in/article/can-bitcoins-remain-unregulated/38388.html\n",
      "Article `download()` failed with 403 Client Error: Forbidden for url: https://www.androidheadlines.com/2014/08/rushwallet-web-based-bitcoin-wallet-works-platform.html on URL https://www.androidheadlines.com/2014/08/rushwallet-web-based-bitcoin-wallet-works-platform.html\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "You must `download()` an article first!\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "Article `download()` failed with 403 Client Error: Forbidden for url: https://www.androidheadlines.com/2014/08/knc-wallet-bitcoin-gets-updated-easy-person-person-payments.html on URL https://www.androidheadlines.com/2014/08/knc-wallet-bitcoin-gets-updated-easy-person-person-payments.html\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "dt = datetime.datetime(2014, 8, 1)\n",
    "end = datetime.datetime(2014, 9, 1)\n",
    "step = datetime.timedelta(days=1)\n",
    "\n",
    "dates_list = []\n",
    "\n",
    "while dt < end:\n",
    "    dates_list.append(dt.strftime('%m/%d/%Y'))\n",
    "    dt += step\n",
    "\n",
    "for dt in dates_list:\n",
    "    def scrape_news_urls(link):\n",
    "        time.sleep(randint(0, 20))\n",
    "        payload = {'as_epq': 'bitcoin', 'tbs':'cdr:1,cd_min:'+dt+',cd_max:'+dt+\"'\",'tbm':'nws'}\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'}\n",
    "        r = requests.get(link, headers=headers, params=payload)\n",
    "        print(r.status_code)  # Print the status code\n",
    "        content = r.text\n",
    "        news_urls = []\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        news_url = soup.findAll(\"a\", {\"class\": \"l _PMs\"}, href=True)\n",
    "        for url in news_url:\n",
    "            news_urls.append(url['href'])\n",
    "        return news_urls\n",
    "    \n",
    "    URL = 'https://www.google.com/search'\n",
    "    n_url = scrape_news_urls(URL)\n",
    "    article_list = []\n",
    "    \n",
    "    for url in n_url:\n",
    "        if re.match(r'https://www.ft.com/',url) is not None:\n",
    "            from sys import exit\n",
    "            from selenium import webdriver\n",
    "            \n",
    "            # Function to access Financial Times(needs mandatory subscription) through Selenium enabled authentication \n",
    "            def open_browser():\n",
    "                global browser\n",
    "                browser = webdriver.Firefox()\n",
    "                browser.get('https://accounts.ft.com/login?location=https%3A%2F%2Fwww.ft.com%2F')\n",
    "                \n",
    "                username_input = input(\"Please enter your username (email address): \")\n",
    "                username_submit = browser.find_element_by_id(\"email\")\n",
    "                username_submit.send_keys(username_input)\n",
    "                browser.find_element_by_name(\"Next\").click()\n",
    "                \n",
    "                print(\"\"\"Please type in your password, then answer the following:\"\"\")\n",
    "                password_input = input(\"Please enter your password: \")\n",
    "                password_submit = browser.find_element_by_id(\"password\")\n",
    "                password_submit.send_keys(password_input)\n",
    "                confirmation = input(\"Have you typed in your password yet? (y/n): \")\n",
    "                \n",
    "                if confirmation == \"y\":\n",
    "                    browser.find_element_by_name(\"Sign in\").click() #submit\n",
    "                else:\n",
    "                    browser.close()\n",
    "                    exit()\n",
    "            \n",
    "            def article_search(url):\n",
    "                browser.get(url)\n",
    "                art = browser.find_element_by_xpath(\"//*[@id='site-content']/div[3]\")\n",
    "                article_list.append(art.text.rstrip('\\n').replace('\\n', ' '))\n",
    "            \n",
    "            # Uncomment if you are running for the first time to ensure access via authentication \n",
    "            def start(url):\n",
    "#                 choice = input(\"Do you need to log into FT.com? (y/n): \")\n",
    "#                 if choice == \"y\":\n",
    "#                     open_browser()    \n",
    "#                 elif choice == \"n\":\n",
    "                article_search(url)        \n",
    "#                 else:\n",
    "#                     exit()\n",
    "\n",
    "            start(url)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                from newspaper import Article\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article_list.append(article.text)\n",
    "            except:\n",
    "                article_list.append(\"\")\n",
    "\n",
    "    news_dictionary[dt] = article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['01/01/2014', '01/02/2014', '01/03/2014', '01/04/2014', '01/05/2014', '01/06/2014', '01/07/2014', '01/08/2014', '01/09/2014', '01/10/2014', '01/11/2014', '01/12/2014', '01/13/2014', '01/14/2014', '01/15/2014', '01/16/2014', '01/17/2014', '01/18/2014', '01/19/2014', '01/20/2014', '01/21/2014', '01/22/2014', '01/23/2014', '01/24/2014', '01/25/2014', '01/26/2014', '01/27/2014', '01/28/2014', '01/29/2014', '01/30/2014', '01/31/2014', '02/01/2014', '02/02/2014', '02/03/2014', '02/04/2014', '02/05/2014', '02/06/2014', '02/07/2014', '02/08/2014', '02/09/2014', '02/10/2014', '02/11/2014', '02/12/2014', '02/13/2014', '02/14/2014', '02/15/2014', '02/16/2014', '02/17/2014', '02/18/2014', '02/19/2014', '02/20/2014', '02/21/2014', '02/22/2014', '02/23/2014', '02/24/2014', '02/25/2014', '02/26/2014', '02/27/2014', '02/28/2014', '03/01/2014', '03/02/2014', '03/03/2014', '03/04/2014', '03/05/2014', '03/06/2014', '03/07/2014', '03/08/2014', '03/09/2014', '03/10/2014', '03/11/2014', '03/12/2014', '03/13/2014', '03/14/2014', '03/15/2014', '03/16/2014', '03/17/2014', '03/18/2014', '03/19/2014', '03/20/2014', '03/21/2014', '03/22/2014', '03/23/2014', '03/24/2014', '03/25/2014', '03/26/2014', '03/27/2014', '03/28/2014', '03/29/2014', '03/30/2014', '03/31/2014', '04/01/2014', '04/02/2014', '04/03/2014', '04/04/2014', '04/05/2014', '04/06/2014', '04/07/2014', '04/08/2014', '04/09/2014', '04/10/2014', '04/11/2014', '04/12/2014', '04/13/2014', '04/14/2014', '04/15/2014', '04/16/2014', '04/17/2014', '04/18/2014', '04/19/2014', '04/20/2014', '04/21/2014', '04/22/2014', '04/23/2014', '04/24/2014', '04/25/2014', '04/26/2014', '04/27/2014', '04/28/2014', '04/29/2014', '04/30/2014', '05/01/2014', '05/02/2014', '05/03/2014', '05/04/2014', '05/05/2014', '05/06/2014', '05/07/2014', '05/08/2014', '05/09/2014', '05/10/2014', '05/11/2014', '05/12/2014', '05/13/2014', '05/14/2014', '05/15/2014', '05/16/2014', '05/17/2014', '05/18/2014', '05/19/2014', '05/20/2014', '05/21/2014', '05/22/2014', '05/23/2014', '05/24/2014', '05/25/2014', '05/26/2014', '05/27/2014', '05/28/2014', '05/29/2014', '05/30/2014', '05/31/2014', '06/01/2014', '06/02/2014', '06/03/2014', '06/04/2014', '06/05/2014', '06/06/2014', '06/07/2014', '06/08/2014', '06/09/2014', '06/10/2014', '06/11/2014', '06/12/2014', '06/13/2014', '06/14/2014', '06/15/2014', '06/16/2014', '06/17/2014', '06/18/2014', '06/19/2014', '06/20/2014', '06/21/2014', '06/22/2014', '06/23/2014', '06/24/2014', '06/25/2014', '06/26/2014', '06/27/2014', '06/28/2014', '06/29/2014', '06/30/2014', '07/01/2014', '07/02/2014', '07/03/2014', '07/04/2014', '07/05/2014', '07/06/2014', '07/07/2014', '07/08/2014', '07/09/2014', '07/10/2014', '07/11/2014', '07/12/2014', '07/13/2014', '07/14/2014', '07/15/2014', '07/16/2014', '07/17/2014', '07/18/2014', '07/19/2014', '07/20/2014', '07/21/2014', '07/22/2014', '07/23/2014', '07/24/2014', '07/25/2014', '07/26/2014', '07/27/2014', '07/28/2014', '07/29/2014', '07/30/2014', '07/31/2014', '08/01/2014', '08/02/2014', '08/03/2014', '08/04/2014', '08/05/2014', '08/06/2014', '08/07/2014', '08/08/2014', '08/09/2014', '08/10/2014', '08/11/2014', '08/12/2014', '08/13/2014', '08/14/2014', '08/15/2014', '08/16/2014', '08/17/2014', '08/18/2014', '08/19/2014', '08/20/2014', '08/21/2014', '08/22/2014', '08/23/2014', '08/24/2014', '08/25/2014', '08/26/2014', '08/27/2014', '08/28/2014', '08/29/2014', '08/30/2014', '08/31/2014'])\n"
     ]
    }
   ],
   "source": [
    "# Check if all dates were processed \n",
    "print(news_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MONTH = SEPTEMBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "You must `download()` an article first!\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "You must `download()` an article first!\n",
      "200\n",
      "You must `download()` an article first!\n",
      "You must `download()` an article first!\n",
      "You must `download()` an article first!\n"
     ]
    }
   ],
   "source": [
    "dt = datetime.datetime(2014, 9, 1)\n",
    "end = datetime.datetime(2014, 10, 1)\n",
    "step = datetime.timedelta(days=1)\n",
    "\n",
    "dates_list = []\n",
    "\n",
    "while dt < end:\n",
    "    dates_list.append(dt.strftime('%m/%d/%Y'))\n",
    "    dt += step\n",
    "\n",
    "for dt in dates_list:\n",
    "    def scrape_news_urls(link):\n",
    "        time.sleep(randint(0, 20))\n",
    "        payload = {'as_epq': 'bitcoin', 'tbs':'cdr:1,cd_min:'+dt+',cd_max:'+dt+\"'\",'tbm':'nws'}\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'}\n",
    "        r = requests.get(link, headers=headers, params=payload)\n",
    "        print(r.status_code)  # Print the status code\n",
    "        content = r.text\n",
    "        news_urls = []\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        news_url = soup.findAll(\"a\", {\"class\": \"l _PMs\"}, href=True)\n",
    "        for url in news_url:\n",
    "            news_urls.append(url['href'])\n",
    "        return news_urls\n",
    "    \n",
    "    URL = 'https://www.google.com/search'\n",
    "    n_url = scrape_news_urls(URL)\n",
    "    article_list = []\n",
    "    \n",
    "    for url in n_url:\n",
    "        if re.match(r'https://www.ft.com/',url) is not None:\n",
    "            from sys import exit\n",
    "            from selenium import webdriver\n",
    "            \n",
    "            # Function to access Financial Times(needs mandatory subscription) through Selenium enabled authentication \n",
    "            def open_browser():\n",
    "                global browser\n",
    "                browser = webdriver.Firefox()\n",
    "                browser.get('https://accounts.ft.com/login?location=https%3A%2F%2Fwww.ft.com%2F')\n",
    "                \n",
    "                username_input = input(\"Please enter your username (email address): \")\n",
    "                username_submit = browser.find_element_by_id(\"email\")\n",
    "                username_submit.send_keys(username_input)\n",
    "                browser.find_element_by_name(\"Next\").click()\n",
    "                \n",
    "                print(\"\"\"Please type in your password, then answer the following:\"\"\")\n",
    "                password_input = input(\"Please enter your password: \")\n",
    "                password_submit = browser.find_element_by_id(\"password\")\n",
    "                password_submit.send_keys(password_input)\n",
    "                confirmation = input(\"Have you typed in your password yet? (y/n): \")\n",
    "                \n",
    "                if confirmation == \"y\":\n",
    "                    browser.find_element_by_name(\"Sign in\").click() #submit\n",
    "                else:\n",
    "                    browser.close()\n",
    "                    exit()\n",
    "            \n",
    "            def article_search(url):\n",
    "                browser.get(url)\n",
    "                art = browser.find_element_by_xpath(\"//*[@id='site-content']/div[3]\")\n",
    "                article_list.append(art.text.rstrip('\\n').replace('\\n', ' '))\n",
    "            \n",
    "            # Uncomment if you are running for the first time to ensure access via authentication \n",
    "            def start(url):\n",
    "#                 choice = input(\"Do you need to log into FT.com? (y/n): \")\n",
    "#                 if choice == \"y\":\n",
    "#                     open_browser()    \n",
    "#                 elif choice == \"n\":\n",
    "                article_search(url)        \n",
    "#                 else:\n",
    "#                     exit()\n",
    "\n",
    "            start(url)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                from newspaper import Article\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article_list.append(article.text)\n",
    "            except:\n",
    "                article_list.append(\"\")\n",
    "\n",
    "    news_dictionary[dt] = article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['01/01/2014', '01/02/2014', '01/03/2014', '01/04/2014', '01/05/2014', '01/06/2014', '01/07/2014', '01/08/2014', '01/09/2014', '01/10/2014', '01/11/2014', '01/12/2014', '01/13/2014', '01/14/2014', '01/15/2014', '01/16/2014', '01/17/2014', '01/18/2014', '01/19/2014', '01/20/2014', '01/21/2014', '01/22/2014', '01/23/2014', '01/24/2014', '01/25/2014', '01/26/2014', '01/27/2014', '01/28/2014', '01/29/2014', '01/30/2014', '01/31/2014', '02/01/2014', '02/02/2014', '02/03/2014', '02/04/2014', '02/05/2014', '02/06/2014', '02/07/2014', '02/08/2014', '02/09/2014', '02/10/2014', '02/11/2014', '02/12/2014', '02/13/2014', '02/14/2014', '02/15/2014', '02/16/2014', '02/17/2014', '02/18/2014', '02/19/2014', '02/20/2014', '02/21/2014', '02/22/2014', '02/23/2014', '02/24/2014', '02/25/2014', '02/26/2014', '02/27/2014', '02/28/2014', '03/01/2014', '03/02/2014', '03/03/2014', '03/04/2014', '03/05/2014', '03/06/2014', '03/07/2014', '03/08/2014', '03/09/2014', '03/10/2014', '03/11/2014', '03/12/2014', '03/13/2014', '03/14/2014', '03/15/2014', '03/16/2014', '03/17/2014', '03/18/2014', '03/19/2014', '03/20/2014', '03/21/2014', '03/22/2014', '03/23/2014', '03/24/2014', '03/25/2014', '03/26/2014', '03/27/2014', '03/28/2014', '03/29/2014', '03/30/2014', '03/31/2014', '04/01/2014', '04/02/2014', '04/03/2014', '04/04/2014', '04/05/2014', '04/06/2014', '04/07/2014', '04/08/2014', '04/09/2014', '04/10/2014', '04/11/2014', '04/12/2014', '04/13/2014', '04/14/2014', '04/15/2014', '04/16/2014', '04/17/2014', '04/18/2014', '04/19/2014', '04/20/2014', '04/21/2014', '04/22/2014', '04/23/2014', '04/24/2014', '04/25/2014', '04/26/2014', '04/27/2014', '04/28/2014', '04/29/2014', '04/30/2014', '05/01/2014', '05/02/2014', '05/03/2014', '05/04/2014', '05/05/2014', '05/06/2014', '05/07/2014', '05/08/2014', '05/09/2014', '05/10/2014', '05/11/2014', '05/12/2014', '05/13/2014', '05/14/2014', '05/15/2014', '05/16/2014', '05/17/2014', '05/18/2014', '05/19/2014', '05/20/2014', '05/21/2014', '05/22/2014', '05/23/2014', '05/24/2014', '05/25/2014', '05/26/2014', '05/27/2014', '05/28/2014', '05/29/2014', '05/30/2014', '05/31/2014', '06/01/2014', '06/02/2014', '06/03/2014', '06/04/2014', '06/05/2014', '06/06/2014', '06/07/2014', '06/08/2014', '06/09/2014', '06/10/2014', '06/11/2014', '06/12/2014', '06/13/2014', '06/14/2014', '06/15/2014', '06/16/2014', '06/17/2014', '06/18/2014', '06/19/2014', '06/20/2014', '06/21/2014', '06/22/2014', '06/23/2014', '06/24/2014', '06/25/2014', '06/26/2014', '06/27/2014', '06/28/2014', '06/29/2014', '06/30/2014', '07/01/2014', '07/02/2014', '07/03/2014', '07/04/2014', '07/05/2014', '07/06/2014', '07/07/2014', '07/08/2014', '07/09/2014', '07/10/2014', '07/11/2014', '07/12/2014', '07/13/2014', '07/14/2014', '07/15/2014', '07/16/2014', '07/17/2014', '07/18/2014', '07/19/2014', '07/20/2014', '07/21/2014', '07/22/2014', '07/23/2014', '07/24/2014', '07/25/2014', '07/26/2014', '07/27/2014', '07/28/2014', '07/29/2014', '07/30/2014', '07/31/2014', '08/01/2014', '08/02/2014', '08/03/2014', '08/04/2014', '08/05/2014', '08/06/2014', '08/07/2014', '08/08/2014', '08/09/2014', '08/10/2014', '08/11/2014', '08/12/2014', '08/13/2014', '08/14/2014', '08/15/2014', '08/16/2014', '08/17/2014', '08/18/2014', '08/19/2014', '08/20/2014', '08/21/2014', '08/22/2014', '08/23/2014', '08/24/2014', '08/25/2014', '08/26/2014', '08/27/2014', '08/28/2014', '08/29/2014', '08/30/2014', '08/31/2014', '09/01/2014', '09/02/2014', '09/03/2014', '09/04/2014', '09/05/2014', '09/06/2014', '09/07/2014', '09/08/2014', '09/09/2014', '09/10/2014', '09/11/2014', '09/12/2014', '09/13/2014', '09/14/2014', '09/15/2014', '09/16/2014', '09/17/2014', '09/18/2014', '09/19/2014', '09/20/2014', '09/21/2014', '09/22/2014', '09/23/2014', '09/24/2014', '09/25/2014', '09/26/2014', '09/27/2014', '09/28/2014', '09/29/2014', '09/30/2014'])\n"
     ]
    }
   ],
   "source": [
    "# Check if all dates were processed \n",
    "print(news_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MONTH = OCTOBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "dt = datetime.datetime(2014, 10, 1)\n",
    "end = datetime.datetime(2014, 11, 1)\n",
    "step = datetime.timedelta(days=1)\n",
    "\n",
    "dates_list = []\n",
    "\n",
    "while dt < end:\n",
    "    dates_list.append(dt.strftime('%m/%d/%Y'))\n",
    "    dt += step\n",
    "\n",
    "for dt in dates_list:\n",
    "    def scrape_news_urls(link):\n",
    "        time.sleep(randint(0, 20))\n",
    "        payload = {'as_epq': 'bitcoin', 'tbs':'cdr:1,cd_min:'+dt+',cd_max:'+dt+\"'\",'tbm':'nws'}\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'}\n",
    "        r = requests.get(link, headers=headers, params=payload)\n",
    "        print(r.status_code)  # Print the status code\n",
    "        content = r.text\n",
    "        news_urls = []\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        news_url = soup.findAll(\"a\", {\"class\": \"l _PMs\"}, href=True)\n",
    "        for url in news_url:\n",
    "            news_urls.append(url['href'])\n",
    "        return news_urls\n",
    "    \n",
    "    URL = 'https://www.google.com/search'\n",
    "    n_url = scrape_news_urls(URL)\n",
    "    article_list = []\n",
    "    \n",
    "    for url in n_url:\n",
    "        if re.match(r'https://www.ft.com/',url) is not None:\n",
    "            from sys import exit\n",
    "            from selenium import webdriver\n",
    "            \n",
    "            # Function to access Financial Times(needs mandatory subscription) through Selenium enabled authentication \n",
    "            def open_browser():\n",
    "                global browser\n",
    "                browser = webdriver.Firefox()\n",
    "                browser.get('https://accounts.ft.com/login?location=https%3A%2F%2Fwww.ft.com%2F')\n",
    "                \n",
    "                username_input = input(\"Please enter your username (email address): \")\n",
    "                username_submit = browser.find_element_by_id(\"email\")\n",
    "                username_submit.send_keys(username_input)\n",
    "                browser.find_element_by_name(\"Next\").click()\n",
    "                \n",
    "                print(\"\"\"Please type in your password, then answer the following:\"\"\")\n",
    "                password_input = input(\"Please enter your password: \")\n",
    "                password_submit = browser.find_element_by_id(\"password\")\n",
    "                password_submit.send_keys(password_input)\n",
    "                confirmation = input(\"Have you typed in your password yet? (y/n): \")\n",
    "                \n",
    "                if confirmation == \"y\":\n",
    "                    browser.find_element_by_name(\"Sign in\").click() #submit\n",
    "                else:\n",
    "                    browser.close()\n",
    "                    exit()\n",
    "            \n",
    "            def article_search(url):\n",
    "                browser.get(url)\n",
    "                art = browser.find_element_by_xpath(\"//*[@id='site-content']/div[3]\")\n",
    "                article_list.append(art.text.rstrip('\\n').replace('\\n', ' '))\n",
    "            \n",
    "            # Uncomment if you are running for the first time to ensure access via authentication \n",
    "            def start(url):\n",
    "#                 choice = input(\"Do you need to log into FT.com? (y/n): \")\n",
    "#                 if choice == \"y\":\n",
    "#                     open_browser()    \n",
    "#                 elif choice == \"n\":\n",
    "                article_search(url)        \n",
    "#                 else:\n",
    "#                     exit()\n",
    "\n",
    "            start(url)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                from newspaper import Article\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article_list.append(article.text)\n",
    "            except:\n",
    "                article_list.append(\"\")\n",
    "\n",
    "    news_dictionary[dt] = article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['01/01/2014', '01/02/2014', '01/03/2014', '01/04/2014', '01/05/2014', '01/06/2014', '01/07/2014', '01/08/2014', '01/09/2014', '01/10/2014', '01/11/2014', '01/12/2014', '01/13/2014', '01/14/2014', '01/15/2014', '01/16/2014', '01/17/2014', '01/18/2014', '01/19/2014', '01/20/2014', '01/21/2014', '01/22/2014', '01/23/2014', '01/24/2014', '01/25/2014', '01/26/2014', '01/27/2014', '01/28/2014', '01/29/2014', '01/30/2014', '01/31/2014', '02/01/2014', '02/02/2014', '02/03/2014', '02/04/2014', '02/05/2014', '02/06/2014', '02/07/2014', '02/08/2014', '02/09/2014', '02/10/2014', '02/11/2014', '02/12/2014', '02/13/2014', '02/14/2014', '02/15/2014', '02/16/2014', '02/17/2014', '02/18/2014', '02/19/2014', '02/20/2014', '02/21/2014', '02/22/2014', '02/23/2014', '02/24/2014', '02/25/2014', '02/26/2014', '02/27/2014', '02/28/2014', '03/01/2014', '03/02/2014', '03/03/2014', '03/04/2014', '03/05/2014', '03/06/2014', '03/07/2014', '03/08/2014', '03/09/2014', '03/10/2014', '03/11/2014', '03/12/2014', '03/13/2014', '03/14/2014', '03/15/2014', '03/16/2014', '03/17/2014', '03/18/2014', '03/19/2014', '03/20/2014', '03/21/2014', '03/22/2014', '03/23/2014', '03/24/2014', '03/25/2014', '03/26/2014', '03/27/2014', '03/28/2014', '03/29/2014', '03/30/2014', '03/31/2014', '04/01/2014', '04/02/2014', '04/03/2014', '04/04/2014', '04/05/2014', '04/06/2014', '04/07/2014', '04/08/2014', '04/09/2014', '04/10/2014', '04/11/2014', '04/12/2014', '04/13/2014', '04/14/2014', '04/15/2014', '04/16/2014', '04/17/2014', '04/18/2014', '04/19/2014', '04/20/2014', '04/21/2014', '04/22/2014', '04/23/2014', '04/24/2014', '04/25/2014', '04/26/2014', '04/27/2014', '04/28/2014', '04/29/2014', '04/30/2014', '05/01/2014', '05/02/2014', '05/03/2014', '05/04/2014', '05/05/2014', '05/06/2014', '05/07/2014', '05/08/2014', '05/09/2014', '05/10/2014', '05/11/2014', '05/12/2014', '05/13/2014', '05/14/2014', '05/15/2014', '05/16/2014', '05/17/2014', '05/18/2014', '05/19/2014', '05/20/2014', '05/21/2014', '05/22/2014', '05/23/2014', '05/24/2014', '05/25/2014', '05/26/2014', '05/27/2014', '05/28/2014', '05/29/2014', '05/30/2014', '05/31/2014', '06/01/2014', '06/02/2014', '06/03/2014', '06/04/2014', '06/05/2014', '06/06/2014', '06/07/2014', '06/08/2014', '06/09/2014', '06/10/2014', '06/11/2014', '06/12/2014', '06/13/2014', '06/14/2014', '06/15/2014', '06/16/2014', '06/17/2014', '06/18/2014', '06/19/2014', '06/20/2014', '06/21/2014', '06/22/2014', '06/23/2014', '06/24/2014', '06/25/2014', '06/26/2014', '06/27/2014', '06/28/2014', '06/29/2014', '06/30/2014', '07/01/2014', '07/02/2014', '07/03/2014', '07/04/2014', '07/05/2014', '07/06/2014', '07/07/2014', '07/08/2014', '07/09/2014', '07/10/2014', '07/11/2014', '07/12/2014', '07/13/2014', '07/14/2014', '07/15/2014', '07/16/2014', '07/17/2014', '07/18/2014', '07/19/2014', '07/20/2014', '07/21/2014', '07/22/2014', '07/23/2014', '07/24/2014', '07/25/2014', '07/26/2014', '07/27/2014', '07/28/2014', '07/29/2014', '07/30/2014', '07/31/2014', '08/01/2014', '08/02/2014', '08/03/2014', '08/04/2014', '08/05/2014', '08/06/2014', '08/07/2014', '08/08/2014', '08/09/2014', '08/10/2014', '08/11/2014', '08/12/2014', '08/13/2014', '08/14/2014', '08/15/2014', '08/16/2014', '08/17/2014', '08/18/2014', '08/19/2014', '08/20/2014', '08/21/2014', '08/22/2014', '08/23/2014', '08/24/2014', '08/25/2014', '08/26/2014', '08/27/2014', '08/28/2014', '08/29/2014', '08/30/2014', '08/31/2014', '09/01/2014', '09/02/2014', '09/03/2014', '09/04/2014', '09/05/2014', '09/06/2014', '09/07/2014', '09/08/2014', '09/09/2014', '09/10/2014', '09/11/2014', '09/12/2014', '09/13/2014', '09/14/2014', '09/15/2014', '09/16/2014', '09/17/2014', '09/18/2014', '09/19/2014', '09/20/2014', '09/21/2014', '09/22/2014', '09/23/2014', '09/24/2014', '09/25/2014', '09/26/2014', '09/27/2014', '09/28/2014', '09/29/2014', '09/30/2014', '10/01/2014', '10/02/2014', '10/03/2014', '10/04/2014', '10/05/2014', '10/06/2014', '10/07/2014', '10/08/2014', '10/09/2014', '10/10/2014', '10/11/2014', '10/12/2014', '10/13/2014', '10/14/2014', '10/15/2014', '10/16/2014', '10/17/2014', '10/18/2014', '10/19/2014', '10/20/2014', '10/21/2014', '10/22/2014', '10/23/2014', '10/24/2014', '10/25/2014', '10/26/2014', '10/27/2014', '10/28/2014', '10/29/2014', '10/30/2014', '10/31/2014'])\n"
     ]
    }
   ],
   "source": [
    "# Check if all dates were processed \n",
    "print(news_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MONTH = NOVEMBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "Do you need to log into FT.com? (y/n): y\n",
      "Please enter your username (email address): arjun.chndr@gmail.com\n",
      "Please type in your password, then answer the following:\n",
      "Please enter your password: 2F1g5e7E4\n",
      "Have you typed in your password yet? (y/n): y\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "Do you need to log into FT.com? (y/n): n\n",
      "Do you need to log into FT.com? (y/n): n\n",
      "Do you need to log into FT.com? (y/n): n\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "Do you need to log into FT.com? (y/n): n\n",
      "Do you need to log into FT.com? (y/n): n\n",
      "Do you need to log into FT.com? (y/n): n\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "Do you need to log into FT.com? (y/n): n\n",
      "Do you need to log into FT.com? (y/n): n\n",
      "Do you need to log into FT.com? (y/n): n\n",
      "200\n",
      "Do you need to log into FT.com? (y/n): n\n",
      "Do you need to log into FT.com? (y/n): n\n"
     ]
    }
   ],
   "source": [
    "dt = datetime.datetime(2014, 11, 1)\n",
    "end = datetime.datetime(2014, 12, 1)\n",
    "step = datetime.timedelta(days=1)\n",
    "\n",
    "dates_list = []\n",
    "\n",
    "while dt < end:\n",
    "    dates_list.append(dt.strftime('%m/%d/%Y'))\n",
    "    dt += step\n",
    "\n",
    "for dt in dates_list:\n",
    "    def scrape_news_urls(link):\n",
    "        time.sleep(randint(0, 20))\n",
    "        payload = {'as_epq': 'bitcoin', 'tbs':'cdr:1,cd_min:'+dt+',cd_max:'+dt+\"'\",'tbm':'nws'}\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'}\n",
    "        r = requests.get(link, headers=headers, params=payload)\n",
    "        print(r.status_code)  # Print the status code\n",
    "        content = r.text\n",
    "        news_urls = []\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        news_url = soup.findAll(\"a\", {\"class\": \"l _PMs\"}, href=True)\n",
    "        for url in news_url:\n",
    "            news_urls.append(url['href'])\n",
    "        return news_urls\n",
    "    \n",
    "    URL = 'https://www.google.com/search'\n",
    "    n_url = scrape_news_urls(URL)\n",
    "    article_list = []\n",
    "    \n",
    "    for url in n_url:\n",
    "        if re.match(r'https://www.ft.com/',url) is not None:\n",
    "            from sys import exit\n",
    "            from selenium import webdriver\n",
    "            \n",
    "            # Function to access Financial Times(needs mandatory subscription) through Selenium enabled authentication \n",
    "            def open_browser():\n",
    "                global browser\n",
    "                browser = webdriver.Firefox()\n",
    "                browser.get('https://accounts.ft.com/login?location=https%3A%2F%2Fwww.ft.com%2F')\n",
    "                \n",
    "                username_input = input(\"Please enter your username (email address): \")\n",
    "                username_submit = browser.find_element_by_id(\"email\")\n",
    "                username_submit.send_keys(username_input)\n",
    "                browser.find_element_by_name(\"Next\").click()\n",
    "                \n",
    "                print(\"\"\"Please type in your password, then answer the following:\"\"\")\n",
    "                password_input = input(\"Please enter your password: \")\n",
    "                password_submit = browser.find_element_by_id(\"password\")\n",
    "                password_submit.send_keys(password_input)\n",
    "                confirmation = input(\"Have you typed in your password yet? (y/n): \")\n",
    "                \n",
    "                if confirmation == \"y\":\n",
    "                    browser.find_element_by_name(\"Sign in\").click() #submit\n",
    "                else:\n",
    "                    browser.close()\n",
    "                    exit()\n",
    "            \n",
    "            def article_search(url):\n",
    "                browser.get(url)\n",
    "                art = browser.find_element_by_xpath(\"//*[@id='site-content']/div[3]\")\n",
    "                article_list.append(art.text.rstrip('\\n').replace('\\n', ' '))\n",
    "            \n",
    "            # Uncomment if you are running for the first time to ensure access via authentication \n",
    "            def start(url):\n",
    "                choice = input(\"Do you need to log into FT.com? (y/n): \")\n",
    "                if choice == \"y\":\n",
    "                    open_browser()    \n",
    "                elif choice == \"n\":\n",
    "                    article_search(url)        \n",
    "                else:\n",
    "                    exit()\n",
    "\n",
    "            start(url)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                from newspaper import Article\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article_list.append(article.text)\n",
    "            except:\n",
    "                article_list.append(\"\")\n",
    "\n",
    "    news_dictionary[dt] = article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['01/01/2014', '01/02/2014', '01/03/2014', '01/04/2014', '01/05/2014', '01/06/2014', '01/07/2014', '01/08/2014', '01/09/2014', '01/10/2014', '01/11/2014', '01/12/2014', '01/13/2014', '01/14/2014', '01/15/2014', '01/16/2014', '01/17/2014', '01/18/2014', '01/19/2014', '01/20/2014', '01/21/2014', '01/22/2014', '01/23/2014', '01/24/2014', '01/25/2014', '01/26/2014', '01/27/2014', '01/28/2014', '01/29/2014', '01/30/2014', '01/31/2014', '02/01/2014', '02/02/2014', '02/03/2014', '02/04/2014', '02/05/2014', '02/06/2014', '02/07/2014', '02/08/2014', '02/09/2014', '02/10/2014', '02/11/2014', '02/12/2014', '02/13/2014', '02/14/2014', '02/15/2014', '02/16/2014', '02/17/2014', '02/18/2014', '02/19/2014', '02/20/2014', '02/21/2014', '02/22/2014', '02/23/2014', '02/24/2014', '02/25/2014', '02/26/2014', '02/27/2014', '02/28/2014', '03/01/2014', '03/02/2014', '03/03/2014', '03/04/2014', '03/05/2014', '03/06/2014', '03/07/2014', '03/08/2014', '03/09/2014', '03/10/2014', '03/11/2014', '03/12/2014', '03/13/2014', '03/14/2014', '03/15/2014', '03/16/2014', '03/17/2014', '03/18/2014', '03/19/2014', '03/20/2014', '03/21/2014', '03/22/2014', '03/23/2014', '03/24/2014', '03/25/2014', '03/26/2014', '03/27/2014', '03/28/2014', '03/29/2014', '03/30/2014', '03/31/2014', '04/01/2014', '04/02/2014', '04/03/2014', '04/04/2014', '04/05/2014', '04/06/2014', '04/07/2014', '04/08/2014', '04/09/2014', '04/10/2014', '04/11/2014', '04/12/2014', '04/13/2014', '04/14/2014', '04/15/2014', '04/16/2014', '04/17/2014', '04/18/2014', '04/19/2014', '04/20/2014', '04/21/2014', '04/22/2014', '04/23/2014', '04/24/2014', '04/25/2014', '04/26/2014', '04/27/2014', '04/28/2014', '04/29/2014', '04/30/2014', '05/01/2014', '05/02/2014', '05/03/2014', '05/04/2014', '05/05/2014', '05/06/2014', '05/07/2014', '05/08/2014', '05/09/2014', '05/10/2014', '05/11/2014', '05/12/2014', '05/13/2014', '05/14/2014', '05/15/2014', '05/16/2014', '05/17/2014', '05/18/2014', '05/19/2014', '05/20/2014', '05/21/2014', '05/22/2014', '05/23/2014', '05/24/2014', '05/25/2014', '05/26/2014', '05/27/2014', '05/28/2014', '05/29/2014', '05/30/2014', '05/31/2014', '06/01/2014', '06/02/2014', '06/03/2014', '06/04/2014', '06/05/2014', '06/06/2014', '06/07/2014', '06/08/2014', '06/09/2014', '06/10/2014', '06/11/2014', '06/12/2014', '06/13/2014', '06/14/2014', '06/15/2014', '06/16/2014', '06/17/2014', '06/18/2014', '06/19/2014', '06/20/2014', '06/21/2014', '06/22/2014', '06/23/2014', '06/24/2014', '06/25/2014', '06/26/2014', '06/27/2014', '06/28/2014', '06/29/2014', '06/30/2014', '07/01/2014', '07/02/2014', '07/03/2014', '07/04/2014', '07/05/2014', '07/06/2014', '07/07/2014', '07/08/2014', '07/09/2014', '07/10/2014', '07/11/2014', '07/12/2014', '07/13/2014', '07/14/2014', '07/15/2014', '07/16/2014', '07/17/2014', '07/18/2014', '07/19/2014', '07/20/2014', '07/21/2014', '07/22/2014', '07/23/2014', '07/24/2014', '07/25/2014', '07/26/2014', '07/27/2014', '07/28/2014', '07/29/2014', '07/30/2014', '07/31/2014', '08/01/2014', '08/02/2014', '08/03/2014', '08/04/2014', '08/05/2014', '08/06/2014', '08/07/2014', '08/08/2014', '08/09/2014', '08/10/2014', '08/11/2014', '08/12/2014', '08/13/2014', '08/14/2014', '08/15/2014', '08/16/2014', '08/17/2014', '08/18/2014', '08/19/2014', '08/20/2014', '08/21/2014', '08/22/2014', '08/23/2014', '08/24/2014', '08/25/2014', '08/26/2014', '08/27/2014', '08/28/2014', '08/29/2014', '08/30/2014', '08/31/2014', '09/01/2014', '09/02/2014', '09/03/2014', '09/04/2014', '09/05/2014', '09/06/2014', '09/07/2014', '09/08/2014', '09/09/2014', '09/10/2014', '09/11/2014', '09/12/2014', '09/13/2014', '09/14/2014', '09/15/2014', '09/16/2014', '09/17/2014', '09/18/2014', '09/19/2014', '09/20/2014', '09/21/2014', '09/22/2014', '09/23/2014', '09/24/2014', '09/25/2014', '09/26/2014', '09/27/2014', '09/28/2014', '09/29/2014', '09/30/2014', '10/01/2014', '10/02/2014', '10/03/2014', '10/04/2014', '10/05/2014', '10/06/2014', '10/07/2014', '10/08/2014', '10/09/2014', '10/10/2014', '10/11/2014', '10/12/2014', '10/13/2014', '10/14/2014', '10/15/2014', '10/16/2014', '10/17/2014', '10/18/2014', '10/19/2014', '10/20/2014', '10/21/2014', '10/22/2014', '10/23/2014', '10/24/2014', '10/25/2014', '10/26/2014', '10/27/2014', '10/28/2014', '10/29/2014', '10/30/2014', '10/31/2014', '11/01/2014', '11/02/2014', '11/03/2014', '11/04/2014', '11/05/2014', '11/06/2014', '11/07/2014', '11/08/2014', '11/09/2014', '11/10/2014', '11/11/2014', '11/12/2014', '11/13/2014', '11/14/2014', '11/15/2014', '11/16/2014', '11/17/2014', '11/18/2014', '11/19/2014', '11/20/2014', '11/21/2014', '11/22/2014', '11/23/2014', '11/24/2014', '11/25/2014', '11/26/2014', '11/27/2014', '11/28/2014', '11/29/2014', '11/30/2014'])\n"
     ]
    }
   ],
   "source": [
    "# Check if all dates were processed \n",
    "print(news_dictionary.keys())\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# dates_14 = list(news_dictionary.keys())\n",
    "# articleslist_14 = list(news_dictionary.values())\n",
    "\n",
    "# df_14 = pd.DataFrame({'Date':dates_14})\n",
    "# df_14['Articles'] = articleslist_14\n",
    "# df_14.to_csv('articles_2014_till_oct.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MONTH = DECEMBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "dt = datetime.datetime(2014, 12, 1)\n",
    "end = datetime.datetime(2015, 1, 1)\n",
    "step = datetime.timedelta(days=1)\n",
    "\n",
    "dates_list = []\n",
    "\n",
    "while dt < end:\n",
    "    dates_list.append(dt.strftime('%m/%d/%Y'))\n",
    "    dt += step\n",
    "\n",
    "for dt in dates_list:\n",
    "    def scrape_news_urls(link):\n",
    "        time.sleep(randint(0, 20))\n",
    "        payload = {'as_epq': 'bitcoin', 'tbs':'cdr:1,cd_min:'+dt+',cd_max:'+dt+\"'\",'tbm':'nws'}\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'}\n",
    "        r = requests.get(link, headers=headers, params=payload)\n",
    "        print(r.status_code)  # Print the status code\n",
    "        content = r.text\n",
    "        news_urls = []\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        news_url = soup.findAll(\"a\", {\"class\": \"l _PMs\"}, href=True)\n",
    "        for url in news_url:\n",
    "            news_urls.append(url['href'])\n",
    "        return news_urls\n",
    "    \n",
    "    URL = 'https://www.google.com/search'\n",
    "    n_url = scrape_news_urls(URL)\n",
    "    article_list = []\n",
    "    \n",
    "    for url in n_url:\n",
    "        if re.match(r'https://www.ft.com/',url) is not None:\n",
    "            from sys import exit\n",
    "            from selenium import webdriver\n",
    "            \n",
    "            # Function to access Financial Times(needs mandatory subscription) through Selenium enabled authentication \n",
    "            def open_browser():\n",
    "                global browser\n",
    "                browser = webdriver.Firefox()\n",
    "                browser.get('https://accounts.ft.com/login?location=https%3A%2F%2Fwww.ft.com%2F')\n",
    "                \n",
    "                username_input = input(\"Please enter your username (email address): \")\n",
    "                username_submit = browser.find_element_by_id(\"email\")\n",
    "                username_submit.send_keys(username_input)\n",
    "                browser.find_element_by_name(\"Next\").click()\n",
    "                \n",
    "                print(\"\"\"Please type in your password, then answer the following:\"\"\")\n",
    "                password_input = input(\"Please enter your password: \")\n",
    "                password_submit = browser.find_element_by_id(\"password\")\n",
    "                password_submit.send_keys(password_input)\n",
    "                confirmation = input(\"Have you typed in your password yet? (y/n): \")\n",
    "                \n",
    "                if confirmation == \"y\":\n",
    "                    browser.find_element_by_name(\"Sign in\").click() #submit\n",
    "                else:\n",
    "                    browser.close()\n",
    "                    exit()\n",
    "            \n",
    "            def article_search(url):\n",
    "                browser.get(url)\n",
    "                art = browser.find_element_by_xpath(\"//*[@id='site-content']/div[3]\")\n",
    "                article_list.append(art.text.rstrip('\\n').replace('\\n', ' '))\n",
    "            \n",
    "            # Uncomment if you are running for the first time to ensure access via authentication \n",
    "            def start(url):\n",
    "#                 choice = input(\"Do you need to log into FT.com? (y/n): \")\n",
    "#                 if choice == \"y\":\n",
    "#                     open_browser()    \n",
    "#                 elif choice == \"n\":\n",
    "                article_search(url)        \n",
    "#                 else:\n",
    "#                     exit()\n",
    "\n",
    "            start(url)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                from newspaper import Article\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article_list.append(article.text)\n",
    "            except:\n",
    "                article_list.append(\"\")\n",
    "\n",
    "    news_dictionary[dt] = article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['01/01/2014', '01/02/2014', '01/03/2014', '01/04/2014', '01/05/2014', '01/06/2014', '01/07/2014', '01/08/2014', '01/09/2014', '01/10/2014', '01/11/2014', '01/12/2014', '01/13/2014', '01/14/2014', '01/15/2014', '01/16/2014', '01/17/2014', '01/18/2014', '01/19/2014', '01/20/2014', '01/21/2014', '01/22/2014', '01/23/2014', '01/24/2014', '01/25/2014', '01/26/2014', '01/27/2014', '01/28/2014', '01/29/2014', '01/30/2014', '01/31/2014', '02/01/2014', '02/02/2014', '02/03/2014', '02/04/2014', '02/05/2014', '02/06/2014', '02/07/2014', '02/08/2014', '02/09/2014', '02/10/2014', '02/11/2014', '02/12/2014', '02/13/2014', '02/14/2014', '02/15/2014', '02/16/2014', '02/17/2014', '02/18/2014', '02/19/2014', '02/20/2014', '02/21/2014', '02/22/2014', '02/23/2014', '02/24/2014', '02/25/2014', '02/26/2014', '02/27/2014', '02/28/2014', '03/01/2014', '03/02/2014', '03/03/2014', '03/04/2014', '03/05/2014', '03/06/2014', '03/07/2014', '03/08/2014', '03/09/2014', '03/10/2014', '03/11/2014', '03/12/2014', '03/13/2014', '03/14/2014', '03/15/2014', '03/16/2014', '03/17/2014', '03/18/2014', '03/19/2014', '03/20/2014', '03/21/2014', '03/22/2014', '03/23/2014', '03/24/2014', '03/25/2014', '03/26/2014', '03/27/2014', '03/28/2014', '03/29/2014', '03/30/2014', '03/31/2014', '04/01/2014', '04/02/2014', '04/03/2014', '04/04/2014', '04/05/2014', '04/06/2014', '04/07/2014', '04/08/2014', '04/09/2014', '04/10/2014', '04/11/2014', '04/12/2014', '04/13/2014', '04/14/2014', '04/15/2014', '04/16/2014', '04/17/2014', '04/18/2014', '04/19/2014', '04/20/2014', '04/21/2014', '04/22/2014', '04/23/2014', '04/24/2014', '04/25/2014', '04/26/2014', '04/27/2014', '04/28/2014', '04/29/2014', '04/30/2014', '05/01/2014', '05/02/2014', '05/03/2014', '05/04/2014', '05/05/2014', '05/06/2014', '05/07/2014', '05/08/2014', '05/09/2014', '05/10/2014', '05/11/2014', '05/12/2014', '05/13/2014', '05/14/2014', '05/15/2014', '05/16/2014', '05/17/2014', '05/18/2014', '05/19/2014', '05/20/2014', '05/21/2014', '05/22/2014', '05/23/2014', '05/24/2014', '05/25/2014', '05/26/2014', '05/27/2014', '05/28/2014', '05/29/2014', '05/30/2014', '05/31/2014', '06/01/2014', '06/02/2014', '06/03/2014', '06/04/2014', '06/05/2014', '06/06/2014', '06/07/2014', '06/08/2014', '06/09/2014', '06/10/2014', '06/11/2014', '06/12/2014', '06/13/2014', '06/14/2014', '06/15/2014', '06/16/2014', '06/17/2014', '06/18/2014', '06/19/2014', '06/20/2014', '06/21/2014', '06/22/2014', '06/23/2014', '06/24/2014', '06/25/2014', '06/26/2014', '06/27/2014', '06/28/2014', '06/29/2014', '06/30/2014', '07/01/2014', '07/02/2014', '07/03/2014', '07/04/2014', '07/05/2014', '07/06/2014', '07/07/2014', '07/08/2014', '07/09/2014', '07/10/2014', '07/11/2014', '07/12/2014', '07/13/2014', '07/14/2014', '07/15/2014', '07/16/2014', '07/17/2014', '07/18/2014', '07/19/2014', '07/20/2014', '07/21/2014', '07/22/2014', '07/23/2014', '07/24/2014', '07/25/2014', '07/26/2014', '07/27/2014', '07/28/2014', '07/29/2014', '07/30/2014', '07/31/2014', '08/01/2014', '08/02/2014', '08/03/2014', '08/04/2014', '08/05/2014', '08/06/2014', '08/07/2014', '08/08/2014', '08/09/2014', '08/10/2014', '08/11/2014', '08/12/2014', '08/13/2014', '08/14/2014', '08/15/2014', '08/16/2014', '08/17/2014', '08/18/2014', '08/19/2014', '08/20/2014', '08/21/2014', '08/22/2014', '08/23/2014', '08/24/2014', '08/25/2014', '08/26/2014', '08/27/2014', '08/28/2014', '08/29/2014', '08/30/2014', '08/31/2014', '09/01/2014', '09/02/2014', '09/03/2014', '09/04/2014', '09/05/2014', '09/06/2014', '09/07/2014', '09/08/2014', '09/09/2014', '09/10/2014', '09/11/2014', '09/12/2014', '09/13/2014', '09/14/2014', '09/15/2014', '09/16/2014', '09/17/2014', '09/18/2014', '09/19/2014', '09/20/2014', '09/21/2014', '09/22/2014', '09/23/2014', '09/24/2014', '09/25/2014', '09/26/2014', '09/27/2014', '09/28/2014', '09/29/2014', '09/30/2014', '10/01/2014', '10/02/2014', '10/03/2014', '10/04/2014', '10/05/2014', '10/06/2014', '10/07/2014', '10/08/2014', '10/09/2014', '10/10/2014', '10/11/2014', '10/12/2014', '10/13/2014', '10/14/2014', '10/15/2014', '10/16/2014', '10/17/2014', '10/18/2014', '10/19/2014', '10/20/2014', '10/21/2014', '10/22/2014', '10/23/2014', '10/24/2014', '10/25/2014', '10/26/2014', '10/27/2014', '10/28/2014', '10/29/2014', '10/30/2014', '10/31/2014', '11/01/2014', '11/02/2014', '11/03/2014', '11/04/2014', '11/05/2014', '11/06/2014', '11/07/2014', '11/08/2014', '11/09/2014', '11/10/2014', '11/11/2014', '11/12/2014', '11/13/2014', '11/14/2014', '11/15/2014', '11/16/2014', '11/17/2014', '11/18/2014', '11/19/2014', '11/20/2014', '11/21/2014', '11/22/2014', '11/23/2014', '11/24/2014', '11/25/2014', '11/26/2014', '11/27/2014', '11/28/2014', '11/29/2014', '11/30/2014', '12/01/2014', '12/02/2014', '12/03/2014', '12/04/2014', '12/05/2014', '12/06/2014', '12/07/2014', '12/08/2014', '12/09/2014', '12/10/2014', '12/11/2014', '12/12/2014', '12/13/2014', '12/14/2014', '12/15/2014', '12/16/2014', '12/17/2014', '12/18/2014', '12/19/2014', '12/20/2014', '12/21/2014', '12/22/2014', '12/23/2014', '12/24/2014', '12/25/2014', '12/26/2014', '12/27/2014', '12/28/2014', '12/29/2014', '12/30/2014', '12/31/2014'])\n"
     ]
    }
   ],
   "source": [
    "# Check if all dates were processed \n",
    "print(news_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><a id='articles_2014_csv'>Read dictionary into Pandas dataframe and write results to CSV</a></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dates_14 = list(news_dictionary.keys())\n",
    "articleslist_14 = list(news_dictionary.values())\n",
    "\n",
    "df_14 = pd.DataFrame({'Date':dates_14})\n",
    "df_14['Articles'] = articleslist_14\n",
    "df_14.to_csv('articles_2014.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YEAR = 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MONTH = JANUARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "dt = datetime.datetime(2015, 1, 1)\n",
    "end = datetime.datetime(2015, 2, 1)\n",
    "step = datetime.timedelta(days=1)\n",
    "\n",
    "dates_list = []\n",
    "\n",
    "while dt < end:\n",
    "    dates_list.append(dt.strftime('%m/%d/%Y'))\n",
    "    dt += step\n",
    "\n",
    "for dt in dates_list:\n",
    "    def scrape_news_urls(link):\n",
    "        time.sleep(randint(0, 20))\n",
    "        payload = {'as_epq': 'bitcoin', 'tbs':'cdr:1,cd_min:'+dt+',cd_max:'+dt+\"'\",'tbm':'nws'}\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'}\n",
    "        r = requests.get(link, headers=headers, params=payload)\n",
    "        print(r.status_code)  # Print the status code\n",
    "        content = r.text\n",
    "        news_urls = []\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        news_url = soup.findAll(\"a\", {\"class\": \"l _PMs\"}, href=True)\n",
    "        for url in news_url:\n",
    "            news_urls.append(url['href'])\n",
    "        return news_urls\n",
    "    \n",
    "    URL = 'https://www.google.com/search'\n",
    "    n_url = scrape_news_urls(URL)\n",
    "    article_list = []\n",
    "    \n",
    "    for url in n_url:\n",
    "        if re.match(r'https://www.ft.com/',url) is not None:\n",
    "            from sys import exit\n",
    "            from selenium import webdriver\n",
    "            \n",
    "            # Function to access Financial Times(needs mandatory subscription) through Selenium enabled authentication \n",
    "            def open_browser():\n",
    "                global browser\n",
    "                browser = webdriver.Firefox()\n",
    "                browser.get('https://accounts.ft.com/login?location=https%3A%2F%2Fwww.ft.com%2F')\n",
    "                \n",
    "                username_input = input(\"Please enter your username (email address): \")\n",
    "                username_submit = browser.find_element_by_id(\"email\")\n",
    "                username_submit.send_keys(username_input)\n",
    "                browser.find_element_by_name(\"Next\").click()\n",
    "                \n",
    "                print(\"\"\"Please type in your password, then answer the following:\"\"\")\n",
    "                password_input = input(\"Please enter your password: \")\n",
    "                password_submit = browser.find_element_by_id(\"password\")\n",
    "                password_submit.send_keys(password_input)\n",
    "                confirmation = input(\"Have you typed in your password yet? (y/n): \")\n",
    "                \n",
    "                if confirmation == \"y\":\n",
    "                    browser.find_element_by_name(\"Sign in\").click() #submit\n",
    "                else:\n",
    "                    browser.close()\n",
    "                    exit()\n",
    "            \n",
    "            def article_search(url):\n",
    "                browser.get(url)\n",
    "                art = browser.find_element_by_xpath(\"//*[@id='site-content']/div[3]\")\n",
    "                article_list.append(art.text.rstrip('\\n').replace('\\n', ' '))\n",
    "            \n",
    "            # Uncomment if you are running for the first time to ensure access via authentication \n",
    "            def start(url):\n",
    "#                 choice = input(\"Do you need to log into FT.com? (y/n): \")\n",
    "#                 if choice == \"y\":\n",
    "#                     open_browser()    \n",
    "#                 elif choice == \"n\":\n",
    "                article_search(url)        \n",
    "#                 else:\n",
    "#                     exit()\n",
    "\n",
    "            start(url)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                from newspaper import Article\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article_list.append(article.text)\n",
    "            except:\n",
    "                article_list.append(\"\")\n",
    "\n",
    "    news_dictionary[dt] = article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all dates were processed \n",
    "print(news_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MONTH = FEBRUARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.datetime(2015, 2, 1)\n",
    "end = datetime.datetime(2015, 3, 1)\n",
    "step = datetime.timedelta(days=1)\n",
    "\n",
    "dates_list = []\n",
    "\n",
    "while dt < end:\n",
    "    dates_list.append(dt.strftime('%m/%d/%Y'))\n",
    "    dt += step\n",
    "\n",
    "for dt in dates_list:\n",
    "    def scrape_news_urls(link):\n",
    "        time.sleep(randint(0, 20))\n",
    "        payload = {'as_epq': 'bitcoin', 'tbs':'cdr:1,cd_min:'+dt+',cd_max:'+dt+\"'\",'tbm':'nws'}\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'}\n",
    "        r = requests.get(link, headers=headers, params=payload)\n",
    "        print(r.status_code)  # Print the status code\n",
    "        content = r.text\n",
    "        news_urls = []\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        news_url = soup.findAll(\"a\", {\"class\": \"l _PMs\"}, href=True)\n",
    "        for url in news_url:\n",
    "            news_urls.append(url['href'])\n",
    "        return news_urls\n",
    "    \n",
    "    URL = 'https://www.google.com/search'\n",
    "    n_url = scrape_news_urls(URL)\n",
    "    article_list = []\n",
    "    \n",
    "    for url in n_url:\n",
    "        if re.match(r'https://www.ft.com/',url) is not None:\n",
    "            from sys import exit\n",
    "            from selenium import webdriver\n",
    "            \n",
    "            # Function to access Financial Times(needs mandatory subscription) through Selenium enabled authentication \n",
    "            def open_browser():\n",
    "                global browser\n",
    "                browser = webdriver.Firefox()\n",
    "                browser.get('https://accounts.ft.com/login?location=https%3A%2F%2Fwww.ft.com%2F')\n",
    "                \n",
    "                username_input = input(\"Please enter your username (email address): \")\n",
    "                username_submit = browser.find_element_by_id(\"email\")\n",
    "                username_submit.send_keys(username_input)\n",
    "                browser.find_element_by_name(\"Next\").click()\n",
    "                \n",
    "                print(\"\"\"Please type in your password, then answer the following:\"\"\")\n",
    "                password_input = input(\"Please enter your password: \")\n",
    "                password_submit = browser.find_element_by_id(\"password\")\n",
    "                password_submit.send_keys(password_input)\n",
    "                confirmation = input(\"Have you typed in your password yet? (y/n): \")\n",
    "                \n",
    "                if confirmation == \"y\":\n",
    "                    browser.find_element_by_name(\"Sign in\").click() #submit\n",
    "                else:\n",
    "                    browser.close()\n",
    "                    exit()\n",
    "            \n",
    "            def article_search(url):\n",
    "                browser.get(url)\n",
    "                art = browser.find_element_by_xpath(\"//*[@id='site-content']/div[3]\")\n",
    "                article_list.append(art.text.rstrip('\\n').replace('\\n', ' '))\n",
    "            \n",
    "            # Uncomment if you are running for the first time to ensure access via authentication \n",
    "            def start(url):\n",
    "#                 choice = input(\"Do you need to log into FT.com? (y/n): \")\n",
    "#                 if choice == \"y\":\n",
    "#                     open_browser()    \n",
    "#                 elif choice == \"n\":\n",
    "                article_search(url)        \n",
    "#                 else:\n",
    "#                     exit()\n",
    "\n",
    "            start(url)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                from newspaper import Article\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article_list.append(article.text)\n",
    "            except:\n",
    "                article_list.append(\"\")\n",
    "\n",
    "    news_dictionary[dt] = article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all dates were processed \n",
    "print(news_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MONTH = MARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.datetime(2015, 3, 1)\n",
    "end = datetime.datetime(2015, 4, 1)\n",
    "step = datetime.timedelta(days=1)\n",
    "\n",
    "dates_list = []\n",
    "\n",
    "while dt < end:\n",
    "    dates_list.append(dt.strftime('%m/%d/%Y'))\n",
    "    dt += step\n",
    "\n",
    "for dt in dates_list:\n",
    "    def scrape_news_urls(link):\n",
    "        time.sleep(randint(0, 20))\n",
    "        payload = {'as_epq': 'bitcoin', 'tbs':'cdr:1,cd_min:'+dt+',cd_max:'+dt+\"'\",'tbm':'nws'}\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'}\n",
    "        r = requests.get(link, headers=headers, params=payload)\n",
    "        print(r.status_code)  # Print the status code\n",
    "        content = r.text\n",
    "        news_urls = []\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        news_url = soup.findAll(\"a\", {\"class\": \"l _PMs\"}, href=True)\n",
    "        for url in news_url:\n",
    "            news_urls.append(url['href'])\n",
    "        return news_urls\n",
    "    \n",
    "    URL = 'https://www.google.com/search'\n",
    "    n_url = scrape_news_urls(URL)\n",
    "    article_list = []\n",
    "    \n",
    "    for url in n_url:\n",
    "        if re.match(r'https://www.ft.com/',url) is not None:\n",
    "            from sys import exit\n",
    "            from selenium import webdriver\n",
    "            \n",
    "            # Function to access Financial Times(needs mandatory subscription) through Selenium enabled authentication \n",
    "            def open_browser():\n",
    "                global browser\n",
    "                browser = webdriver.Firefox()\n",
    "                browser.get('https://accounts.ft.com/login?location=https%3A%2F%2Fwww.ft.com%2F')\n",
    "                \n",
    "                username_input = input(\"Please enter your username (email address): \")\n",
    "                username_submit = browser.find_element_by_id(\"email\")\n",
    "                username_submit.send_keys(username_input)\n",
    "                browser.find_element_by_name(\"Next\").click()\n",
    "                \n",
    "                print(\"\"\"Please type in your password, then answer the following:\"\"\")\n",
    "                password_input = input(\"Please enter your password: \")\n",
    "                password_submit = browser.find_element_by_id(\"password\")\n",
    "                password_submit.send_keys(password_input)\n",
    "                confirmation = input(\"Have you typed in your password yet? (y/n): \")\n",
    "                \n",
    "                if confirmation == \"y\":\n",
    "                    browser.find_element_by_name(\"Sign in\").click() #submit\n",
    "                else:\n",
    "                    browser.close()\n",
    "                    exit()\n",
    "            \n",
    "            def article_search(url):\n",
    "                browser.get(url)\n",
    "                art = browser.find_element_by_xpath(\"//*[@id='site-content']/div[3]\")\n",
    "                article_list.append(art.text.rstrip('\\n').replace('\\n', ' '))\n",
    "            \n",
    "            # Uncomment if you are running for the first time to ensure access via authentication \n",
    "            def start(url):\n",
    "#                 choice = input(\"Do you need to log into FT.com? (y/n): \")\n",
    "#                 if choice == \"y\":\n",
    "#                     open_browser()    \n",
    "#                 elif choice == \"n\":\n",
    "                article_search(url)        \n",
    "#                 else:\n",
    "#                     exit()\n",
    "\n",
    "            start(url)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                from newspaper import Article\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article_list.append(article.text)\n",
    "            except:\n",
    "                article_list.append(\"\")\n",
    "\n",
    "    news_dictionary[dt] = article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all dates were processed \n",
    "print(news_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MONTH = APRIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.datetime(2015, 4, 1)\n",
    "end = datetime.datetime(2015, 5, 1)\n",
    "step = datetime.timedelta(days=1)\n",
    "\n",
    "dates_list = []\n",
    "\n",
    "while dt < end:\n",
    "    dates_list.append(dt.strftime('%m/%d/%Y'))\n",
    "    dt += step\n",
    "\n",
    "for dt in dates_list:\n",
    "    def scrape_news_urls(link):\n",
    "        time.sleep(randint(0, 20))\n",
    "        payload = {'as_epq': 'bitcoin', 'tbs':'cdr:1,cd_min:'+dt+',cd_max:'+dt+\"'\",'tbm':'nws'}\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'}\n",
    "        r = requests.get(link, headers=headers, params=payload)\n",
    "        print(r.status_code)  # Print the status code\n",
    "        content = r.text\n",
    "        news_urls = []\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        news_url = soup.findAll(\"a\", {\"class\": \"l _PMs\"}, href=True)\n",
    "        for url in news_url:\n",
    "            news_urls.append(url['href'])\n",
    "        return news_urls\n",
    "    \n",
    "    URL = 'https://www.google.com/search'\n",
    "    n_url = scrape_news_urls(URL)\n",
    "    article_list = []\n",
    "    \n",
    "    for url in n_url:\n",
    "        if re.match(r'https://www.ft.com/',url) is not None:\n",
    "            from sys import exit\n",
    "            from selenium import webdriver\n",
    "            \n",
    "            # Function to access Financial Times(needs mandatory subscription) through Selenium enabled authentication \n",
    "            def open_browser():\n",
    "                global browser\n",
    "                browser = webdriver.Firefox()\n",
    "                browser.get('https://accounts.ft.com/login?location=https%3A%2F%2Fwww.ft.com%2F')\n",
    "                \n",
    "                username_input = input(\"Please enter your username (email address): \")\n",
    "                username_submit = browser.find_element_by_id(\"email\")\n",
    "                username_submit.send_keys(username_input)\n",
    "                browser.find_element_by_name(\"Next\").click()\n",
    "                \n",
    "                print(\"\"\"Please type in your password, then answer the following:\"\"\")\n",
    "                password_input = input(\"Please enter your password: \")\n",
    "                password_submit = browser.find_element_by_id(\"password\")\n",
    "                password_submit.send_keys(password_input)\n",
    "                confirmation = input(\"Have you typed in your password yet? (y/n): \")\n",
    "                \n",
    "                if confirmation == \"y\":\n",
    "                    browser.find_element_by_name(\"Sign in\").click() #submit\n",
    "                else:\n",
    "                    browser.close()\n",
    "                    exit()\n",
    "            \n",
    "            def article_search(url):\n",
    "                browser.get(url)\n",
    "                art = browser.find_element_by_xpath(\"//*[@id='site-content']/div[3]\")\n",
    "                article_list.append(art.text.rstrip('\\n').replace('\\n', ' '))\n",
    "            \n",
    "            # Uncomment if you are running for the first time to ensure access via authentication \n",
    "            def start(url):\n",
    "#                 choice = input(\"Do you need to log into FT.com? (y/n): \")\n",
    "#                 if choice == \"y\":\n",
    "#                     open_browser()    \n",
    "#                 elif choice == \"n\":\n",
    "                article_search(url)        \n",
    "#                 else:\n",
    "#                     exit()\n",
    "\n",
    "            start(url)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                from newspaper import Article\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article_list.append(article.text)\n",
    "            except:\n",
    "                article_list.append(\"\")\n",
    "\n",
    "    news_dictionary[dt] = article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all dates were processed \n",
    "print(news_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MONTH = MAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.datetime(2015, 5, 1)\n",
    "end = datetime.datetime(2015, 6, 1)\n",
    "step = datetime.timedelta(days=1)\n",
    "\n",
    "dates_list = []\n",
    "\n",
    "while dt < end:\n",
    "    dates_list.append(dt.strftime('%m/%d/%Y'))\n",
    "    dt += step\n",
    "\n",
    "for dt in dates_list:\n",
    "    def scrape_news_urls(link):\n",
    "        time.sleep(randint(0, 20))\n",
    "        payload = {'as_epq': 'bitcoin', 'tbs':'cdr:1,cd_min:'+dt+',cd_max:'+dt+\"'\",'tbm':'nws'}\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'}\n",
    "        r = requests.get(link, headers=headers, params=payload)\n",
    "        print(r.status_code)  # Print the status code\n",
    "        content = r.text\n",
    "        news_urls = []\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        news_url = soup.findAll(\"a\", {\"class\": \"l _PMs\"}, href=True)\n",
    "        for url in news_url:\n",
    "            news_urls.append(url['href'])\n",
    "        return news_urls\n",
    "    \n",
    "    URL = 'https://www.google.com/search'\n",
    "    n_url = scrape_news_urls(URL)\n",
    "    article_list = []\n",
    "    \n",
    "    for url in n_url:\n",
    "        if re.match(r'https://www.ft.com/',url) is not None:\n",
    "            from sys import exit\n",
    "            from selenium import webdriver\n",
    "            \n",
    "            # Function to access Financial Times(needs mandatory subscription) through Selenium enabled authentication \n",
    "            def open_browser():\n",
    "                global browser\n",
    "                browser = webdriver.Firefox()\n",
    "                browser.get('https://accounts.ft.com/login?location=https%3A%2F%2Fwww.ft.com%2F')\n",
    "                \n",
    "                username_input = input(\"Please enter your username (email address): \")\n",
    "                username_submit = browser.find_element_by_id(\"email\")\n",
    "                username_submit.send_keys(username_input)\n",
    "                browser.find_element_by_name(\"Next\").click()\n",
    "                \n",
    "                print(\"\"\"Please type in your password, then answer the following:\"\"\")\n",
    "                password_input = input(\"Please enter your password: \")\n",
    "                password_submit = browser.find_element_by_id(\"password\")\n",
    "                password_submit.send_keys(password_input)\n",
    "                confirmation = input(\"Have you typed in your password yet? (y/n): \")\n",
    "                \n",
    "                if confirmation == \"y\":\n",
    "                    browser.find_element_by_name(\"Sign in\").click() #submit\n",
    "                else:\n",
    "                    browser.close()\n",
    "                    exit()\n",
    "            \n",
    "            def article_search(url):\n",
    "                browser.get(url)\n",
    "                art = browser.find_element_by_xpath(\"//*[@id='site-content']/div[3]\")\n",
    "                article_list.append(art.text.rstrip('\\n').replace('\\n', ' '))\n",
    "            \n",
    "            # Uncomment if you are running for the first time to ensure access via authentication \n",
    "            def start(url):\n",
    "#                 choice = input(\"Do you need to log into FT.com? (y/n): \")\n",
    "#                 if choice == \"y\":\n",
    "#                     open_browser()    \n",
    "#                 elif choice == \"n\":\n",
    "                article_search(url)        \n",
    "#                 else:\n",
    "#                     exit()\n",
    "\n",
    "            start(url)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                from newspaper import Article\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article_list.append(article.text)\n",
    "            except:\n",
    "                article_list.append(\"\")\n",
    "\n",
    "    news_dictionary[dt] = article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all dates were processed \n",
    "print(news_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MONTH = JUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.datetime(2015, 6, 1)\n",
    "end = datetime.datetime(2015, 7, 1)\n",
    "step = datetime.timedelta(days=1)\n",
    "\n",
    "dates_list = []\n",
    "\n",
    "while dt < end:\n",
    "    dates_list.append(dt.strftime('%m/%d/%Y'))\n",
    "    dt += step\n",
    "\n",
    "for dt in dates_list:\n",
    "    def scrape_news_urls(link):\n",
    "        time.sleep(randint(0, 20))\n",
    "        payload = {'as_epq': 'bitcoin', 'tbs':'cdr:1,cd_min:'+dt+',cd_max:'+dt+\"'\",'tbm':'nws'}\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'}\n",
    "        r = requests.get(link, headers=headers, params=payload)\n",
    "        print(r.status_code)  # Print the status code\n",
    "        content = r.text\n",
    "        news_urls = []\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        news_url = soup.findAll(\"a\", {\"class\": \"l _PMs\"}, href=True)\n",
    "        for url in news_url:\n",
    "            news_urls.append(url['href'])\n",
    "        return news_urls\n",
    "    \n",
    "    URL = 'https://www.google.com/search'\n",
    "    n_url = scrape_news_urls(URL)\n",
    "    article_list = []\n",
    "    \n",
    "    for url in n_url:\n",
    "        if re.match(r'https://www.ft.com/',url) is not None:\n",
    "            from sys import exit\n",
    "            from selenium import webdriver\n",
    "            \n",
    "            # Function to access Financial Times(needs mandatory subscription) through Selenium enabled authentication \n",
    "            def open_browser():\n",
    "                global browser\n",
    "                browser = webdriver.Firefox()\n",
    "                browser.get('https://accounts.ft.com/login?location=https%3A%2F%2Fwww.ft.com%2F')\n",
    "                \n",
    "                username_input = input(\"Please enter your username (email address): \")\n",
    "                username_submit = browser.find_element_by_id(\"email\")\n",
    "                username_submit.send_keys(username_input)\n",
    "                browser.find_element_by_name(\"Next\").click()\n",
    "                \n",
    "                print(\"\"\"Please type in your password, then answer the following:\"\"\")\n",
    "                password_input = input(\"Please enter your password: \")\n",
    "                password_submit = browser.find_element_by_id(\"password\")\n",
    "                password_submit.send_keys(password_input)\n",
    "                confirmation = input(\"Have you typed in your password yet? (y/n): \")\n",
    "                \n",
    "                if confirmation == \"y\":\n",
    "                    browser.find_element_by_name(\"Sign in\").click() #submit\n",
    "                else:\n",
    "                    browser.close()\n",
    "                    exit()\n",
    "            \n",
    "            def article_search(url):\n",
    "                browser.get(url)\n",
    "                art = browser.find_element_by_xpath(\"//*[@id='site-content']/div[3]\")\n",
    "                article_list.append(art.text.rstrip('\\n').replace('\\n', ' '))\n",
    "            \n",
    "            # Uncomment if you are running for the first time to ensure access via authentication \n",
    "            def start(url):\n",
    "#                 choice = input(\"Do you need to log into FT.com? (y/n): \")\n",
    "#                 if choice == \"y\":\n",
    "#                     open_browser()    \n",
    "#                 elif choice == \"n\":\n",
    "                article_search(url)        \n",
    "#                 else:\n",
    "#                     exit()\n",
    "\n",
    "            start(url)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                from newspaper import Article\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article_list.append(article.text)\n",
    "            except:\n",
    "                article_list.append(\"\")\n",
    "\n",
    "    news_dictionary[dt] = article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all dates were processed \n",
    "print(news_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MONTH = JULY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.datetime(2015, 7, 1)\n",
    "end = datetime.datetime(2015, 8, 1)\n",
    "step = datetime.timedelta(days=1)\n",
    "\n",
    "dates_list = []\n",
    "\n",
    "while dt < end:\n",
    "    dates_list.append(dt.strftime('%m/%d/%Y'))\n",
    "    dt += step\n",
    "\n",
    "for dt in dates_list:\n",
    "    def scrape_news_urls(link):\n",
    "        time.sleep(randint(0, 20))\n",
    "        payload = {'as_epq': 'bitcoin', 'tbs':'cdr:1,cd_min:'+dt+',cd_max:'+dt+\"'\",'tbm':'nws'}\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'}\n",
    "        r = requests.get(link, headers=headers, params=payload)\n",
    "        print(r.status_code)  # Print the status code\n",
    "        content = r.text\n",
    "        news_urls = []\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        news_url = soup.findAll(\"a\", {\"class\": \"l _PMs\"}, href=True)\n",
    "        for url in news_url:\n",
    "            news_urls.append(url['href'])\n",
    "        return news_urls\n",
    "    \n",
    "    URL = 'https://www.google.com/search'\n",
    "    n_url = scrape_news_urls(URL)\n",
    "    article_list = []\n",
    "    \n",
    "    for url in n_url:\n",
    "        if re.match(r'https://www.ft.com/',url) is not None:\n",
    "            from sys import exit\n",
    "            from selenium import webdriver\n",
    "            \n",
    "            # Function to access Financial Times(needs mandatory subscription) through Selenium enabled authentication \n",
    "            def open_browser():\n",
    "                global browser\n",
    "                browser = webdriver.Firefox()\n",
    "                browser.get('https://accounts.ft.com/login?location=https%3A%2F%2Fwww.ft.com%2F')\n",
    "                \n",
    "                username_input = input(\"Please enter your username (email address): \")\n",
    "                username_submit = browser.find_element_by_id(\"email\")\n",
    "                username_submit.send_keys(username_input)\n",
    "                browser.find_element_by_name(\"Next\").click()\n",
    "                \n",
    "                print(\"\"\"Please type in your password, then answer the following:\"\"\")\n",
    "                password_input = input(\"Please enter your password: \")\n",
    "                password_submit = browser.find_element_by_id(\"password\")\n",
    "                password_submit.send_keys(password_input)\n",
    "                confirmation = input(\"Have you typed in your password yet? (y/n): \")\n",
    "                \n",
    "                if confirmation == \"y\":\n",
    "                    browser.find_element_by_name(\"Sign in\").click() #submit\n",
    "                else:\n",
    "                    browser.close()\n",
    "                    exit()\n",
    "            \n",
    "            def article_search(url):\n",
    "                browser.get(url)\n",
    "                art = browser.find_element_by_xpath(\"//*[@id='site-content']/div[3]\")\n",
    "                article_list.append(art.text.rstrip('\\n').replace('\\n', ' '))\n",
    "            \n",
    "            # Uncomment if you are running for the first time to ensure access via authentication \n",
    "            def start(url):\n",
    "#                 choice = input(\"Do you need to log into FT.com? (y/n): \")\n",
    "#                 if choice == \"y\":\n",
    "#                     open_browser()    \n",
    "#                 elif choice == \"n\":\n",
    "                article_search(url)        \n",
    "#                 else:\n",
    "#                     exit()\n",
    "\n",
    "            start(url)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                from newspaper import Article\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article_list.append(article.text)\n",
    "            except:\n",
    "                article_list.append(\"\")\n",
    "\n",
    "    news_dictionary[dt] = article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all dates were processed \n",
    "print(news_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MONTH = AUGUST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.datetime(2015, 8, 1)\n",
    "end = datetime.datetime(2015, 9, 1)\n",
    "step = datetime.timedelta(days=1)\n",
    "\n",
    "dates_list = []\n",
    "\n",
    "while dt < end:\n",
    "    dates_list.append(dt.strftime('%m/%d/%Y'))\n",
    "    dt += step\n",
    "\n",
    "for dt in dates_list:\n",
    "    def scrape_news_urls(link):\n",
    "        time.sleep(randint(0, 20))\n",
    "        payload = {'as_epq': 'bitcoin', 'tbs':'cdr:1,cd_min:'+dt+',cd_max:'+dt+\"'\",'tbm':'nws'}\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'}\n",
    "        r = requests.get(link, headers=headers, params=payload)\n",
    "        print(r.status_code)  # Print the status code\n",
    "        content = r.text\n",
    "        news_urls = []\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        news_url = soup.findAll(\"a\", {\"class\": \"l _PMs\"}, href=True)\n",
    "        for url in news_url:\n",
    "            news_urls.append(url['href'])\n",
    "        return news_urls\n",
    "    \n",
    "    URL = 'https://www.google.com/search'\n",
    "    n_url = scrape_news_urls(URL)\n",
    "    article_list = []\n",
    "    \n",
    "    for url in n_url:\n",
    "        if re.match(r'https://www.ft.com/',url) is not None:\n",
    "            from sys import exit\n",
    "            from selenium import webdriver\n",
    "            \n",
    "            # Function to access Financial Times(needs mandatory subscription) through Selenium enabled authentication \n",
    "            def open_browser():\n",
    "                global browser\n",
    "                browser = webdriver.Firefox()\n",
    "                browser.get('https://accounts.ft.com/login?location=https%3A%2F%2Fwww.ft.com%2F')\n",
    "                \n",
    "                username_input = input(\"Please enter your username (email address): \")\n",
    "                username_submit = browser.find_element_by_id(\"email\")\n",
    "                username_submit.send_keys(username_input)\n",
    "                browser.find_element_by_name(\"Next\").click()\n",
    "                \n",
    "                print(\"\"\"Please type in your password, then answer the following:\"\"\")\n",
    "                password_input = input(\"Please enter your password: \")\n",
    "                password_submit = browser.find_element_by_id(\"password\")\n",
    "                password_submit.send_keys(password_input)\n",
    "                confirmation = input(\"Have you typed in your password yet? (y/n): \")\n",
    "                \n",
    "                if confirmation == \"y\":\n",
    "                    browser.find_element_by_name(\"Sign in\").click() #submit\n",
    "                else:\n",
    "                    browser.close()\n",
    "                    exit()\n",
    "            \n",
    "            def article_search(url):\n",
    "                browser.get(url)\n",
    "                art = browser.find_element_by_xpath(\"//*[@id='site-content']/div[3]\")\n",
    "                article_list.append(art.text.rstrip('\\n').replace('\\n', ' '))\n",
    "            \n",
    "            # Uncomment if you are running for the first time to ensure access via authentication \n",
    "            def start(url):\n",
    "#                 choice = input(\"Do you need to log into FT.com? (y/n): \")\n",
    "#                 if choice == \"y\":\n",
    "#                     open_browser()    \n",
    "#                 elif choice == \"n\":\n",
    "                article_search(url)        \n",
    "#                 else:\n",
    "#                     exit()\n",
    "\n",
    "            start(url)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                from newspaper import Article\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article_list.append(article.text)\n",
    "            except:\n",
    "                article_list.append(\"\")\n",
    "\n",
    "    news_dictionary[dt] = article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all dates were processed \n",
    "print(news_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MONTH = SEPTEMBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.datetime(2015, 9, 1)\n",
    "end = datetime.datetime(2015, 10, 1)\n",
    "step = datetime.timedelta(days=1)\n",
    "\n",
    "dates_list = []\n",
    "\n",
    "while dt < end:\n",
    "    dates_list.append(dt.strftime('%m/%d/%Y'))\n",
    "    dt += step\n",
    "\n",
    "for dt in dates_list:\n",
    "    def scrape_news_urls(link):\n",
    "        time.sleep(randint(0, 20))\n",
    "        payload = {'as_epq': 'bitcoin', 'tbs':'cdr:1,cd_min:'+dt+',cd_max:'+dt+\"'\",'tbm':'nws'}\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'}\n",
    "        r = requests.get(link, headers=headers, params=payload)\n",
    "        print(r.status_code)  # Print the status code\n",
    "        content = r.text\n",
    "        news_urls = []\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        news_url = soup.findAll(\"a\", {\"class\": \"l _PMs\"}, href=True)\n",
    "        for url in news_url:\n",
    "            news_urls.append(url['href'])\n",
    "        return news_urls\n",
    "    \n",
    "    URL = 'https://www.google.com/search'\n",
    "    n_url = scrape_news_urls(URL)\n",
    "    article_list = []\n",
    "    \n",
    "    for url in n_url:\n",
    "        if re.match(r'https://www.ft.com/',url) is not None:\n",
    "            from sys import exit\n",
    "            from selenium import webdriver\n",
    "            \n",
    "            # Function to access Financial Times(needs mandatory subscription) through Selenium enabled authentication \n",
    "            def open_browser():\n",
    "                global browser\n",
    "                browser = webdriver.Firefox()\n",
    "                browser.get('https://accounts.ft.com/login?location=https%3A%2F%2Fwww.ft.com%2F')\n",
    "                \n",
    "                username_input = input(\"Please enter your username (email address): \")\n",
    "                username_submit = browser.find_element_by_id(\"email\")\n",
    "                username_submit.send_keys(username_input)\n",
    "                browser.find_element_by_name(\"Next\").click()\n",
    "                \n",
    "                print(\"\"\"Please type in your password, then answer the following:\"\"\")\n",
    "                password_input = input(\"Please enter your password: \")\n",
    "                password_submit = browser.find_element_by_id(\"password\")\n",
    "                password_submit.send_keys(password_input)\n",
    "                confirmation = input(\"Have you typed in your password yet? (y/n): \")\n",
    "                \n",
    "                if confirmation == \"y\":\n",
    "                    browser.find_element_by_name(\"Sign in\").click() #submit\n",
    "                else:\n",
    "                    browser.close()\n",
    "                    exit()\n",
    "            \n",
    "            def article_search(url):\n",
    "                browser.get(url)\n",
    "                art = browser.find_element_by_xpath(\"//*[@id='site-content']/div[3]\")\n",
    "                article_list.append(art.text.rstrip('\\n').replace('\\n', ' '))\n",
    "            \n",
    "            # Uncomment if you are running for the first time to ensure access via authentication \n",
    "            def start(url):\n",
    "#                 choice = input(\"Do you need to log into FT.com? (y/n): \")\n",
    "#                 if choice == \"y\":\n",
    "#                     open_browser()    \n",
    "#                 elif choice == \"n\":\n",
    "                article_search(url)        \n",
    "#                 else:\n",
    "#                     exit()\n",
    "\n",
    "            start(url)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                from newspaper import Article\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article_list.append(article.text)\n",
    "            except:\n",
    "                article_list.append(\"\")\n",
    "\n",
    "    news_dictionary[dt] = article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all dates were processed \n",
    "print(news_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MONTH = OCTOBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.datetime(2015, 10, 1)\n",
    "end = datetime.datetime(2015, 11, 1)\n",
    "step = datetime.timedelta(days=1)\n",
    "\n",
    "dates_list = []\n",
    "\n",
    "while dt < end:\n",
    "    dates_list.append(dt.strftime('%m/%d/%Y'))\n",
    "    dt += step\n",
    "\n",
    "for dt in dates_list:\n",
    "    def scrape_news_urls(link):\n",
    "        time.sleep(randint(0, 20))\n",
    "        payload = {'as_epq': 'bitcoin', 'tbs':'cdr:1,cd_min:'+dt+',cd_max:'+dt+\"'\",'tbm':'nws'}\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'}\n",
    "        r = requests.get(link, headers=headers, params=payload)\n",
    "        print(r.status_code)  # Print the status code\n",
    "        content = r.text\n",
    "        news_urls = []\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        news_url = soup.findAll(\"a\", {\"class\": \"l _PMs\"}, href=True)\n",
    "        for url in news_url:\n",
    "            news_urls.append(url['href'])\n",
    "        return news_urls\n",
    "    \n",
    "    URL = 'https://www.google.com/search'\n",
    "    n_url = scrape_news_urls(URL)\n",
    "    article_list = []\n",
    "    \n",
    "    for url in n_url:\n",
    "        if re.match(r'https://www.ft.com/',url) is not None:\n",
    "            from sys import exit\n",
    "            from selenium import webdriver\n",
    "            \n",
    "            # Function to access Financial Times(needs mandatory subscription) through Selenium enabled authentication \n",
    "            def open_browser():\n",
    "                global browser\n",
    "                browser = webdriver.Firefox()\n",
    "                browser.get('https://accounts.ft.com/login?location=https%3A%2F%2Fwww.ft.com%2F')\n",
    "                \n",
    "                username_input = input(\"Please enter your username (email address): \")\n",
    "                username_submit = browser.find_element_by_id(\"email\")\n",
    "                username_submit.send_keys(username_input)\n",
    "                browser.find_element_by_name(\"Next\").click()\n",
    "                \n",
    "                print(\"\"\"Please type in your password, then answer the following:\"\"\")\n",
    "                password_input = input(\"Please enter your password: \")\n",
    "                password_submit = browser.find_element_by_id(\"password\")\n",
    "                password_submit.send_keys(password_input)\n",
    "                confirmation = input(\"Have you typed in your password yet? (y/n): \")\n",
    "                \n",
    "                if confirmation == \"y\":\n",
    "                    browser.find_element_by_name(\"Sign in\").click() #submit\n",
    "                else:\n",
    "                    browser.close()\n",
    "                    exit()\n",
    "            \n",
    "            def article_search(url):\n",
    "                browser.get(url)\n",
    "                art = browser.find_element_by_xpath(\"//*[@id='site-content']/div[3]\")\n",
    "                article_list.append(art.text.rstrip('\\n').replace('\\n', ' '))\n",
    "            \n",
    "            # Uncomment if you are running for the first time to ensure access via authentication \n",
    "            def start(url):\n",
    "#                 choice = input(\"Do you need to log into FT.com? (y/n): \")\n",
    "#                 if choice == \"y\":\n",
    "#                     open_browser()    \n",
    "#                 elif choice == \"n\":\n",
    "                article_search(url)        \n",
    "#                 else:\n",
    "#                     exit()\n",
    "\n",
    "            start(url)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                from newspaper import Article\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article_list.append(article.text)\n",
    "            except:\n",
    "                article_list.append(\"\")\n",
    "\n",
    "    news_dictionary[dt] = article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all dates were processed \n",
    "print(news_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MONTH = NOVEMBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.datetime(2015, 11, 1)\n",
    "end = datetime.datetime(2015, 12, 1)\n",
    "step = datetime.timedelta(days=1)\n",
    "\n",
    "dates_list = []\n",
    "\n",
    "while dt < end:\n",
    "    dates_list.append(dt.strftime('%m/%d/%Y'))\n",
    "    dt += step\n",
    "\n",
    "for dt in dates_list:\n",
    "    def scrape_news_urls(link):\n",
    "        time.sleep(randint(0, 20))\n",
    "        payload = {'as_epq': 'bitcoin', 'tbs':'cdr:1,cd_min:'+dt+',cd_max:'+dt+\"'\",'tbm':'nws'}\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'}\n",
    "        r = requests.get(link, headers=headers, params=payload)\n",
    "        print(r.status_code)  # Print the status code\n",
    "        content = r.text\n",
    "        news_urls = []\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        news_url = soup.findAll(\"a\", {\"class\": \"l _PMs\"}, href=True)\n",
    "        for url in news_url:\n",
    "            news_urls.append(url['href'])\n",
    "        return news_urls\n",
    "    \n",
    "    URL = 'https://www.google.com/search'\n",
    "    n_url = scrape_news_urls(URL)\n",
    "    article_list = []\n",
    "    \n",
    "    for url in n_url:\n",
    "        if re.match(r'https://www.ft.com/',url) is not None:\n",
    "            from sys import exit\n",
    "            from selenium import webdriver\n",
    "            \n",
    "            # Function to access Financial Times(needs mandatory subscription) through Selenium enabled authentication \n",
    "            def open_browser():\n",
    "                global browser\n",
    "                browser = webdriver.Firefox()\n",
    "                browser.get('https://accounts.ft.com/login?location=https%3A%2F%2Fwww.ft.com%2F')\n",
    "                \n",
    "                username_input = input(\"Please enter your username (email address): \")\n",
    "                username_submit = browser.find_element_by_id(\"email\")\n",
    "                username_submit.send_keys(username_input)\n",
    "                browser.find_element_by_name(\"Next\").click()\n",
    "                \n",
    "                print(\"\"\"Please type in your password, then answer the following:\"\"\")\n",
    "                password_input = input(\"Please enter your password: \")\n",
    "                password_submit = browser.find_element_by_id(\"password\")\n",
    "                password_submit.send_keys(password_input)\n",
    "                confirmation = input(\"Have you typed in your password yet? (y/n): \")\n",
    "                \n",
    "                if confirmation == \"y\":\n",
    "                    browser.find_element_by_name(\"Sign in\").click() #submit\n",
    "                else:\n",
    "                    browser.close()\n",
    "                    exit()\n",
    "            \n",
    "            def article_search(url):\n",
    "                browser.get(url)\n",
    "                art = browser.find_element_by_xpath(\"//*[@id='site-content']/div[3]\")\n",
    "                article_list.append(art.text.rstrip('\\n').replace('\\n', ' '))\n",
    "            \n",
    "            # Uncomment if you are running for the first time to ensure access via authentication \n",
    "            def start(url):\n",
    "#                 choice = input(\"Do you need to log into FT.com? (y/n): \")\n",
    "#                 if choice == \"y\":\n",
    "#                     open_browser()    \n",
    "#                 elif choice == \"n\":\n",
    "                article_search(url)        \n",
    "#                 else:\n",
    "#                     exit()\n",
    "\n",
    "            start(url)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                from newspaper import Article\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article_list.append(article.text)\n",
    "            except:\n",
    "                article_list.append(\"\")\n",
    "\n",
    "    news_dictionary[dt] = article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all dates were processed \n",
    "print(news_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MONTH = DECEMBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.datetime(2015, 12, 1)\n",
    "end = datetime.datetime(2016, 1, 1)\n",
    "step = datetime.timedelta(days=1)\n",
    "\n",
    "dates_list = []\n",
    "\n",
    "while dt < end:\n",
    "    dates_list.append(dt.strftime('%m/%d/%Y'))\n",
    "    dt += step\n",
    "\n",
    "for dt in dates_list:\n",
    "    def scrape_news_urls(link):\n",
    "        time.sleep(randint(0, 20))\n",
    "        payload = {'as_epq': 'bitcoin', 'tbs':'cdr:1,cd_min:'+dt+',cd_max:'+dt+\"'\",'tbm':'nws'}\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'}\n",
    "        r = requests.get(link, headers=headers, params=payload)\n",
    "        print(r.status_code)  # Print the status code\n",
    "        content = r.text\n",
    "        news_urls = []\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        news_url = soup.findAll(\"a\", {\"class\": \"l _PMs\"}, href=True)\n",
    "        for url in news_url:\n",
    "            news_urls.append(url['href'])\n",
    "        return news_urls\n",
    "    \n",
    "    URL = 'https://www.google.com/search'\n",
    "    n_url = scrape_news_urls(URL)\n",
    "    article_list = []\n",
    "    \n",
    "    for url in n_url:\n",
    "        if re.match(r'https://www.ft.com/',url) is not None:\n",
    "            from sys import exit\n",
    "            from selenium import webdriver\n",
    "            \n",
    "            # Function to access Financial Times(needs mandatory subscription) through Selenium enabled authentication \n",
    "            def open_browser():\n",
    "                global browser\n",
    "                browser = webdriver.Firefox()\n",
    "                browser.get('https://accounts.ft.com/login?location=https%3A%2F%2Fwww.ft.com%2F')\n",
    "                \n",
    "                username_input = input(\"Please enter your username (email address): \")\n",
    "                username_submit = browser.find_element_by_id(\"email\")\n",
    "                username_submit.send_keys(username_input)\n",
    "                browser.find_element_by_name(\"Next\").click()\n",
    "                \n",
    "                print(\"\"\"Please type in your password, then answer the following:\"\"\")\n",
    "                password_input = input(\"Please enter your password: \")\n",
    "                password_submit = browser.find_element_by_id(\"password\")\n",
    "                password_submit.send_keys(password_input)\n",
    "                confirmation = input(\"Have you typed in your password yet? (y/n): \")\n",
    "                \n",
    "                if confirmation == \"y\":\n",
    "                    browser.find_element_by_name(\"Sign in\").click() #submit\n",
    "                else:\n",
    "                    browser.close()\n",
    "                    exit()\n",
    "            \n",
    "            def article_search(url):\n",
    "                browser.get(url)\n",
    "                art = browser.find_element_by_xpath(\"//*[@id='site-content']/div[3]\")\n",
    "                article_list.append(art.text.rstrip('\\n').replace('\\n', ' '))\n",
    "            \n",
    "            # Uncomment if you are running for the first time to ensure access via authentication \n",
    "            def start(url):\n",
    "#                 choice = input(\"Do you need to log into FT.com? (y/n): \")\n",
    "#                 if choice == \"y\":\n",
    "#                     open_browser()    \n",
    "#                 elif choice == \"n\":\n",
    "                article_search(url)        \n",
    "#                 else:\n",
    "#                     exit()\n",
    "\n",
    "            start(url)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                from newspaper import Article\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article_list.append(article.text)\n",
    "            except:\n",
    "                article_list.append(\"\")\n",
    "\n",
    "    news_dictionary[dt] = article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all dates were processed \n",
    "print(news_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YEAR = 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MONTH = JANUARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "dt = datetime.datetime(2016, 1, 1)\n",
    "end = datetime.datetime(2016, 2, 1)\n",
    "step = datetime.timedelta(days=1)\n",
    "\n",
    "dates_list = []\n",
    "\n",
    "while dt < end:\n",
    "    dates_list.append(dt.strftime('%m/%d/%Y'))\n",
    "    dt += step\n",
    "\n",
    "for dt in dates_list:\n",
    "    def scrape_news_urls(link):\n",
    "        time.sleep(randint(0, 20))\n",
    "        payload = {'as_epq': 'bitcoin', 'tbs':'cdr:1,cd_min:'+dt+',cd_max:'+dt+\"'\",'tbm':'nws'}\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'}\n",
    "        r = requests.get(link, headers=headers, params=payload)\n",
    "        print(r.status_code)  # Print the status code\n",
    "        content = r.text\n",
    "        news_urls = []\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        news_url = soup.findAll(\"a\", {\"class\": \"l _PMs\"}, href=True)\n",
    "        for url in news_url:\n",
    "            news_urls.append(url['href'])\n",
    "        return news_urls\n",
    "    \n",
    "    URL = 'https://www.google.com/search'\n",
    "    n_url = scrape_news_urls(URL)\n",
    "    article_list = []\n",
    "    \n",
    "    for url in n_url:\n",
    "        if re.match(r'https://www.ft.com/',url) is not None:\n",
    "            from sys import exit\n",
    "            from selenium import webdriver\n",
    "            \n",
    "            # Function to access Financial Times(needs mandatory subscription) through Selenium enabled authentication \n",
    "            def open_browser():\n",
    "                global browser\n",
    "                browser = webdriver.Firefox()\n",
    "                browser.get('https://accounts.ft.com/login?location=https%3A%2F%2Fwww.ft.com%2F')\n",
    "                \n",
    "                username_input = input(\"Please enter your username (email address): \")\n",
    "                username_submit = browser.find_element_by_id(\"email\")\n",
    "                username_submit.send_keys(username_input)\n",
    "                browser.find_element_by_name(\"Next\").click()\n",
    "                \n",
    "                print(\"\"\"Please type in your password, then answer the following:\"\"\")\n",
    "                password_input = input(\"Please enter your password: \")\n",
    "                password_submit = browser.find_element_by_id(\"password\")\n",
    "                password_submit.send_keys(password_input)\n",
    "                confirmation = input(\"Have you typed in your password yet? (y/n): \")\n",
    "                \n",
    "                if confirmation == \"y\":\n",
    "                    browser.find_element_by_name(\"Sign in\").click() #submit\n",
    "                else:\n",
    "                    browser.close()\n",
    "                    exit()\n",
    "            \n",
    "            def article_search(url):\n",
    "                browser.get(url)\n",
    "                art = browser.find_element_by_xpath(\"//*[@id='site-content']/div[3]\")\n",
    "                article_list.append(art.text.rstrip('\\n').replace('\\n', ' '))\n",
    "            \n",
    "            # Uncomment if you are running for the first time to ensure access via authentication \n",
    "            def start(url):\n",
    "#                 choice = input(\"Do you need to log into FT.com? (y/n): \")\n",
    "#                 if choice == \"y\":\n",
    "#                     open_browser()    \n",
    "#                 elif choice == \"n\":\n",
    "                article_search(url)        \n",
    "#                 else:\n",
    "#                     exit()\n",
    "\n",
    "            start(url)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                from newspaper import Article\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article_list.append(article.text)\n",
    "            except:\n",
    "                article_list.append(\"\")\n",
    "\n",
    "    news_dictionary[dt] = article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all dates were processed \n",
    "print(news_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MONTH = FEBRUARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.datetime(2016, 2, 1)\n",
    "end = datetime.datetime(2016, 3, 1)\n",
    "step = datetime.timedelta(days=1)\n",
    "\n",
    "dates_list = []\n",
    "\n",
    "while dt < end:\n",
    "    dates_list.append(dt.strftime('%m/%d/%Y'))\n",
    "    dt += step\n",
    "\n",
    "for dt in dates_list:\n",
    "    def scrape_news_urls(link):\n",
    "        time.sleep(randint(0, 20))\n",
    "        payload = {'as_epq': 'bitcoin', 'tbs':'cdr:1,cd_min:'+dt+',cd_max:'+dt+\"'\",'tbm':'nws'}\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'}\n",
    "        r = requests.get(link, headers=headers, params=payload)\n",
    "        print(r.status_code)  # Print the status code\n",
    "        content = r.text\n",
    "        news_urls = []\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        news_url = soup.findAll(\"a\", {\"class\": \"l _PMs\"}, href=True)\n",
    "        for url in news_url:\n",
    "            news_urls.append(url['href'])\n",
    "        return news_urls\n",
    "    \n",
    "    URL = 'https://www.google.com/search'\n",
    "    n_url = scrape_news_urls(URL)\n",
    "    article_list = []\n",
    "    \n",
    "    for url in n_url:\n",
    "        if re.match(r'https://www.ft.com/',url) is not None:\n",
    "            from sys import exit\n",
    "            from selenium import webdriver\n",
    "            \n",
    "            # Function to access Financial Times(needs mandatory subscription) through Selenium enabled authentication \n",
    "            def open_browser():\n",
    "                global browser\n",
    "                browser = webdriver.Firefox()\n",
    "                browser.get('https://accounts.ft.com/login?location=https%3A%2F%2Fwww.ft.com%2F')\n",
    "                \n",
    "                username_input = input(\"Please enter your username (email address): \")\n",
    "                username_submit = browser.find_element_by_id(\"email\")\n",
    "                username_submit.send_keys(username_input)\n",
    "                browser.find_element_by_name(\"Next\").click()\n",
    "                \n",
    "                print(\"\"\"Please type in your password, then answer the following:\"\"\")\n",
    "                password_input = input(\"Please enter your password: \")\n",
    "                password_submit = browser.find_element_by_id(\"password\")\n",
    "                password_submit.send_keys(password_input)\n",
    "                confirmation = input(\"Have you typed in your password yet? (y/n): \")\n",
    "                \n",
    "                if confirmation == \"y\":\n",
    "                    browser.find_element_by_name(\"Sign in\").click() #submit\n",
    "                else:\n",
    "                    browser.close()\n",
    "                    exit()\n",
    "            \n",
    "            def article_search(url):\n",
    "                browser.get(url)\n",
    "                art = browser.find_element_by_xpath(\"//*[@id='site-content']/div[3]\")\n",
    "                article_list.append(art.text.rstrip('\\n').replace('\\n', ' '))\n",
    "            \n",
    "            # Uncomment if you are running for the first time to ensure access via authentication \n",
    "            def start(url):\n",
    "#                 choice = input(\"Do you need to log into FT.com? (y/n): \")\n",
    "#                 if choice == \"y\":\n",
    "#                     open_browser()    \n",
    "#                 elif choice == \"n\":\n",
    "                article_search(url)        \n",
    "#                 else:\n",
    "#                     exit()\n",
    "\n",
    "            start(url)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                from newspaper import Article\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article_list.append(article.text)\n",
    "            except:\n",
    "                article_list.append(\"\")\n",
    "\n",
    "    news_dictionary[dt] = article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all dates were processed \n",
    "print(news_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MONTH = MARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.datetime(2016, 3, 1)\n",
    "end = datetime.datetime(2016, 4, 1)\n",
    "step = datetime.timedelta(days=1)\n",
    "\n",
    "dates_list = []\n",
    "\n",
    "while dt < end:\n",
    "    dates_list.append(dt.strftime('%m/%d/%Y'))\n",
    "    dt += step\n",
    "\n",
    "for dt in dates_list:\n",
    "    def scrape_news_urls(link):\n",
    "        time.sleep(randint(0, 20))\n",
    "        payload = {'as_epq': 'bitcoin', 'tbs':'cdr:1,cd_min:'+dt+',cd_max:'+dt+\"'\",'tbm':'nws'}\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'}\n",
    "        r = requests.get(link, headers=headers, params=payload)\n",
    "        print(r.status_code)  # Print the status code\n",
    "        content = r.text\n",
    "        news_urls = []\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        news_url = soup.findAll(\"a\", {\"class\": \"l _PMs\"}, href=True)\n",
    "        for url in news_url:\n",
    "            news_urls.append(url['href'])\n",
    "        return news_urls\n",
    "    \n",
    "    URL = 'https://www.google.com/search'\n",
    "    n_url = scrape_news_urls(URL)\n",
    "    article_list = []\n",
    "    \n",
    "    for url in n_url:\n",
    "        if re.match(r'https://www.ft.com/',url) is not None:\n",
    "            from sys import exit\n",
    "            from selenium import webdriver\n",
    "            \n",
    "            # Function to access Financial Times(needs mandatory subscription) through Selenium enabled authentication \n",
    "            def open_browser():\n",
    "                global browser\n",
    "                browser = webdriver.Firefox()\n",
    "                browser.get('https://accounts.ft.com/login?location=https%3A%2F%2Fwww.ft.com%2F')\n",
    "                \n",
    "                username_input = input(\"Please enter your username (email address): \")\n",
    "                username_submit = browser.find_element_by_id(\"email\")\n",
    "                username_submit.send_keys(username_input)\n",
    "                browser.find_element_by_name(\"Next\").click()\n",
    "                \n",
    "                print(\"\"\"Please type in your password, then answer the following:\"\"\")\n",
    "                password_input = input(\"Please enter your password: \")\n",
    "                password_submit = browser.find_element_by_id(\"password\")\n",
    "                password_submit.send_keys(password_input)\n",
    "                confirmation = input(\"Have you typed in your password yet? (y/n): \")\n",
    "                \n",
    "                if confirmation == \"y\":\n",
    "                    browser.find_element_by_name(\"Sign in\").click() #submit\n",
    "                else:\n",
    "                    browser.close()\n",
    "                    exit()\n",
    "            \n",
    "            def article_search(url):\n",
    "                browser.get(url)\n",
    "                art = browser.find_element_by_xpath(\"//*[@id='site-content']/div[3]\")\n",
    "                article_list.append(art.text.rstrip('\\n').replace('\\n', ' '))\n",
    "            \n",
    "            # Uncomment if you are running for the first time to ensure access via authentication \n",
    "            def start(url):\n",
    "#                 choice = input(\"Do you need to log into FT.com? (y/n): \")\n",
    "#                 if choice == \"y\":\n",
    "#                     open_browser()    \n",
    "#                 elif choice == \"n\":\n",
    "                article_search(url)        \n",
    "#                 else:\n",
    "#                     exit()\n",
    "\n",
    "            start(url)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                from newspaper import Article\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article_list.append(article.text)\n",
    "            except:\n",
    "                article_list.append(\"\")\n",
    "\n",
    "    news_dictionary[dt] = article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all dates were processed \n",
    "print(news_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MONTH = APRIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.datetime(2016, 4, 1)\n",
    "end = datetime.datetime(2016, 5, 1)\n",
    "step = datetime.timedelta(days=1)\n",
    "\n",
    "dates_list = []\n",
    "\n",
    "while dt < end:\n",
    "    dates_list.append(dt.strftime('%m/%d/%Y'))\n",
    "    dt += step\n",
    "\n",
    "for dt in dates_list:\n",
    "    def scrape_news_urls(link):\n",
    "        time.sleep(randint(0, 20))\n",
    "        payload = {'as_epq': 'bitcoin', 'tbs':'cdr:1,cd_min:'+dt+',cd_max:'+dt+\"'\",'tbm':'nws'}\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'}\n",
    "        r = requests.get(link, headers=headers, params=payload)\n",
    "        print(r.status_code)  # Print the status code\n",
    "        content = r.text\n",
    "        news_urls = []\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        news_url = soup.findAll(\"a\", {\"class\": \"l _PMs\"}, href=True)\n",
    "        for url in news_url:\n",
    "            news_urls.append(url['href'])\n",
    "        return news_urls\n",
    "    \n",
    "    URL = 'https://www.google.com/search'\n",
    "    n_url = scrape_news_urls(URL)\n",
    "    article_list = []\n",
    "    \n",
    "    for url in n_url:\n",
    "        if re.match(r'https://www.ft.com/',url) is not None:\n",
    "            from sys import exit\n",
    "            from selenium import webdriver\n",
    "            \n",
    "            # Function to access Financial Times(needs mandatory subscription) through Selenium enabled authentication \n",
    "            def open_browser():\n",
    "                global browser\n",
    "                browser = webdriver.Firefox()\n",
    "                browser.get('https://accounts.ft.com/login?location=https%3A%2F%2Fwww.ft.com%2F')\n",
    "                \n",
    "                username_input = input(\"Please enter your username (email address): \")\n",
    "                username_submit = browser.find_element_by_id(\"email\")\n",
    "                username_submit.send_keys(username_input)\n",
    "                browser.find_element_by_name(\"Next\").click()\n",
    "                \n",
    "                print(\"\"\"Please type in your password, then answer the following:\"\"\")\n",
    "                password_input = input(\"Please enter your password: \")\n",
    "                password_submit = browser.find_element_by_id(\"password\")\n",
    "                password_submit.send_keys(password_input)\n",
    "                confirmation = input(\"Have you typed in your password yet? (y/n): \")\n",
    "                \n",
    "                if confirmation == \"y\":\n",
    "                    browser.find_element_by_name(\"Sign in\").click() #submit\n",
    "                else:\n",
    "                    browser.close()\n",
    "                    exit()\n",
    "            \n",
    "            def article_search(url):\n",
    "                browser.get(url)\n",
    "                art = browser.find_element_by_xpath(\"//*[@id='site-content']/div[3]\")\n",
    "                article_list.append(art.text.rstrip('\\n').replace('\\n', ' '))\n",
    "            \n",
    "            # Uncomment if you are running for the first time to ensure access via authentication \n",
    "            def start(url):\n",
    "#                 choice = input(\"Do you need to log into FT.com? (y/n): \")\n",
    "#                 if choice == \"y\":\n",
    "#                     open_browser()    \n",
    "#                 elif choice == \"n\":\n",
    "                article_search(url)        \n",
    "#                 else:\n",
    "#                     exit()\n",
    "\n",
    "            start(url)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                from newspaper import Article\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article_list.append(article.text)\n",
    "            except:\n",
    "                article_list.append(\"\")\n",
    "\n",
    "    news_dictionary[dt] = article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all dates were processed \n",
    "print(news_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MONTH = MAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.datetime(2016, 5, 1)\n",
    "end = datetime.datetime(2016, 6, 1)\n",
    "step = datetime.timedelta(days=1)\n",
    "\n",
    "dates_list = []\n",
    "\n",
    "while dt < end:\n",
    "    dates_list.append(dt.strftime('%m/%d/%Y'))\n",
    "    dt += step\n",
    "\n",
    "for dt in dates_list:\n",
    "    def scrape_news_urls(link):\n",
    "        time.sleep(randint(0, 20))\n",
    "        payload = {'as_epq': 'bitcoin', 'tbs':'cdr:1,cd_min:'+dt+',cd_max:'+dt+\"'\",'tbm':'nws'}\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'}\n",
    "        r = requests.get(link, headers=headers, params=payload)\n",
    "        print(r.status_code)  # Print the status code\n",
    "        content = r.text\n",
    "        news_urls = []\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        news_url = soup.findAll(\"a\", {\"class\": \"l _PMs\"}, href=True)\n",
    "        for url in news_url:\n",
    "            news_urls.append(url['href'])\n",
    "        return news_urls\n",
    "    \n",
    "    URL = 'https://www.google.com/search'\n",
    "    n_url = scrape_news_urls(URL)\n",
    "    article_list = []\n",
    "    \n",
    "    for url in n_url:\n",
    "        if re.match(r'https://www.ft.com/',url) is not None:\n",
    "            from sys import exit\n",
    "            from selenium import webdriver\n",
    "            \n",
    "            # Function to access Financial Times(needs mandatory subscription) through Selenium enabled authentication \n",
    "            def open_browser():\n",
    "                global browser\n",
    "                browser = webdriver.Firefox()\n",
    "                browser.get('https://accounts.ft.com/login?location=https%3A%2F%2Fwww.ft.com%2F')\n",
    "                \n",
    "                username_input = input(\"Please enter your username (email address): \")\n",
    "                username_submit = browser.find_element_by_id(\"email\")\n",
    "                username_submit.send_keys(username_input)\n",
    "                browser.find_element_by_name(\"Next\").click()\n",
    "                \n",
    "                print(\"\"\"Please type in your password, then answer the following:\"\"\")\n",
    "                password_input = input(\"Please enter your password: \")\n",
    "                password_submit = browser.find_element_by_id(\"password\")\n",
    "                password_submit.send_keys(password_input)\n",
    "                confirmation = input(\"Have you typed in your password yet? (y/n): \")\n",
    "                \n",
    "                if confirmation == \"y\":\n",
    "                    browser.find_element_by_name(\"Sign in\").click() #submit\n",
    "                else:\n",
    "                    browser.close()\n",
    "                    exit()\n",
    "            \n",
    "            def article_search(url):\n",
    "                browser.get(url)\n",
    "                art = browser.find_element_by_xpath(\"//*[@id='site-content']/div[3]\")\n",
    "                article_list.append(art.text.rstrip('\\n').replace('\\n', ' '))\n",
    "            \n",
    "            # Uncomment if you are running for the first time to ensure access via authentication \n",
    "            def start(url):\n",
    "#                 choice = input(\"Do you need to log into FT.com? (y/n): \")\n",
    "#                 if choice == \"y\":\n",
    "#                     open_browser()    \n",
    "#                 elif choice == \"n\":\n",
    "                article_search(url)        \n",
    "#                 else:\n",
    "#                     exit()\n",
    "\n",
    "            start(url)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                from newspaper import Article\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article_list.append(article.text)\n",
    "            except:\n",
    "                article_list.append(\"\")\n",
    "\n",
    "    news_dictionary[dt] = article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all dates were processed \n",
    "print(news_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MONTH = JUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.datetime(2016, 6, 1)\n",
    "end = datetime.datetime(2016, 7, 1)\n",
    "step = datetime.timedelta(days=1)\n",
    "\n",
    "dates_list = []\n",
    "\n",
    "while dt < end:\n",
    "    dates_list.append(dt.strftime('%m/%d/%Y'))\n",
    "    dt += step\n",
    "\n",
    "for dt in dates_list:\n",
    "    def scrape_news_urls(link):\n",
    "        time.sleep(randint(0, 20))\n",
    "        payload = {'as_epq': 'bitcoin', 'tbs':'cdr:1,cd_min:'+dt+',cd_max:'+dt+\"'\",'tbm':'nws'}\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'}\n",
    "        r = requests.get(link, headers=headers, params=payload)\n",
    "        print(r.status_code)  # Print the status code\n",
    "        content = r.text\n",
    "        news_urls = []\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        news_url = soup.findAll(\"a\", {\"class\": \"l _PMs\"}, href=True)\n",
    "        for url in news_url:\n",
    "            news_urls.append(url['href'])\n",
    "        return news_urls\n",
    "    \n",
    "    URL = 'https://www.google.com/search'\n",
    "    n_url = scrape_news_urls(URL)\n",
    "    article_list = []\n",
    "    \n",
    "    for url in n_url:\n",
    "        if re.match(r'https://www.ft.com/',url) is not None:\n",
    "            from sys import exit\n",
    "            from selenium import webdriver\n",
    "            \n",
    "            # Function to access Financial Times(needs mandatory subscription) through Selenium enabled authentication \n",
    "            def open_browser():\n",
    "                global browser\n",
    "                browser = webdriver.Firefox()\n",
    "                browser.get('https://accounts.ft.com/login?location=https%3A%2F%2Fwww.ft.com%2F')\n",
    "                \n",
    "                username_input = input(\"Please enter your username (email address): \")\n",
    "                username_submit = browser.find_element_by_id(\"email\")\n",
    "                username_submit.send_keys(username_input)\n",
    "                browser.find_element_by_name(\"Next\").click()\n",
    "                \n",
    "                print(\"\"\"Please type in your password, then answer the following:\"\"\")\n",
    "                password_input = input(\"Please enter your password: \")\n",
    "                password_submit = browser.find_element_by_id(\"password\")\n",
    "                password_submit.send_keys(password_input)\n",
    "                confirmation = input(\"Have you typed in your password yet? (y/n): \")\n",
    "                \n",
    "                if confirmation == \"y\":\n",
    "                    browser.find_element_by_name(\"Sign in\").click() #submit\n",
    "                else:\n",
    "                    browser.close()\n",
    "                    exit()\n",
    "            \n",
    "            def article_search(url):\n",
    "                browser.get(url)\n",
    "                art = browser.find_element_by_xpath(\"//*[@id='site-content']/div[3]\")\n",
    "                article_list.append(art.text.rstrip('\\n').replace('\\n', ' '))\n",
    "            \n",
    "            # Uncomment if you are running for the first time to ensure access via authentication \n",
    "            def start(url):\n",
    "#                 choice = input(\"Do you need to log into FT.com? (y/n): \")\n",
    "#                 if choice == \"y\":\n",
    "#                     open_browser()    \n",
    "#                 elif choice == \"n\":\n",
    "                article_search(url)        \n",
    "#                 else:\n",
    "#                     exit()\n",
    "\n",
    "            start(url)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                from newspaper import Article\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article_list.append(article.text)\n",
    "            except:\n",
    "                article_list.append(\"\")\n",
    "\n",
    "    news_dictionary[dt] = article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all dates were processed \n",
    "print(news_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MONTH = JULY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.datetime(2016, 7, 1)\n",
    "end = datetime.datetime(2016, 8, 1)\n",
    "step = datetime.timedelta(days=1)\n",
    "\n",
    "dates_list = []\n",
    "\n",
    "while dt < end:\n",
    "    dates_list.append(dt.strftime('%m/%d/%Y'))\n",
    "    dt += step\n",
    "\n",
    "for dt in dates_list:\n",
    "    def scrape_news_urls(link):\n",
    "        time.sleep(randint(0, 20))\n",
    "        payload = {'as_epq': 'bitcoin', 'tbs':'cdr:1,cd_min:'+dt+',cd_max:'+dt+\"'\",'tbm':'nws'}\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'}\n",
    "        r = requests.get(link, headers=headers, params=payload)\n",
    "        print(r.status_code)  # Print the status code\n",
    "        content = r.text\n",
    "        news_urls = []\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        news_url = soup.findAll(\"a\", {\"class\": \"l _PMs\"}, href=True)\n",
    "        for url in news_url:\n",
    "            news_urls.append(url['href'])\n",
    "        return news_urls\n",
    "    \n",
    "    URL = 'https://www.google.com/search'\n",
    "    n_url = scrape_news_urls(URL)\n",
    "    article_list = []\n",
    "    \n",
    "    for url in n_url:\n",
    "        if re.match(r'https://www.ft.com/',url) is not None:\n",
    "            from sys import exit\n",
    "            from selenium import webdriver\n",
    "            \n",
    "            # Function to access Financial Times(needs mandatory subscription) through Selenium enabled authentication \n",
    "            def open_browser():\n",
    "                global browser\n",
    "                browser = webdriver.Firefox()\n",
    "                browser.get('https://accounts.ft.com/login?location=https%3A%2F%2Fwww.ft.com%2F')\n",
    "                \n",
    "                username_input = input(\"Please enter your username (email address): \")\n",
    "                username_submit = browser.find_element_by_id(\"email\")\n",
    "                username_submit.send_keys(username_input)\n",
    "                browser.find_element_by_name(\"Next\").click()\n",
    "                \n",
    "                print(\"\"\"Please type in your password, then answer the following:\"\"\")\n",
    "                password_input = input(\"Please enter your password: \")\n",
    "                password_submit = browser.find_element_by_id(\"password\")\n",
    "                password_submit.send_keys(password_input)\n",
    "                confirmation = input(\"Have you typed in your password yet? (y/n): \")\n",
    "                \n",
    "                if confirmation == \"y\":\n",
    "                    browser.find_element_by_name(\"Sign in\").click() #submit\n",
    "                else:\n",
    "                    browser.close()\n",
    "                    exit()\n",
    "            \n",
    "            def article_search(url):\n",
    "                browser.get(url)\n",
    "                art = browser.find_element_by_xpath(\"//*[@id='site-content']/div[3]\")\n",
    "                article_list.append(art.text.rstrip('\\n').replace('\\n', ' '))\n",
    "            \n",
    "            # Uncomment if you are running for the first time to ensure access via authentication \n",
    "            def start(url):\n",
    "#                 choice = input(\"Do you need to log into FT.com? (y/n): \")\n",
    "#                 if choice == \"y\":\n",
    "#                     open_browser()    \n",
    "#                 elif choice == \"n\":\n",
    "                article_search(url)        \n",
    "#                 else:\n",
    "#                     exit()\n",
    "\n",
    "            start(url)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                from newspaper import Article\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article_list.append(article.text)\n",
    "            except:\n",
    "                article_list.append(\"\")\n",
    "\n",
    "    news_dictionary[dt] = article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all dates were processed \n",
    "print(news_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MONTH = AUGUST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.datetime(2016, 8, 1)\n",
    "end = datetime.datetime(2016, 9, 1)\n",
    "step = datetime.timedelta(days=1)\n",
    "\n",
    "dates_list = []\n",
    "news_dictionary={}\n",
    "\n",
    "while dt < end:\n",
    "    dates_list.append(dt.strftime('%m/%d/%Y'))\n",
    "    dt += step\n",
    "\n",
    "for dt in dates_list:\n",
    "    def scrape_news_urls(link):\n",
    "        time.sleep(randint(0, 20))\n",
    "        payload = {'as_epq': 'bitcoin', 'tbs':'cdr:1,cd_min:'+dt+',cd_max:'+dt+\"'\",'tbm':'nws'}\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'}\n",
    "        r = requests.get(link, headers=headers, params=payload)\n",
    "        print(r.status_code)  # Print the status code\n",
    "        content = r.text\n",
    "        news_urls = []\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        news_url = soup.findAll(\"a\", {\"class\": \"l _PMs\"}, href=True)\n",
    "        for url in news_url:\n",
    "            news_urls.append(url['href'])\n",
    "        return news_urls\n",
    "    \n",
    "    URL = 'https://www.google.com/search'\n",
    "    n_url = scrape_news_urls(URL)\n",
    "    article_list = []\n",
    "    \n",
    "    for url in n_url:\n",
    "        if re.match(r'https://www.ft.com/',url) is not None:\n",
    "            from sys import exit\n",
    "            from selenium import webdriver\n",
    "            \n",
    "            # Function to access Financial Times(needs mandatory subscription) through Selenium enabled authentication \n",
    "            def open_browser():\n",
    "                global browser\n",
    "                browser = webdriver.Firefox()\n",
    "                browser.get('https://accounts.ft.com/login?location=https%3A%2F%2Fwww.ft.com%2F')\n",
    "                \n",
    "                username_input = input(\"Please enter your username (email address): \")\n",
    "                username_submit = browser.find_element_by_id(\"email\")\n",
    "                username_submit.send_keys(username_input)\n",
    "                browser.find_element_by_name(\"Next\").click()\n",
    "                \n",
    "                print(\"\"\"Please type in your password, then answer the following:\"\"\")\n",
    "                password_input = input(\"Please enter your password: \")\n",
    "                password_submit = browser.find_element_by_id(\"password\")\n",
    "                password_submit.send_keys(password_input)\n",
    "                confirmation = input(\"Have you typed in your password yet? (y/n): \")\n",
    "                \n",
    "                if confirmation == \"y\":\n",
    "                    browser.find_element_by_name(\"Sign in\").click() #submit\n",
    "                else:\n",
    "                    browser.close()\n",
    "                    exit()\n",
    "            \n",
    "            def article_search(url):\n",
    "                browser.get(url)\n",
    "                art = browser.find_element_by_xpath(\"//*[@id='site-content']/div[3]\")\n",
    "                article_list.append(art.text.rstrip('\\n').replace('\\n', ' '))\n",
    "            \n",
    "            # Uncomment if you are running for the first time to ensure access via authentication \n",
    "            def start(url):\n",
    "                choice = input(\"Do you need to log into FT.com? (y/n): \")\n",
    "                if choice == \"y\":\n",
    "                    open_browser()    \n",
    "                elif choice == \"n\":\n",
    "                    article_search(url)        \n",
    "                else:\n",
    "                    exit()\n",
    "\n",
    "            start(url)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                from newspaper import Article\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article_list.append(article.text)\n",
    "            except:\n",
    "                article_list.append(\"\")\n",
    "\n",
    "    news_dictionary[dt] = article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all dates were processed \n",
    "print(news_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MONTH = SEPTEMBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.datetime(2016, 9, 1)\n",
    "end = datetime.datetime(2016, 10, 1)\n",
    "step = datetime.timedelta(days=1)\n",
    "\n",
    "dates_list = []\n",
    "news_dictionary={}\n",
    "\n",
    "while dt < end:\n",
    "    dates_list.append(dt.strftime('%m/%d/%Y'))\n",
    "    dt += step\n",
    "\n",
    "for dt in dates_list:\n",
    "    def scrape_news_urls(link):\n",
    "        time.sleep(randint(0, 20))\n",
    "        payload = {'as_epq': 'bitcoin', 'tbs':'cdr:1,cd_min:'+dt+',cd_max:'+dt+\"'\",'tbm':'nws'}\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'}\n",
    "        r = requests.get(link, headers=headers, params=payload)\n",
    "        print(r.status_code)  # Print the status code\n",
    "        content = r.text\n",
    "        news_urls = []\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        news_url = soup.findAll(\"a\", {\"class\": \"l _PMs\"}, href=True)\n",
    "        for url in news_url:\n",
    "            news_urls.append(url['href'])\n",
    "        return news_urls\n",
    "    \n",
    "    URL = 'https://www.google.com/search'\n",
    "    n_url = scrape_news_urls(URL)\n",
    "    article_list = []\n",
    "    \n",
    "    for url in n_url:\n",
    "        if re.match(r'https://www.ft.com/',url) is not None:\n",
    "            from sys import exit\n",
    "            from selenium import webdriver\n",
    "            \n",
    "            # Function to access Financial Times(needs mandatory subscription) through Selenium enabled authentication \n",
    "            def open_browser():\n",
    "                global browser\n",
    "                browser = webdriver.Firefox()\n",
    "                browser.get('https://accounts.ft.com/login?location=https%3A%2F%2Fwww.ft.com%2F')\n",
    "                \n",
    "                username_input = input(\"Please enter your username (email address): \")\n",
    "                username_submit = browser.find_element_by_id(\"email\")\n",
    "                username_submit.send_keys(username_input)\n",
    "                browser.find_element_by_name(\"Next\").click()\n",
    "                \n",
    "                print(\"\"\"Please type in your password, then answer the following:\"\"\")\n",
    "                password_input = input(\"Please enter your password: \")\n",
    "                password_submit = browser.find_element_by_id(\"password\")\n",
    "                password_submit.send_keys(password_input)\n",
    "                confirmation = input(\"Have you typed in your password yet? (y/n): \")\n",
    "                \n",
    "                if confirmation == \"y\":\n",
    "                    browser.find_element_by_name(\"Sign in\").click() #submit\n",
    "                else:\n",
    "                    browser.close()\n",
    "                    exit()\n",
    "            \n",
    "            def article_search(url):\n",
    "                browser.get(url)\n",
    "                art = browser.find_element_by_xpath(\"//*[@id='site-content']/div[3]\")\n",
    "                article_list.append(art.text.rstrip('\\n').replace('\\n', ' '))\n",
    "            \n",
    "            # Uncomment if you are running for the first time to ensure access via authentication \n",
    "            def start(url):\n",
    "#                 choice = input(\"Do you need to log into FT.com? (y/n): \")\n",
    "#                 if choice == \"y\":\n",
    "#                     open_browser()    \n",
    "#                 elif choice == \"n\":\n",
    "                article_search(url)        \n",
    "#                 else:\n",
    "#                     exit()\n",
    "\n",
    "            start(url)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                from newspaper import Article\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article_list.append(article.text)\n",
    "            except:\n",
    "                article_list.append(\"\")\n",
    "\n",
    "    news_dictionary[dt] = article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all dates were processed \n",
    "print(news_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MONTH = OCTOBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.datetime(2016, 10, 1)\n",
    "end = datetime.datetime(2016, 11, 1)\n",
    "step = datetime.timedelta(days=1)\n",
    "\n",
    "dates_list = []\n",
    "news_dictionary={}\n",
    "\n",
    "while dt < end:\n",
    "    dates_list.append(dt.strftime('%m/%d/%Y'))\n",
    "    dt += step\n",
    "\n",
    "for dt in dates_list:\n",
    "    def scrape_news_urls(link):\n",
    "        time.sleep(randint(0, 20))\n",
    "        payload = {'as_epq': 'bitcoin', 'tbs':'cdr:1,cd_min:'+dt+',cd_max:'+dt+\"'\",'tbm':'nws'}\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'}\n",
    "        r = requests.get(link, headers=headers, params=payload)\n",
    "        print(r.status_code)  # Print the status code\n",
    "        content = r.text\n",
    "        news_urls = []\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        news_url = soup.findAll(\"a\", {\"class\": \"l _PMs\"}, href=True)\n",
    "        for url in news_url:\n",
    "            news_urls.append(url['href'])\n",
    "        return news_urls\n",
    "    \n",
    "    URL = 'https://www.google.com/search'\n",
    "    n_url = scrape_news_urls(URL)\n",
    "    article_list = []\n",
    "    \n",
    "    for url in n_url:\n",
    "        if re.match(r'https://www.ft.com/',url) is not None:\n",
    "            from sys import exit\n",
    "            from selenium import webdriver\n",
    "            \n",
    "            # Function to access Financial Times(needs mandatory subscription) through Selenium enabled authentication \n",
    "            def open_browser():\n",
    "                global browser\n",
    "                browser = webdriver.Firefox()\n",
    "                browser.get('https://accounts.ft.com/login?location=https%3A%2F%2Fwww.ft.com%2F')\n",
    "                \n",
    "                username_input = input(\"Please enter your username (email address): \")\n",
    "                username_submit = browser.find_element_by_id(\"email\")\n",
    "                username_submit.send_keys(username_input)\n",
    "                browser.find_element_by_name(\"Next\").click()\n",
    "                \n",
    "                print(\"\"\"Please type in your password, then answer the following:\"\"\")\n",
    "                password_input = input(\"Please enter your password: \")\n",
    "                password_submit = browser.find_element_by_id(\"password\")\n",
    "                password_submit.send_keys(password_input)\n",
    "                confirmation = input(\"Have you typed in your password yet? (y/n): \")\n",
    "                \n",
    "                if confirmation == \"y\":\n",
    "                    browser.find_element_by_name(\"Sign in\").click() #submit\n",
    "                else:\n",
    "                    browser.close()\n",
    "                    exit()\n",
    "            \n",
    "            def article_search(url):\n",
    "                browser.get(url)\n",
    "                art = browser.find_element_by_xpath(\"//*[@id='site-content']/div[3]\")\n",
    "                article_list.append(art.text.rstrip('\\n').replace('\\n', ' '))\n",
    "            \n",
    "            # Uncomment if you are running for the first time to ensure access via authentication \n",
    "            def start(url):\n",
    "#                 choice = input(\"Do you need to log into FT.com? (y/n): \")\n",
    "#                 if choice == \"y\":\n",
    "#                     open_browser()    \n",
    "#                 elif choice == \"n\":\n",
    "                article_search(url)        \n",
    "#                 else:\n",
    "#                     exit()\n",
    "\n",
    "            start(url)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                from newspaper import Article\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article_list.append(article.text)\n",
    "            except:\n",
    "                article_list.append(\"\")\n",
    "\n",
    "    news_dictionary[dt] = article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all dates were processed \n",
    "print(news_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## MONTH = NOVEMBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.datetime(2016, 11, 1)\n",
    "end = datetime.datetime(2016, 12, 1)\n",
    "step = datetime.timedelta(days=1)\n",
    "\n",
    "dates_list = []\n",
    "\n",
    "while dt < end:\n",
    "    dates_list.append(dt.strftime('%m/%d/%Y'))\n",
    "    dt += step\n",
    "\n",
    "for dt in dates_list:\n",
    "    def scrape_news_urls(link):\n",
    "        time.sleep(randint(0, 20))\n",
    "        payload = {'as_epq': 'bitcoin', 'tbs':'cdr:1,cd_min:'+dt+',cd_max:'+dt+\"'\",'tbm':'nws'}\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'}\n",
    "        r = requests.get(link, headers=headers, params=payload)\n",
    "        print(r.status_code)  # Print the status code\n",
    "        content = r.text\n",
    "        news_urls = []\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        news_url = soup.findAll(\"a\", {\"class\": \"l _PMs\"}, href=True)\n",
    "        for url in news_url:\n",
    "            news_urls.append(url['href'])\n",
    "        return news_urls\n",
    "    \n",
    "    URL = 'https://www.google.com/search'\n",
    "    n_url = scrape_news_urls(URL)\n",
    "    article_list = []\n",
    "    \n",
    "    for url in n_url:\n",
    "        if re.match(r'https://www.ft.com/',url) is not None:\n",
    "            from sys import exit\n",
    "            from selenium import webdriver\n",
    "            \n",
    "            # Function to access Financial Times(needs mandatory subscription) through Selenium enabled authentication \n",
    "            def open_browser():\n",
    "                global browser\n",
    "                browser = webdriver.Firefox()\n",
    "                browser.get('https://accounts.ft.com/login?location=https%3A%2F%2Fwww.ft.com%2F')\n",
    "                \n",
    "                username_input = input(\"Please enter your username (email address): \")\n",
    "                username_submit = browser.find_element_by_id(\"email\")\n",
    "                username_submit.send_keys(username_input)\n",
    "                browser.find_element_by_name(\"Next\").click()\n",
    "                \n",
    "                print(\"\"\"Please type in your password, then answer the following:\"\"\")\n",
    "                password_input = input(\"Please enter your password: \")\n",
    "                password_submit = browser.find_element_by_id(\"password\")\n",
    "                password_submit.send_keys(password_input)\n",
    "                confirmation = input(\"Have you typed in your password yet? (y/n): \")\n",
    "                \n",
    "                if confirmation == \"y\":\n",
    "                    browser.find_element_by_name(\"Sign in\").click() #submit\n",
    "                else:\n",
    "                    browser.close()\n",
    "                    exit()\n",
    "            \n",
    "            def article_search(url):\n",
    "                browser.get(url)\n",
    "                art = browser.find_element_by_xpath(\"//*[@id='site-content']/div[3]\")\n",
    "                article_list.append(art.text.rstrip('\\n').replace('\\n', ' '))\n",
    "            \n",
    "            # Uncomment if you are running for the first time to ensure access via authentication \n",
    "            def start(url):\n",
    "#                 choice = input(\"Do you need to log into FT.com? (y/n): \")\n",
    "#                 if choice == \"y\":\n",
    "#                     open_browser()    \n",
    "#                 elif choice == \"n\":\n",
    "                article_search(url)        \n",
    "#                 else:\n",
    "#                     exit()\n",
    "\n",
    "            start(url)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                from newspaper import Article\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article_list.append(article.text)\n",
    "            except:\n",
    "                article_list.append(\"\")\n",
    "\n",
    "    news_dictionary[dt] = article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all dates were processed \n",
    "print(news_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MONTH = DECEMBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.datetime(2016, 12, 1)\n",
    "end = datetime.datetime(2017, 1, 1)\n",
    "step = datetime.timedelta(days=1)\n",
    "\n",
    "dates_list = []\n",
    "\n",
    "while dt < end:\n",
    "    dates_list.append(dt.strftime('%m/%d/%Y'))\n",
    "    dt += step\n",
    "\n",
    "for dt in dates_list:\n",
    "    def scrape_news_urls(link):\n",
    "        time.sleep(randint(0, 20))\n",
    "        payload = {'as_epq': 'bitcoin', 'tbs':'cdr:1,cd_min:'+dt+',cd_max:'+dt+\"'\",'tbm':'nws'}\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'}\n",
    "        r = requests.get(link, headers=headers, params=payload)\n",
    "        print(r.status_code)  # Print the status code\n",
    "        content = r.text\n",
    "        news_urls = []\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        news_url = soup.findAll(\"a\", {\"class\": \"l _PMs\"}, href=True)\n",
    "        for url in news_url:\n",
    "            news_urls.append(url['href'])\n",
    "        return news_urls\n",
    "    \n",
    "    URL = 'https://www.google.com/search'\n",
    "    n_url = scrape_news_urls(URL)\n",
    "    article_list = []\n",
    "    \n",
    "    for url in n_url:\n",
    "        if re.match(r'https://www.ft.com/',url) is not None:\n",
    "            from sys import exit\n",
    "            from selenium import webdriver\n",
    "            \n",
    "            # Function to access Financial Times(needs mandatory subscription) through Selenium enabled authentication \n",
    "            def open_browser():\n",
    "                global browser\n",
    "                browser = webdriver.Firefox()\n",
    "                browser.get('https://accounts.ft.com/login?location=https%3A%2F%2Fwww.ft.com%2F')\n",
    "                \n",
    "                username_input = input(\"Please enter your username (email address): \")\n",
    "                username_submit = browser.find_element_by_id(\"email\")\n",
    "                username_submit.send_keys(username_input)\n",
    "                browser.find_element_by_name(\"Next\").click()\n",
    "                \n",
    "                print(\"\"\"Please type in your password, then answer the following:\"\"\")\n",
    "                password_input = input(\"Please enter your password: \")\n",
    "                password_submit = browser.find_element_by_id(\"password\")\n",
    "                password_submit.send_keys(password_input)\n",
    "                confirmation = input(\"Have you typed in your password yet? (y/n): \")\n",
    "                \n",
    "                if confirmation == \"y\":\n",
    "                    browser.find_element_by_name(\"Sign in\").click() #submit\n",
    "                else:\n",
    "                    browser.close()\n",
    "                    exit()\n",
    "            \n",
    "            def article_search(url):\n",
    "                browser.get(url)\n",
    "                art = browser.find_element_by_xpath(\"//*[@id='site-content']/div[3]\")\n",
    "                article_list.append(art.text.rstrip('\\n').replace('\\n', ' '))\n",
    "            \n",
    "            # Uncomment if you are running for the first time to ensure access via authentication \n",
    "            def start(url):\n",
    "#                 choice = input(\"Do you need to log into FT.com? (y/n): \")\n",
    "#                 if choice == \"y\":\n",
    "#                     open_browser()    \n",
    "#                 elif choice == \"n\":\n",
    "                article_search(url)        \n",
    "#                 else:\n",
    "#                     exit()\n",
    "\n",
    "            start(url)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                from newspaper import Article\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article_list.append(article.text)\n",
    "            except:\n",
    "                article_list.append(\"\")\n",
    "\n",
    "    news_dictionary[dt] = article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all dates were processed \n",
    "print(news_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YEAR = 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MONTH = JANUARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "dt = datetime.datetime(2017, 1, 1)\n",
    "end = datetime.datetime(2017, 2, 1)\n",
    "step = datetime.timedelta(days=1)\n",
    "\n",
    "dates_list = []\n",
    "news_dictionary={}\n",
    "\n",
    "while dt < end:\n",
    "    dates_list.append(dt.strftime('%m/%d/%Y'))\n",
    "    dt += step\n",
    "\n",
    "for dt in dates_list:\n",
    "    def scrape_news_urls(link):\n",
    "        time.sleep(randint(0, 20))\n",
    "        payload = {'as_epq': 'bitcoin', 'tbs':'cdr:1,cd_min:'+dt+',cd_max:'+dt+\"'\",'tbm':'nws'}\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'}\n",
    "        r = requests.get(link, headers=headers, params=payload)\n",
    "        print(r.status_code)  # Print the status code\n",
    "        content = r.text\n",
    "        news_urls = []\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        news_url = soup.findAll(\"a\", {\"class\": \"l _PMs\"}, href=True)\n",
    "        for url in news_url:\n",
    "            news_urls.append(url['href'])\n",
    "        return news_urls\n",
    "    \n",
    "    URL = 'https://www.google.com/search'\n",
    "    n_url = scrape_news_urls(URL)\n",
    "    article_list = []\n",
    "    \n",
    "    for url in n_url:\n",
    "        if re.match(r'https://www.ft.com/',url) is not None:\n",
    "            from sys import exit\n",
    "            from selenium import webdriver\n",
    "            \n",
    "            # Function to access Financial Times(needs mandatory subscription) through Selenium enabled authentication \n",
    "            def open_browser():\n",
    "                global browser\n",
    "                browser = webdriver.Firefox()\n",
    "                browser.get('https://accounts.ft.com/login?location=https%3A%2F%2Fwww.ft.com%2F')\n",
    "                \n",
    "                username_input = input(\"Please enter your username (email address): \")\n",
    "                username_submit = browser.find_element_by_id(\"email\")\n",
    "                username_submit.send_keys(username_input)\n",
    "                browser.find_element_by_name(\"Next\").click()\n",
    "                \n",
    "                print(\"\"\"Please type in your password, then answer the following:\"\"\")\n",
    "                password_input = input(\"Please enter your password: \")\n",
    "                password_submit = browser.find_element_by_id(\"password\")\n",
    "                password_submit.send_keys(password_input)\n",
    "                confirmation = input(\"Have you typed in your password yet? (y/n): \")\n",
    "                \n",
    "                if confirmation == \"y\":\n",
    "                    browser.find_element_by_name(\"Sign in\").click() #submit\n",
    "                else:\n",
    "                    browser.close()\n",
    "                    exit()\n",
    "            \n",
    "            def article_search(url):\n",
    "                browser.get(url)\n",
    "                art = browser.find_element_by_xpath(\"//*[@id='site-content']/div[3]\")\n",
    "                article_list.append(art.text.rstrip('\\n').replace('\\n', ' '))\n",
    "            \n",
    "            # Uncomment if you are running for the first time to ensure access via authentication \n",
    "            def start(url):\n",
    "#                 choice = input(\"Do you need to log into FT.com? (y/n): \")\n",
    "#                 if choice == \"y\":\n",
    "#                     open_browser()    \n",
    "#                 elif choice == \"n\":\n",
    "                article_search(url)        \n",
    "#                 else:\n",
    "#                     exit()\n",
    "\n",
    "            start(url)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                from newspaper import Article\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article_list.append(article.text)\n",
    "            except:\n",
    "                article_list.append(\"\")\n",
    "\n",
    "    news_dictionary[dt] = article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all dates were processed \n",
    "print(news_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MONTH = FEBRUARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.datetime(2017, 2, 1)\n",
    "end = datetime.datetime(2017, 3, 1)\n",
    "step = datetime.timedelta(days=1)\n",
    "\n",
    "dates_list = []\n",
    "news_dictionary={}\n",
    "\n",
    "while dt < end:\n",
    "    dates_list.append(dt.strftime('%m/%d/%Y'))\n",
    "    dt += step\n",
    "\n",
    "for dt in dates_list:\n",
    "    def scrape_news_urls(link):\n",
    "        time.sleep(randint(0, 20))\n",
    "        payload = {'as_epq': 'bitcoin', 'tbs':'cdr:1,cd_min:'+dt+',cd_max:'+dt+\"'\",'tbm':'nws'}\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'}\n",
    "        r = requests.get(link, headers=headers, params=payload)\n",
    "        print(r.status_code)  # Print the status code\n",
    "        content = r.text\n",
    "        news_urls = []\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        news_url = soup.findAll(\"a\", {\"class\": \"l _PMs\"}, href=True)\n",
    "        for url in news_url:\n",
    "            news_urls.append(url['href'])\n",
    "        return news_urls\n",
    "    \n",
    "    URL = 'https://www.google.com/search'\n",
    "    n_url = scrape_news_urls(URL)\n",
    "    article_list = []\n",
    "    \n",
    "    for url in n_url:\n",
    "        if re.match(r'https://www.ft.com/',url) is not None:\n",
    "            from sys import exit\n",
    "            from selenium import webdriver\n",
    "            \n",
    "            # Function to access Financial Times(needs mandatory subscription) through Selenium enabled authentication \n",
    "            def open_browser():\n",
    "                global browser\n",
    "                browser = webdriver.Firefox()\n",
    "                browser.get('https://accounts.ft.com/login?location=https%3A%2F%2Fwww.ft.com%2F')\n",
    "                \n",
    "                username_input = input(\"Please enter your username (email address): \")\n",
    "                username_submit = browser.find_element_by_id(\"email\")\n",
    "                username_submit.send_keys(username_input)\n",
    "                browser.find_element_by_name(\"Next\").click()\n",
    "                \n",
    "                print(\"\"\"Please type in your password, then answer the following:\"\"\")\n",
    "                password_input = input(\"Please enter your password: \")\n",
    "                password_submit = browser.find_element_by_id(\"password\")\n",
    "                password_submit.send_keys(password_input)\n",
    "                confirmation = input(\"Have you typed in your password yet? (y/n): \")\n",
    "                \n",
    "                if confirmation == \"y\":\n",
    "                    browser.find_element_by_name(\"Sign in\").click() #submit\n",
    "                else:\n",
    "                    browser.close()\n",
    "                    exit()\n",
    "            \n",
    "            def article_search(url):\n",
    "                browser.get(url)\n",
    "                art = browser.find_element_by_xpath(\"//*[@id='site-content']/div[3]\")\n",
    "                article_list.append(art.text.rstrip('\\n').replace('\\n', ' '))\n",
    "            \n",
    "            # Uncomment if you are running for the first time to ensure access via authentication \n",
    "            def start(url):\n",
    "#                 choice = input(\"Do you need to log into FT.com? (y/n): \")\n",
    "#                 if choice == \"y\":\n",
    "#                     open_browser()    \n",
    "#                 elif choice == \"n\":\n",
    "                article_search(url)        \n",
    "#                 else:\n",
    "#                     exit()\n",
    "\n",
    "            start(url)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                from newspaper import Article\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article_list.append(article.text)\n",
    "            except:\n",
    "                article_list.append(\"\")\n",
    "\n",
    "    news_dictionary[dt] = article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all dates were processed \n",
    "print(news_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MONTH = MARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.datetime(2017, 3, 1)\n",
    "end = datetime.datetime(2017, 4, 1)\n",
    "step = datetime.timedelta(days=1)\n",
    "\n",
    "dates_list = []\n",
    "news_dictionary={}\n",
    "\n",
    "while dt < end:\n",
    "    dates_list.append(dt.strftime('%m/%d/%Y'))\n",
    "    dt += step\n",
    "\n",
    "for dt in dates_list:\n",
    "    def scrape_news_urls(link):\n",
    "        time.sleep(randint(0, 20))\n",
    "        payload = {'as_epq': 'bitcoin', 'tbs':'cdr:1,cd_min:'+dt+',cd_max:'+dt+\"'\",'tbm':'nws'}\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'}\n",
    "        r = requests.get(link, headers=headers, params=payload)\n",
    "        print(r.status_code)  # Print the status code\n",
    "        content = r.text\n",
    "        news_urls = []\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        news_url = soup.findAll(\"a\", {\"class\": \"l _PMs\"}, href=True)\n",
    "        for url in news_url:\n",
    "            news_urls.append(url['href'])\n",
    "        return news_urls\n",
    "    \n",
    "    URL = 'https://www.google.com/search'\n",
    "    n_url = scrape_news_urls(URL)\n",
    "    article_list = []\n",
    "    \n",
    "    for url in n_url:\n",
    "        if re.match(r'https://www.ft.com/',url) is not None:\n",
    "            from sys import exit\n",
    "            from selenium import webdriver\n",
    "            \n",
    "            # Function to access Financial Times(needs mandatory subscription) through Selenium enabled authentication \n",
    "            def open_browser():\n",
    "                global browser\n",
    "                browser = webdriver.Firefox()\n",
    "                browser.get('https://accounts.ft.com/login?location=https%3A%2F%2Fwww.ft.com%2F')\n",
    "                \n",
    "                username_input = input(\"Please enter your username (email address): \")\n",
    "                username_submit = browser.find_element_by_id(\"email\")\n",
    "                username_submit.send_keys(username_input)\n",
    "                browser.find_element_by_name(\"Next\").click()\n",
    "                \n",
    "                print(\"\"\"Please type in your password, then answer the following:\"\"\")\n",
    "                password_input = input(\"Please enter your password: \")\n",
    "                password_submit = browser.find_element_by_id(\"password\")\n",
    "                password_submit.send_keys(password_input)\n",
    "                confirmation = input(\"Have you typed in your password yet? (y/n): \")\n",
    "                \n",
    "                if confirmation == \"y\":\n",
    "                    browser.find_element_by_name(\"Sign in\").click() #submit\n",
    "                else:\n",
    "                    browser.close()\n",
    "                    exit()\n",
    "            \n",
    "            def article_search(url):\n",
    "                browser.get(url)\n",
    "                art = browser.find_element_by_xpath(\"//*[@id='site-content']/div[3]\")\n",
    "                article_list.append(art.text.rstrip('\\n').replace('\\n', ' '))\n",
    "            \n",
    "            # Uncomment if you are running for the first time to ensure access via authentication \n",
    "            def start(url):\n",
    "#                 choice = input(\"Do you need to log into FT.com? (y/n): \")\n",
    "#                 if choice == \"y\":\n",
    "#                     open_browser()    \n",
    "#                 elif choice == \"n\":\n",
    "                article_search(url)        \n",
    "#                 else:\n",
    "#                     exit()\n",
    "\n",
    "            start(url)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                from newspaper import Article\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article_list.append(article.text)\n",
    "            except:\n",
    "                article_list.append(\"\")\n",
    "\n",
    "    news_dictionary[dt] = article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all dates were processed \n",
    "print(news_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MONTH = APRIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.datetime(2017, 4, 1)\n",
    "end = datetime.datetime(2017, 5, 1)\n",
    "step = datetime.timedelta(days=1)\n",
    "\n",
    "dates_list = []\n",
    "\n",
    "while dt < end:\n",
    "    dates_list.append(dt.strftime('%m/%d/%Y'))\n",
    "    dt += step\n",
    "\n",
    "for dt in dates_list:\n",
    "    def scrape_news_urls(link):\n",
    "        time.sleep(randint(0, 20))\n",
    "        payload = {'as_epq': 'bitcoin', 'tbs':'cdr:1,cd_min:'+dt+',cd_max:'+dt+\"'\",'tbm':'nws'}\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'}\n",
    "        r = requests.get(link, headers=headers, params=payload)\n",
    "        print(r.status_code)  # Print the status code\n",
    "        content = r.text\n",
    "        news_urls = []\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        news_url = soup.findAll(\"a\", {\"class\": \"l _PMs\"}, href=True)\n",
    "        for url in news_url:\n",
    "            news_urls.append(url['href'])\n",
    "        return news_urls\n",
    "    \n",
    "    URL = 'https://www.google.com/search'\n",
    "    n_url = scrape_news_urls(URL)\n",
    "    article_list = []\n",
    "    \n",
    "    for url in n_url:\n",
    "        if re.match(r'https://www.ft.com/',url) is not None:\n",
    "            from sys import exit\n",
    "            from selenium import webdriver\n",
    "            \n",
    "            # Function to access Financial Times(needs mandatory subscription) through Selenium enabled authentication \n",
    "            def open_browser():\n",
    "                global browser\n",
    "                browser = webdriver.Firefox()\n",
    "                browser.get('https://accounts.ft.com/login?location=https%3A%2F%2Fwww.ft.com%2F')\n",
    "                \n",
    "                username_input = input(\"Please enter your username (email address): \")\n",
    "                username_submit = browser.find_element_by_id(\"email\")\n",
    "                username_submit.send_keys(username_input)\n",
    "                browser.find_element_by_name(\"Next\").click()\n",
    "                \n",
    "                print(\"\"\"Please type in your password, then answer the following:\"\"\")\n",
    "                password_input = input(\"Please enter your password: \")\n",
    "                password_submit = browser.find_element_by_id(\"password\")\n",
    "                password_submit.send_keys(password_input)\n",
    "                confirmation = input(\"Have you typed in your password yet? (y/n): \")\n",
    "                \n",
    "                if confirmation == \"y\":\n",
    "                    browser.find_element_by_name(\"Sign in\").click() #submit\n",
    "                else:\n",
    "                    browser.close()\n",
    "                    exit()\n",
    "            \n",
    "            def article_search(url):\n",
    "                browser.get(url)\n",
    "                art = browser.find_element_by_xpath(\"//*[@id='site-content']/div[3]\")\n",
    "                article_list.append(art.text.rstrip('\\n').replace('\\n', ' '))\n",
    "            \n",
    "            # Uncomment if you are running for the first time to ensure access via authentication \n",
    "            def start(url):\n",
    "#                 choice = input(\"Do you need to log into FT.com? (y/n): \")\n",
    "#                 if choice == \"y\":\n",
    "#                     open_browser()    \n",
    "#                 elif choice == \"n\":\n",
    "                article_search(url)        \n",
    "#                 else:\n",
    "#                     exit()\n",
    "\n",
    "            start(url)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                from newspaper import Article\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article_list.append(article.text)\n",
    "            except:\n",
    "                article_list.append(\"\")\n",
    "\n",
    "    news_dictionary[dt] = article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all dates were processed \n",
    "print(news_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MONTH = MAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.datetime(2017, 5, 1)\n",
    "end = datetime.datetime(2017, 6, 1)\n",
    "step = datetime.timedelta(days=1)\n",
    "\n",
    "dates_list = []\n",
    "\n",
    "while dt < end:\n",
    "    dates_list.append(dt.strftime('%m/%d/%Y'))\n",
    "    dt += step\n",
    "\n",
    "for dt in dates_list:\n",
    "    def scrape_news_urls(link):\n",
    "        time.sleep(randint(0, 20))\n",
    "        payload = {'as_epq': 'bitcoin', 'tbs':'cdr:1,cd_min:'+dt+',cd_max:'+dt+\"'\",'tbm':'nws'}\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'}\n",
    "        r = requests.get(link, headers=headers, params=payload)\n",
    "        print(r.status_code)  # Print the status code\n",
    "        content = r.text\n",
    "        news_urls = []\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        news_url = soup.findAll(\"a\", {\"class\": \"l _PMs\"}, href=True)\n",
    "        for url in news_url:\n",
    "            news_urls.append(url['href'])\n",
    "        return news_urls\n",
    "    \n",
    "    URL = 'https://www.google.com/search'\n",
    "    n_url = scrape_news_urls(URL)\n",
    "    article_list = []\n",
    "    \n",
    "    for url in n_url:\n",
    "        if re.match(r'https://www.ft.com/',url) is not None:\n",
    "            from sys import exit\n",
    "            from selenium import webdriver\n",
    "            \n",
    "            # Function to access Financial Times(needs mandatory subscription) through Selenium enabled authentication \n",
    "            def open_browser():\n",
    "                global browser\n",
    "                browser = webdriver.Firefox()\n",
    "                browser.get('https://accounts.ft.com/login?location=https%3A%2F%2Fwww.ft.com%2F')\n",
    "                \n",
    "                username_input = input(\"Please enter your username (email address): \")\n",
    "                username_submit = browser.find_element_by_id(\"email\")\n",
    "                username_submit.send_keys(username_input)\n",
    "                browser.find_element_by_name(\"Next\").click()\n",
    "                \n",
    "                print(\"\"\"Please type in your password, then answer the following:\"\"\")\n",
    "                password_input = input(\"Please enter your password: \")\n",
    "                password_submit = browser.find_element_by_id(\"password\")\n",
    "                password_submit.send_keys(password_input)\n",
    "                confirmation = input(\"Have you typed in your password yet? (y/n): \")\n",
    "                \n",
    "                if confirmation == \"y\":\n",
    "                    browser.find_element_by_name(\"Sign in\").click() #submit\n",
    "                else:\n",
    "                    browser.close()\n",
    "                    exit()\n",
    "            \n",
    "            def article_search(url):\n",
    "                browser.get(url)\n",
    "                art = browser.find_element_by_xpath(\"//*[@id='site-content']/div[3]\")\n",
    "                article_list.append(art.text.rstrip('\\n').replace('\\n', ' '))\n",
    "            \n",
    "            # Uncomment if you are running for the first time to ensure access via authentication \n",
    "            def start(url):\n",
    "#                 choice = input(\"Do you need to log into FT.com? (y/n): \")\n",
    "#                 if choice == \"y\":\n",
    "#                     open_browser()    \n",
    "#                 elif choice == \"n\":\n",
    "                article_search(url)        \n",
    "#                 else:\n",
    "#                     exit()\n",
    "\n",
    "            start(url)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                from newspaper import Article\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article_list.append(article.text)\n",
    "            except:\n",
    "                article_list.append(\"\")\n",
    "\n",
    "    news_dictionary[dt] = article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all dates were processed \n",
    "print(news_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MONTH = JUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.datetime(2017, 6, 1)\n",
    "end = datetime.datetime(2017, 7, 1)\n",
    "step = datetime.timedelta(days=1)\n",
    "\n",
    "dates_list = []\n",
    "\n",
    "while dt < end:\n",
    "    dates_list.append(dt.strftime('%m/%d/%Y'))\n",
    "    dt += step\n",
    "\n",
    "for dt in dates_list:\n",
    "    def scrape_news_urls(link):\n",
    "        time.sleep(randint(0, 20))\n",
    "        payload = {'as_epq': 'bitcoin', 'tbs':'cdr:1,cd_min:'+dt+',cd_max:'+dt+\"'\",'tbm':'nws'}\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'}\n",
    "        r = requests.get(link, headers=headers, params=payload)\n",
    "        print(r.status_code)  # Print the status code\n",
    "        content = r.text\n",
    "        news_urls = []\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        news_url = soup.findAll(\"a\", {\"class\": \"l _PMs\"}, href=True)\n",
    "        for url in news_url:\n",
    "            news_urls.append(url['href'])\n",
    "        return news_urls\n",
    "    \n",
    "    URL = 'https://www.google.com/search'\n",
    "    n_url = scrape_news_urls(URL)\n",
    "    article_list = []\n",
    "    \n",
    "    for url in n_url:\n",
    "        if re.match(r'https://www.ft.com/',url) is not None:\n",
    "            from sys import exit\n",
    "            from selenium import webdriver\n",
    "            \n",
    "            # Function to access Financial Times(needs mandatory subscription) through Selenium enabled authentication \n",
    "            def open_browser():\n",
    "                global browser\n",
    "                browser = webdriver.Firefox()\n",
    "                browser.get('https://accounts.ft.com/login?location=https%3A%2F%2Fwww.ft.com%2F')\n",
    "                \n",
    "                username_input = input(\"Please enter your username (email address): \")\n",
    "                username_submit = browser.find_element_by_id(\"email\")\n",
    "                username_submit.send_keys(username_input)\n",
    "                browser.find_element_by_name(\"Next\").click()\n",
    "                \n",
    "                print(\"\"\"Please type in your password, then answer the following:\"\"\")\n",
    "                password_input = input(\"Please enter your password: \")\n",
    "                password_submit = browser.find_element_by_id(\"password\")\n",
    "                password_submit.send_keys(password_input)\n",
    "                confirmation = input(\"Have you typed in your password yet? (y/n): \")\n",
    "                \n",
    "                if confirmation == \"y\":\n",
    "                    browser.find_element_by_name(\"Sign in\").click() #submit\n",
    "                else:\n",
    "                    browser.close()\n",
    "                    exit()\n",
    "            \n",
    "            def article_search(url):\n",
    "                browser.get(url)\n",
    "                art = browser.find_element_by_xpath(\"//*[@id='site-content']/div[3]\")\n",
    "                article_list.append(art.text.rstrip('\\n').replace('\\n', ' '))\n",
    "            \n",
    "            # Uncomment if you are running for the first time to ensure access via authentication \n",
    "            def start(url):\n",
    "#                 choice = input(\"Do you need to log into FT.com? (y/n): \")\n",
    "#                 if choice == \"y\":\n",
    "#                     open_browser()    \n",
    "#                 elif choice == \"n\":\n",
    "                article_search(url)        \n",
    "#                 else:\n",
    "#                     exit()\n",
    "\n",
    "            start(url)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                from newspaper import Article\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article_list.append(article.text)\n",
    "            except:\n",
    "                article_list.append(\"\")\n",
    "\n",
    "    news_dictionary[dt] = article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all dates were processed \n",
    "print(news_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MONTH = JULY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.datetime(2017, 7, 1)\n",
    "end = datetime.datetime(2017, 8, 1)\n",
    "step = datetime.timedelta(days=1)\n",
    "\n",
    "dates_list = []\n",
    "\n",
    "while dt < end:\n",
    "    dates_list.append(dt.strftime('%m/%d/%Y'))\n",
    "    dt += step\n",
    "\n",
    "for dt in dates_list:\n",
    "    def scrape_news_urls(link):\n",
    "        time.sleep(randint(0, 20))\n",
    "        payload = {'as_epq': 'bitcoin', 'tbs':'cdr:1,cd_min:'+dt+',cd_max:'+dt+\"'\",'tbm':'nws'}\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'}\n",
    "        r = requests.get(link, headers=headers, params=payload)\n",
    "        print(r.status_code)  # Print the status code\n",
    "        content = r.text\n",
    "        news_urls = []\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        news_url = soup.findAll(\"a\", {\"class\": \"l _PMs\"}, href=True)\n",
    "        for url in news_url:\n",
    "            news_urls.append(url['href'])\n",
    "        return news_urls\n",
    "    \n",
    "    URL = 'https://www.google.com/search'\n",
    "    n_url = scrape_news_urls(URL)\n",
    "    article_list = []\n",
    "    \n",
    "    for url in n_url:\n",
    "        if re.match(r'https://www.ft.com/',url) is not None:\n",
    "            from sys import exit\n",
    "            from selenium import webdriver\n",
    "            \n",
    "            # Function to access Financial Times(needs mandatory subscription) through Selenium enabled authentication \n",
    "            def open_browser():\n",
    "                global browser\n",
    "                browser = webdriver.Firefox()\n",
    "                browser.get('https://accounts.ft.com/login?location=https%3A%2F%2Fwww.ft.com%2F')\n",
    "                \n",
    "                username_input = input(\"Please enter your username (email address): \")\n",
    "                username_submit = browser.find_element_by_id(\"email\")\n",
    "                username_submit.send_keys(username_input)\n",
    "                browser.find_element_by_name(\"Next\").click()\n",
    "                \n",
    "                print(\"\"\"Please type in your password, then answer the following:\"\"\")\n",
    "                password_input = input(\"Please enter your password: \")\n",
    "                password_submit = browser.find_element_by_id(\"password\")\n",
    "                password_submit.send_keys(password_input)\n",
    "                confirmation = input(\"Have you typed in your password yet? (y/n): \")\n",
    "                \n",
    "                if confirmation == \"y\":\n",
    "                    browser.find_element_by_name(\"Sign in\").click() #submit\n",
    "                else:\n",
    "                    browser.close()\n",
    "                    exit()\n",
    "            \n",
    "            def article_search(url):\n",
    "                browser.get(url)\n",
    "                art = browser.find_element_by_xpath(\"//*[@id='site-content']/div[3]\")\n",
    "                article_list.append(art.text.rstrip('\\n').replace('\\n', ' '))\n",
    "            \n",
    "            # Uncomment if you are running for the first time to ensure access via authentication \n",
    "            def start(url):\n",
    "#                 choice = input(\"Do you need to log into FT.com? (y/n): \")\n",
    "#                 if choice == \"y\":\n",
    "#                     open_browser()    \n",
    "#                 elif choice == \"n\":\n",
    "                article_search(url)        \n",
    "#                 else:\n",
    "#                     exit()\n",
    "\n",
    "            start(url)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                from newspaper import Article\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article_list.append(article.text)\n",
    "            except:\n",
    "                article_list.append(\"\")\n",
    "\n",
    "    news_dictionary[dt] = article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all dates were processed \n",
    "print(news_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MONTH = AUGUST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.datetime(2017, 8, 1)\n",
    "end = datetime.datetime(2017, 9, 1)\n",
    "step = datetime.timedelta(days=1)\n",
    "\n",
    "dates_list = []\n",
    "\n",
    "while dt < end:\n",
    "    dates_list.append(dt.strftime('%m/%d/%Y'))\n",
    "    dt += step\n",
    "\n",
    "for dt in dates_list:\n",
    "    def scrape_news_urls(link):\n",
    "        time.sleep(randint(0, 20))\n",
    "        payload = {'as_epq': 'bitcoin', 'tbs':'cdr:1,cd_min:'+dt+',cd_max:'+dt+\"'\",'tbm':'nws'}\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'}\n",
    "        r = requests.get(link, headers=headers, params=payload)\n",
    "        print(r.status_code)  # Print the status code\n",
    "        content = r.text\n",
    "        news_urls = []\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        news_url = soup.findAll(\"a\", {\"class\": \"l _PMs\"}, href=True)\n",
    "        for url in news_url:\n",
    "            news_urls.append(url['href'])\n",
    "        return news_urls\n",
    "    \n",
    "    URL = 'https://www.google.com/search'\n",
    "    n_url = scrape_news_urls(URL)\n",
    "    article_list = []\n",
    "    \n",
    "    for url in n_url:\n",
    "        if re.match(r'https://www.ft.com/',url) is not None:\n",
    "            from sys import exit\n",
    "            from selenium import webdriver\n",
    "            \n",
    "            # Function to access Financial Times(needs mandatory subscription) through Selenium enabled authentication \n",
    "            def open_browser():\n",
    "                global browser\n",
    "                browser = webdriver.Firefox()\n",
    "                browser.get('https://accounts.ft.com/login?location=https%3A%2F%2Fwww.ft.com%2F')\n",
    "                \n",
    "                username_input = input(\"Please enter your username (email address): \")\n",
    "                username_submit = browser.find_element_by_id(\"email\")\n",
    "                username_submit.send_keys(username_input)\n",
    "                browser.find_element_by_name(\"Next\").click()\n",
    "                \n",
    "                print(\"\"\"Please type in your password, then answer the following:\"\"\")\n",
    "                password_input = input(\"Please enter your password: \")\n",
    "                password_submit = browser.find_element_by_id(\"password\")\n",
    "                password_submit.send_keys(password_input)\n",
    "                confirmation = input(\"Have you typed in your password yet? (y/n): \")\n",
    "                \n",
    "                if confirmation == \"y\":\n",
    "                    browser.find_element_by_name(\"Sign in\").click() #submit\n",
    "                else:\n",
    "                    browser.close()\n",
    "                    exit()\n",
    "            \n",
    "            def article_search(url):\n",
    "                browser.get(url)\n",
    "                art = browser.find_element_by_xpath(\"//*[@id='site-content']/div[3]\")\n",
    "                article_list.append(art.text.rstrip('\\n').replace('\\n', ' '))\n",
    "            \n",
    "            # Uncomment if you are running for the first time to ensure access via authentication \n",
    "            def start(url):\n",
    "#                 choice = input(\"Do you need to log into FT.com? (y/n): \")\n",
    "#                 if choice == \"y\":\n",
    "#                     open_browser()    \n",
    "#                 elif choice == \"n\":\n",
    "                article_search(url)        \n",
    "#                 else:\n",
    "#                     exit()\n",
    "\n",
    "            start(url)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                from newspaper import Article\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article_list.append(article.text)\n",
    "            except:\n",
    "                article_list.append(\"\")\n",
    "\n",
    "    news_dictionary[dt] = article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all dates were processed \n",
    "print(news_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MONTH = SEPTEMBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.datetime(2017, 9, 1)\n",
    "end = datetime.datetime(2017, 10, 1)\n",
    "step = datetime.timedelta(days=1)\n",
    "\n",
    "dates_list = []\n",
    "\n",
    "while dt < end:\n",
    "    dates_list.append(dt.strftime('%m/%d/%Y'))\n",
    "    dt += step\n",
    "\n",
    "for dt in dates_list:\n",
    "    def scrape_news_urls(link):\n",
    "        time.sleep(randint(0, 20))\n",
    "        payload = {'as_epq': 'bitcoin', 'tbs':'cdr:1,cd_min:'+dt+',cd_max:'+dt+\"'\",'tbm':'nws'}\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'}\n",
    "        r = requests.get(link, headers=headers, params=payload)\n",
    "        print(r.status_code)  # Print the status code\n",
    "        content = r.text\n",
    "        news_urls = []\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        news_url = soup.findAll(\"a\", {\"class\": \"l _PMs\"}, href=True)\n",
    "        for url in news_url:\n",
    "            news_urls.append(url['href'])\n",
    "        return news_urls\n",
    "    \n",
    "    URL = 'https://www.google.com/search'\n",
    "    n_url = scrape_news_urls(URL)\n",
    "    article_list = []\n",
    "    \n",
    "    for url in n_url:\n",
    "        if re.match(r'https://www.ft.com/',url) is not None:\n",
    "            from sys import exit\n",
    "            from selenium import webdriver\n",
    "            \n",
    "            # Function to access Financial Times(needs mandatory subscription) through Selenium enabled authentication \n",
    "            def open_browser():\n",
    "                global browser\n",
    "                browser = webdriver.Firefox()\n",
    "                browser.get('https://accounts.ft.com/login?location=https%3A%2F%2Fwww.ft.com%2F')\n",
    "                \n",
    "                username_input = input(\"Please enter your username (email address): \")\n",
    "                username_submit = browser.find_element_by_id(\"email\")\n",
    "                username_submit.send_keys(username_input)\n",
    "                browser.find_element_by_name(\"Next\").click()\n",
    "                \n",
    "                print(\"\"\"Please type in your password, then answer the following:\"\"\")\n",
    "                password_input = input(\"Please enter your password: \")\n",
    "                password_submit = browser.find_element_by_id(\"password\")\n",
    "                password_submit.send_keys(password_input)\n",
    "                confirmation = input(\"Have you typed in your password yet? (y/n): \")\n",
    "                \n",
    "                if confirmation == \"y\":\n",
    "                    browser.find_element_by_name(\"Sign in\").click() #submit\n",
    "                else:\n",
    "                    browser.close()\n",
    "                    exit()\n",
    "            \n",
    "            def article_search(url):\n",
    "                browser.get(url)\n",
    "                art = browser.find_element_by_xpath(\"//*[@id='site-content']/div[3]\")\n",
    "                article_list.append(art.text.rstrip('\\n').replace('\\n', ' '))\n",
    "            \n",
    "            # Uncomment if you are running for the first time to ensure access via authentication \n",
    "            def start(url):\n",
    "#                 choice = input(\"Do you need to log into FT.com? (y/n): \")\n",
    "#                 if choice == \"y\":\n",
    "#                     open_browser()    \n",
    "#                 elif choice == \"n\":\n",
    "                article_search(url)        \n",
    "#                 else:\n",
    "#                     exit()\n",
    "\n",
    "            start(url)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                from newspaper import Article\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article_list.append(article.text)\n",
    "            except:\n",
    "                article_list.append(\"\")\n",
    "\n",
    "    news_dictionary[dt] = article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all dates were processed \n",
    "print(news_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MONTH = OCTOBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.datetime(2017, 10, 1)\n",
    "end = datetime.datetime(2017, 11, 1)\n",
    "step = datetime.timedelta(days=1)\n",
    "\n",
    "dates_list = []\n",
    "\n",
    "while dt < end:\n",
    "    dates_list.append(dt.strftime('%m/%d/%Y'))\n",
    "    dt += step\n",
    "\n",
    "for dt in dates_list:\n",
    "    def scrape_news_urls(link):\n",
    "        time.sleep(randint(0, 20))\n",
    "        payload = {'as_epq': 'bitcoin', 'tbs':'cdr:1,cd_min:'+dt+',cd_max:'+dt+\"'\",'tbm':'nws'}\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'}\n",
    "        r = requests.get(link, headers=headers, params=payload)\n",
    "        print(r.status_code)  # Print the status code\n",
    "        content = r.text\n",
    "        news_urls = []\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        news_url = soup.findAll(\"a\", {\"class\": \"l _PMs\"}, href=True)\n",
    "        for url in news_url:\n",
    "            news_urls.append(url['href'])\n",
    "        return news_urls\n",
    "    \n",
    "    URL = 'https://www.google.com/search'\n",
    "    n_url = scrape_news_urls(URL)\n",
    "    article_list = []\n",
    "    \n",
    "    for url in n_url:\n",
    "        if re.match(r'https://www.ft.com/',url) is not None:\n",
    "            from sys import exit\n",
    "            from selenium import webdriver\n",
    "            \n",
    "            # Function to access Financial Times(needs mandatory subscription) through Selenium enabled authentication \n",
    "            def open_browser():\n",
    "                global browser\n",
    "                browser = webdriver.Firefox()\n",
    "                browser.get('https://accounts.ft.com/login?location=https%3A%2F%2Fwww.ft.com%2F')\n",
    "                \n",
    "                username_input = input(\"Please enter your username (email address): \")\n",
    "                username_submit = browser.find_element_by_id(\"email\")\n",
    "                username_submit.send_keys(username_input)\n",
    "                browser.find_element_by_name(\"Next\").click()\n",
    "                \n",
    "                print(\"\"\"Please type in your password, then answer the following:\"\"\")\n",
    "                password_input = input(\"Please enter your password: \")\n",
    "                password_submit = browser.find_element_by_id(\"password\")\n",
    "                password_submit.send_keys(password_input)\n",
    "                confirmation = input(\"Have you typed in your password yet? (y/n): \")\n",
    "                \n",
    "                if confirmation == \"y\":\n",
    "                    browser.find_element_by_name(\"Sign in\").click() #submit\n",
    "                else:\n",
    "                    browser.close()\n",
    "                    exit()\n",
    "            \n",
    "            def article_search(url):\n",
    "                browser.get(url)\n",
    "                art = browser.find_element_by_xpath(\"//*[@id='site-content']/div[3]\")\n",
    "                article_list.append(art.text.rstrip('\\n').replace('\\n', ' '))\n",
    "            \n",
    "            # Uncomment if you are running for the first time to ensure access via authentication \n",
    "            def start(url):\n",
    "#                 choice = input(\"Do you need to log into FT.com? (y/n): \")\n",
    "#                 if choice == \"y\":\n",
    "#                     open_browser()    \n",
    "#                 elif choice == \"n\":\n",
    "                article_search(url)        \n",
    "#                 else:\n",
    "#                     exit()\n",
    "\n",
    "            start(url)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                from newspaper import Article\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article_list.append(article.text)\n",
    "            except:\n",
    "                article_list.append(\"\")\n",
    "\n",
    "    news_dictionary[dt] = article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all dates were processed \n",
    "print(news_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MONTH = NOVEMBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.datetime(2017, 11, 1)\n",
    "end = datetime.datetime(2017, 12, 1)\n",
    "step = datetime.timedelta(days=1)\n",
    "\n",
    "dates_list = []\n",
    "\n",
    "while dt < end:\n",
    "    dates_list.append(dt.strftime('%m/%d/%Y'))\n",
    "    dt += step\n",
    "\n",
    "for dt in dates_list:\n",
    "    def scrape_news_urls(link):\n",
    "        time.sleep(randint(0, 20))\n",
    "        payload = {'as_epq': 'bitcoin', 'tbs':'cdr:1,cd_min:'+dt+',cd_max:'+dt+\"'\",'tbm':'nws'}\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'}\n",
    "        r = requests.get(link, headers=headers, params=payload)\n",
    "        print(r.status_code)  # Print the status code\n",
    "        content = r.text\n",
    "        news_urls = []\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        news_url = soup.findAll(\"a\", {\"class\": \"l _PMs\"}, href=True)\n",
    "        for url in news_url:\n",
    "            news_urls.append(url['href'])\n",
    "        return news_urls\n",
    "    \n",
    "    URL = 'https://www.google.com/search'\n",
    "    n_url = scrape_news_urls(URL)\n",
    "    article_list = []\n",
    "    \n",
    "    for url in n_url:\n",
    "        if re.match(r'https://www.ft.com/',url) is not None:\n",
    "            from sys import exit\n",
    "            from selenium import webdriver\n",
    "            \n",
    "            # Function to access Financial Times(needs mandatory subscription) through Selenium enabled authentication \n",
    "            def open_browser():\n",
    "                global browser\n",
    "                browser = webdriver.Firefox()\n",
    "                browser.get('https://accounts.ft.com/login?location=https%3A%2F%2Fwww.ft.com%2F')\n",
    "                \n",
    "                username_input = input(\"Please enter your username (email address): \")\n",
    "                username_submit = browser.find_element_by_id(\"email\")\n",
    "                username_submit.send_keys(username_input)\n",
    "                browser.find_element_by_name(\"Next\").click()\n",
    "                \n",
    "                print(\"\"\"Please type in your password, then answer the following:\"\"\")\n",
    "                password_input = input(\"Please enter your password: \")\n",
    "                password_submit = browser.find_element_by_id(\"password\")\n",
    "                password_submit.send_keys(password_input)\n",
    "                confirmation = input(\"Have you typed in your password yet? (y/n): \")\n",
    "                \n",
    "                if confirmation == \"y\":\n",
    "                    browser.find_element_by_name(\"Sign in\").click() #submit\n",
    "                else:\n",
    "                    browser.close()\n",
    "                    exit()\n",
    "            \n",
    "            def article_search(url):\n",
    "                browser.get(url)\n",
    "                art = browser.find_element_by_xpath(\"//*[@id='site-content']/div[3]\")\n",
    "                article_list.append(art.text.rstrip('\\n').replace('\\n', ' '))\n",
    "            \n",
    "            # Uncomment if you are running for the first time to ensure access via authentication \n",
    "            def start(url):\n",
    "#                 choice = input(\"Do you need to log into FT.com? (y/n): \")\n",
    "#                 if choice == \"y\":\n",
    "#                     open_browser()    \n",
    "#                 elif choice == \"n\":\n",
    "                article_search(url)        \n",
    "#                 else:\n",
    "#                     exit()\n",
    "\n",
    "            start(url)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                from newspaper import Article\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article_list.append(article.text)\n",
    "            except:\n",
    "                article_list.append(\"\")\n",
    "\n",
    "    news_dictionary[dt] = article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all dates were processed \n",
    "print(news_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MONTH = DECEMBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = datetime.datetime(2017, 12, 1)\n",
    "end = datetime.datetime(2017, 1, 13)\n",
    "step = datetime.timedelta(days=1)\n",
    "\n",
    "dates_list = []\n",
    "\n",
    "while dt < end:\n",
    "    dates_list.append(dt.strftime('%m/%d/%Y'))\n",
    "    dt += step\n",
    "\n",
    "for dt in dates_list:\n",
    "    def scrape_news_urls(link):\n",
    "        time.sleep(randint(0, 20))\n",
    "        payload = {'as_epq': 'bitcoin', 'tbs':'cdr:1,cd_min:'+dt+',cd_max:'+dt+\"'\",'tbm':'nws'}\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36'}\n",
    "        r = requests.get(link, headers=headers, params=payload)\n",
    "        print(r.status_code)  # Print the status code\n",
    "        content = r.text\n",
    "        news_urls = []\n",
    "        soup = BeautifulSoup(content, \"lxml\")\n",
    "        news_url = soup.findAll(\"a\", {\"class\": \"l _PMs\"}, href=True)\n",
    "        for url in news_url:\n",
    "            news_urls.append(url['href'])\n",
    "        return news_urls\n",
    "    \n",
    "    URL = 'https://www.google.com/search'\n",
    "    n_url = scrape_news_urls(URL)\n",
    "    article_list = []\n",
    "    \n",
    "    for url in n_url:\n",
    "        if re.match(r'https://www.ft.com/',url) is not None:\n",
    "            from sys import exit\n",
    "            from selenium import webdriver\n",
    "            \n",
    "            # Function to access Financial Times(needs mandatory subscription) through Selenium enabled authentication \n",
    "            def open_browser():\n",
    "                global browser\n",
    "                browser = webdriver.Firefox()\n",
    "                browser.get('https://accounts.ft.com/login?location=https%3A%2F%2Fwww.ft.com%2F')\n",
    "                \n",
    "                username_input = input(\"Please enter your username (email address): \")\n",
    "                username_submit = browser.find_element_by_id(\"email\")\n",
    "                username_submit.send_keys(username_input)\n",
    "                browser.find_element_by_name(\"Next\").click()\n",
    "                \n",
    "                print(\"\"\"Please type in your password, then answer the following:\"\"\")\n",
    "                password_input = input(\"Please enter your password: \")\n",
    "                password_submit = browser.find_element_by_id(\"password\")\n",
    "                password_submit.send_keys(password_input)\n",
    "                confirmation = input(\"Have you typed in your password yet? (y/n): \")\n",
    "                \n",
    "                if confirmation == \"y\":\n",
    "                    browser.find_element_by_name(\"Sign in\").click() #submit\n",
    "                else:\n",
    "                    browser.close()\n",
    "                    exit()\n",
    "            \n",
    "            def article_search(url):\n",
    "                browser.get(url)\n",
    "                art = browser.find_element_by_xpath(\"//*[@id='site-content']/div[3]\")\n",
    "                article_list.append(art.text.rstrip('\\n').replace('\\n', ' '))\n",
    "            \n",
    "            # Uncomment if you are running for the first time to ensure access via authentication \n",
    "            def start(url):\n",
    "#                 choice = input(\"Do you need to log into FT.com? (y/n): \")\n",
    "#                 if choice == \"y\":\n",
    "#                     open_browser()    \n",
    "#                 elif choice == \"n\":\n",
    "                article_search(url)        \n",
    "#                 else:\n",
    "#                     exit()\n",
    "\n",
    "            start(url)\n",
    "            \n",
    "        else:\n",
    "            try:\n",
    "                from newspaper import Article\n",
    "                article = Article(url)\n",
    "                article.download()\n",
    "                article.parse()\n",
    "                article_list.append(article.text)\n",
    "            except:\n",
    "                article_list.append(\"\")\n",
    "\n",
    "    news_dictionary[dt] = article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all dates were processed \n",
    "print(news_dictionary.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
